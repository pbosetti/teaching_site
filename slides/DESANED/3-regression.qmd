---
title: "Regression"
subtitle: |
  Design of Experiments and Statistical Analysis of Experimental Data, 2024 \
  ![](images/by-nc-sa.png){height=30}
author: "Paolo Bosetti"
institute: "Universit√† di Trento, Dipartimento di Ingegneria Industriale"
format: 
  revealjs:
    width: 1280
    height: 700
    margin: 0.1
    slide-number: true
    code-line-numbers: true
    code-annotations: below
    preview-links: auto
    theme: [default, style.scss]
    chalkboard: true
    footer: "paolo.bosetti@unitn.it --- [https://paolobosetti.quarto.pub/desaned/](https://paolobosetti.quarto.pub/desaned/)"
    fig-width: 5
    fig-height: 4
    fig-dpi: 300
execute: 
  cache: true
---

```{r}
options(width = 60)
set.seed(0)
library(latex2exp)
library(glue)
library(modelr)
library(tidyverse)
theme_set(theme_gray()+theme(legend.position = "bottom"))
```

# Regression
$\renewcommand{\hat}[1]{\widehat{#1}}$
$\renewcommand{\tilde}[1]{\widetilde{#1}}$
$\renewcommand{\theta}{\vartheta}$

Each **model** of a physical system or process depends on **parameters**. These parameters must be calculated **adapting** the model to experimental observations (measurements)

The operation of *fitting* a model is carried out using **regression**

## Basics

- A model of a physical system can be expressed as:
$$
     y=f(x_1, x_2, \dots, x_n, c_1, c_2, \dots, c_m)
$$
where $x_i$ are the physical (random) variables, called **regressors** or **predictors**, while the $c_i$ are the **parameters** (constant) of the model, and $y$ is the **response** or dependent variable

- If $n=1$ there is a single predictor and a single dependent variable we talk about **simple regression**

- For example, $y=a+bx+cx^2$ is a simple linear model with parameters $a,~b,~c$

- **regressing the model** means making measurements of $y$ for different values of $x$ and determining the values of the parameters $a,~b,~c$ that **minimize the distance** between the model and experimental observations

## Types of regression

We will consider three types of regression:

- Linear regression: the model is a **linear combination of parameters**
- Generalized linear regression
- Least squares regression: parameters are combined in a non-linear way

# Linear Regression

Valid for every model that is **linear in the parameters** (while predictors can appear with a degree other than 1)

## Basics {.smaller}

The **statistical model** of a process with $n$ parameters to be regressed is defined as follows:
$$
y_i=f(x_{1i}, x_{2i}, \dots, y_{ni}) = \hat{y_i} + \varepsilon_{ij},~~~i=1,2,\dots,N
$$

where $i$ is the observation index ($N\geq n+1$ in total), $\hat{y_i}$ is the **regressed value**, or **prediction**, at the observation $i$, and $\varepsilon_{ij}$ are the **residuals**

- The regressed value corresponds to the **deterministic component**
- The residuals are the **random component**
- The hypothesis of **normality of the residuals** is assumed, i.e. that $\varepsilon_{ij} \sim\mathcal{N}(0, \sigma^2)$

. . .

If the $f(\cdot)$ is an analytical function with one predictor and linear in the coefficients, then we can express it as $\mathbf{A}\mathbf{k}=\mathbf{y}$, where

* $\mathbf{y}$ is the vector of $y_i,~i=1\dots N$
* $\mathbf{k}$ is the vector of parameters $c_j,~j=1\dots n+1$
* $\mathbf{A}$ a matrix $N\times (n+1)$ composed as follows:

$$
\mathbf A = \begin{bmatrix}
x_1^{n-1} & x_1^{n-2} & \dots & x_1 & 1 \\
x_2^{n-1} & x_2^{n-2} & \dots & x_2 & 1 \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
x_N^{n-1} & x_N^{n-2} & \dots & x_N & 1 \\
\end{bmatrix}
$$

## Basics

The linear equation can be solved with the **pseudo-inverse** method:
$$
\begin{align}
\mathbf A^T \mathbf A\cdot \mathbf{k} &= \mathbf A^T \cdot \mathbf{y} \\
(\mathbf A^T \mathbf A)^{-1} \mathbf A^T \mathbf A\cdot \mathbf{k} &= (\mathbf A^T \mathbf A)^{-1} \mathbf A ^T \cdot \mathbf{y} \\
\mathbf{k} &= (\mathbf A^T \mathbf A)^{-1} \mathbf A^T \cdot \mathbf{y}
\end{align}
$$
This relationship makes clear what is meant by **linear regression**: it has nothing to do with the **degree of the regressed function** (which is not required to be linear in the predictors!), but only with the equation **linear in the parameters** that represents the model

**NOTE**:

- from the previous matrix equation it is clear that the regression can be performed if and only if $N\geq n+1$, i.e. if the number of observations is at least equal to the number of parameters
- even in the case of a $f(x_1,x_2,\dots,x_n)$, if it is linear in the coefficients it is always possible to express it as $\mathbf{A}\mathbf{k}=\mathbf{y}$ and therefore solve it with the pseudo-inverse method

## Example

::: columns
::: column
Let the model to be regressed be of the type $y_i=(ax_i + b) + \varepsilon_i$; then it can be represented as:

$$
\begin{bmatrix}
x_1 & 1 \\
x_2 & 1 \\
\vdots & \vdots \\
x_N & 1 \\
\end{bmatrix} \cdot
\begin{bmatrix}
a \\
b
\end{bmatrix} =
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_N
\end{bmatrix}
$$
:::

::: column
```{r}
library(modelr)
set.seed(1)
df <- tibble(x=1:10, y=3+2*x+rnorm(length(x), sd=3))
df.lm <- lm(y~x, data=df)
df <- add_predictions(df, df.lm)
df <- add_residuals(df, df.lm)
ggplot(df, aes(x=x, y=y)) +
   geom_line(aes(y=pred), color="red") +
   geom_linerange(aes(ymin=pred, ymax=pred+resid), color="blue", size=1) +
   geom_point(size=2)
```
:::
:::

The figure shows the observations as points with coordinates $(x_i, y_i)$, the regressed model as a **red line** (linear model in the coefficients $a,~b$ and first degree of the predictor $x$) and the residuals $\varepsilon_i$ as blue segments representing the difference between the $y_i$ and the corresponding regressed value $\hat{y_i}$

# Least Squares Regression

If the model is not linear in its parameters, it is still possible to regress it with the **least squares** method

## Least Squares Regression

That is, we look for the set of parameter values that **minimizes the distance** between the model and the observations. This minimization can be achieved by defining an **index of merit** which represents the distance between the model and observations **depending on the parameters**:
$$
\Phi(c_1, c_2,\dots,c_m)=\sum_{i=1}^N \left(y_i - f(x_{1i},x_{2i},\dots,x_{ni}, c_1, c_2 ,\dots,c_m) \right)^2
$$ \

* If the $f(\cdot)$ is analytic and differentiable, then we can **minimize** $\Phi(\cdot)$ by differentiation, i.e. by solving the system of $m$ equations
$$
\frac{\partial\Phi}{\partial c_i}(c_1,c_2,\dots,c_m) = 0
$$

- If $f(\cdot)$ **is not differentiable**, the minimum of $\Phi(\cdot)$ can still be calculated numerically (e.g. Newton-Raphson method)

# Regression quality

To a given set of observations it is possible to fit **an infinite number of models**

It is possible to define some **parameters of merit** and some **verification methods** that allow you to evaluate the quality of a regression and, therefore, identify the model that best fits the observations

## Coefficient of Determination

::: columns
::: column
- The coefficient of merit most used to evaluate a regression is the **coefficient of determination** $R^2$

- It is defined as $R^2 = 1 - \frac{SS_\mathrm{res}}{SS_\mathrm{tot}}$, where $SS_\mathrm{res} = \sum \varepsilon_i^2$ and $SS_ \mathrm{tot} = \sum(y_i - \bar y)^2$

- If the regressed values correspond to the observed values $y_i=\hat{y_i}$, then the residuals are all zero and $R^2 = 1$

- The quality of the regression decreases as $R^2$ decreases
:::

::: column
::: panel-tabset
### Two datasets

```{r}
set.seed(0)
N <- 20
df <- tibble(
   x=runif(N, 0, 10),
   y1=5*x+12 + rnorm(N,sd=1),
   y2=0.5*x^2 + 1*x + 3 + rnorm(N,sd=1)
)
lm1 <- lm(y1~x, data=df)
df <- add_predictions(df, lm1, var="y1_hat")
lm2 <- lm(y2~x, data=df)
df <- add_predictions(df, lm2, var="y2_hat")
lm3 <- lm(y2~poly(x, degree=2, raw=T), data=df)
df <- add_predictions(df, lm3, var="y3_hat")
r <- c(summary(lm1)$r.squared, summary(lm2)$r.squared, summary(lm3)$r.squared)

df %>% ggplot(aes(x=x)) +
   geom_point(aes(y=y1, color="first set")) +
   geom_line(aes(y=y1_hat, color="first set")) +
   geom_point(aes(y=y2, color="second set")) +
   geom_line(aes(y=y2_hat, color="second set")) +
   labs(color="data:", title=TeX(glue("$R^2_1={r[1]}$, $R^2_2={r[2]}$")), x="predictor" , y="observations") +
   theme(legend.position = "bottom")
```

### Same set, two models

```{r}
df %>% ggplot(aes(x=x)) +
   geom_point(aes(y=y2)) +
   geom_line(aes(y=y2_hat, color="first degree")) +
   geom_line(aes(y=y3_hat, color="second degree")) +
   labs(color="pattern:", title=TeX(glue("$R^2_1={r[2]}$, $R^2_2={r[3]}$")), x="predictor" , y="observations") +
   theme(legend.position = "bottom")
```
:::
:::
:::

:::aside
**Note**: As you can see, the values of the regressors do not need to be equally spaced!
:::

## Underfitting

::: columns
::: column
There is **underfitting** when the model has a lower degree than the apparent behavior of the observations

It can be highlighted, in addition to a low $R^2$, by studying the distribution of the residuals: if there is under-fitting the residuals can be non-normal and, above all, show **trends**, or *patterns *

A *pattern* is a regular trend of the residuals as a function of the regressors

From the number of maximums and minimums present in the possible *pattern* it is possible to estimate **how many degrees are missing**
:::

::: column
::: panel-tabset
### Underfitting

```{r}
#| fig-width: 5
#| fig-height: 2
df %>% ggplot(aes(x=x)) +
   geom_point(aes(y=y2)) +
   geom_line(aes(y=y2_hat)) +
   labs(x="predictor", y="observations", title="first-order model")
df %>% ggplot(aes(x=x)) +
   geom_point(aes(y=lm2$residuals)) +
   labs(x="predictor", y="residuals")
```

### Correct fit

```{r}
#| fig-width: 5
#| fig-height: 2
df %>% ggplot(aes(x=x)) +
   geom_point(aes(y=y2)) +
   geom_line(aes(y=y3_hat)) +
   labs(x="predictor", y="observations", title="second order model")
df %>% ggplot(aes(x=x)) +
   geom_point(aes(y=lm3$residuals)) +
   labs(x="predictor", y="residuals")
```
:::
:::
:::

## Overfitting

::: columns
::: column
If the degree of the model is excessive, the model tends to **chase individual points**

The value of $R^2$ increases, reaching 1 when the degree equals the number of observations minus 1

However, the model **loses generality** and is no longer able to correctly predict **new values** acquired at a later time (the red crosses in the figure)

Overfitting has particularly dramatic effects in case of **extrapolation**, i.e. when evaluating the model outside the range into which it has been regressed
:::

::: column
```{r}
set.seed(0)
N <- 8
df <- tibble(
   x=1:N,
   y=3*x+4 + rnorm(N, sd=2),
   y2=3*x+4 + rnorm(N, sd=2)
)
dfp <- tibble(x=seq(-1, N+2, length.out=101))
lm1 <- lm(y~x, data=df)
dfp <- add_predictions(dfp, lm1, var="pred.lm1")
lm2 <- lm(y~poly(x, deg=6, raw=T), data=df)
dfp <- add_predictions(dfp, lm2, var="pred.lm2")
df %>% ggplot(aes(x=x)) +
   geom_point(aes(y=y)) +
   geom_point(aes(y=y2), color="red", shape=3) +
   geom_line(aes(x=x, y=pred.lm1, color="degree 1"), data=dfp) +
   geom_line(aes(x=x, y=pred.lm2, color="degree 6"), data=dfp) +
   coord_cartesian(ylim=c(5,27), xlim=c(0,9)) +
   labs(color="model:", x="predictor", y="observations") +
   theme(legend.position = "bottom")
```
:::
:::


## Prediction Bands

::: columns
::: column
It is a band **symmetric with respect to the regression** within which the observations (present and future) have an assigned probability of falling

In general, for a sufficiently large number of observations ($>50$) the 95% prediction band contains 95% of the observations
:::

::: column
```{r}
set.seed(1)
f <- function(x, offset) 0.3*(x-offset)^2+1*(x-offset)
N <- 20
offset <- 5
df <- tibble(
   x=seq(0, 10, length.out=N),
   y= f(x, offset) + rnorm(N, sd=1)
)
dfp <- tibble(
   x=seq(0, 10, length.out=N*10),
   y=f(x, offset) + rnorm(N, sd=1)
)
lm <- lm(y~poly(x, deg=2, raw=T), data=df)
dfp <- cbind(dfp, predict(lm, dfp, interval="prediction"), level=0.95)

df %>% ggplot(aes(x=x)) +
   geom_point(aes(y=y)) +
   geom_line(aes(x=x, y=fit), data=dfp, color="blue", linewidth=1) +
   geom_line(aes(x=x, y=upr), data=dfp, color="red", linetype=2) +
   geom_line(aes(x=x, y=lwr), data=dfp, color="red", linetype=2) +
   geom_point(aes(x=x, y=y), data=dfp, color=gray(2/3), shape=3) +
   coord_cartesian(ylim=c(-4,15)) +
   labs(x="predictor", y="observations", title="95% prediction band")
```
:::
:::

## Confidence Bands

::: columns
::: column
It is a band symmetric with respect to the regression within which **the expected value of the model** has an assigned probability of falling

It is always narrower than the prediction band

It is the multi-dimensional equivalent of the confidence interval for a T-test: as this is the interval, within which the value corresponding to the null hypothesis has an assigned probability of falling, here we can assume that the "*true*" model falls within the confidence band with a certain probability
:::

::: column
```{r}
df %>% ggplot(aes(x=x)) +
   geom_point(aes(y=y)) +
   geom_line(aes(x=x, y=fit), data=dfp) +
   geom_smooth(aes(y=y), method="lm", formula=lm$call$formula, level=0.95) +
   coord_cartesian(ylim=c(-4,15)) +
   labs(x="predictor", y="observations", title="95% confidence band")
```
:::
:::

It is obtained by calculating the confidence intervals on the regression parameters, then calculating---for each value of the predictor---the maximum and minimum value of the regression corresponding to the extreme values of the parameters in their confidence intervals

# Generalized Linear Regression

Classic linear regression assumes the hypothesis of **normality of residuals**

When this hypothesis is not true, but the **model is still linear in the parameters**, generalized linear regression can be used

## Basics

In the case of linear regression:
$$
\begin{align}
y_i &= f(\mathbf{x}_i, \mathbf{k}) + \varepsilon_i = \eta_i + \varepsilon_i \\
\varepsilon_i &\sim \mathcal{N}(0, \sigma^2)
\end{align}
$$
In the case of **generalized linear regression**:

$$
\begin{align}
y_i &= \eta_i + \varepsilon_i \\
\varepsilon_i &\sim D(p_1,p_2,\dots,p_k)
\end{align}
$$
where $D$ is a generic distribution with $k$ parameters belonging to the **family of exponential distributions** (normal, binomial, gamma, inverse normal, Poisson, quasinormal, quasibinomial and quasipoissonian)

The problem can be solved by introducing a **link function** that rescales the residuals by projecting them onto a normal distribution

## Basics
The **link function** $g(\cdot)$ is such that:

$$
\begin{align}
y_i &= \eta_i + g(\varepsilon_i) \\
\varepsilon_i &\sim D(p_1,p_2,\dots,p_k);~g(\varepsilon_i)\sim \mathcal{N}(0, \sigma^2)
\end{align}
$$
The link functions for the most common distributions are:

:::{.columns}
:::{.column}

| Distribution | Link function |
|---------------|-------------------------|
| Normal | $g(x)=x$ |
| Binomial | $g(x)=\mathrm{logit}(x)$ |
| Poisson | $g(x)=\log(x)$ |
| Range | $g(x)=1/x$ |

In particular, it holds: $\mathrm{logit}(x)=\frac{1}{1+e^{-p(x-x_0)}}$
:::

:::{.column}
```{r}
logit <- function(x, p=1, x0=0) 1/(1+exp(-p*(x-x0)))
tibble(x=seq(-6, 6, 0.1), y1=logit(x, 1, 1), y2=logit(x, -4, 1)) %>%
   ggplot(aes(x=x)) +
   geom_hline(yintercept=c(0, 0.5, 1), lty=2) +
   geom_vline(xintercept=1, lty=2) +
   geom_line(aes(y=y1, color="p=1")) +
   geom_line(aes(y=y2, color="p=-4")) +
   labs(y="logit(x, x0=2, p=1)", color="slope:") +
   theme(legend.position = "bottom")
```

:::
:::

## Logistic Regression

:::{.columns}
:::{.column}
The typical case of logistic regression is the **binomial event** classifier

we consider a process that, depending on one or more predictors, can provide a result that can only be valid for one of two alternatives (success/failure, broken/intact, true/false, 1/0). We want to **identify the threshold of predictors** that switches the outcome

A linear regression is not suitable for the situation: it is clear that the **residuals are not normal** and that the slope of the regression depends a lot on how many points are collected in the "safe" zones
:::

:::{.column}
:::{.panel-tabset}
### Data
```{r}
library(PearsonDS)
set.seed(0)
N <- 50
N2 <- 0
v0 <- c(90, 102)
sd <- 5
m1 <- list(m=90, variance=5, skewness=1.1, kurtosis=4)
m2 <- list(m=99, variance=5, skewness=-1.1, kurtosis=4)
df <- tibble(
   OK=c(rep(c(T, F), N), rep(T, N2))
) %>%
   arrange(OK) %>%
   mutate(OKv = as.numeric(OK), val=c(
     rpearson(N, moments=m1), rpearson(N, moments=m2), runif(N2, 100, 120)))

df.lm <- lm(OKv~val, data=df)
df <- add_predictions(df, df.lm, var="pred.lm")
df <- add_residuals(df, df.lm, var="res.lm")

df %>% ggplot(aes(x=val,y=OKv)) +
   geom_point(aes(color=OK)) +
   geom_line(aes(y=pred.lm), color="blue") +
   coord_cartesian(ylim=c(0,1)) +
   labs(x="value", y="success", color="success:") +
   theme(legend.position = "bottom")
```
### Linear model residuals
```{r}
#| fig-width: 5
#| fig-height: 2
df %>% ggplot(aes(x=val, y=res.lm)) +
   geom_point() +
   labs(x="value", y="residues", title="residues")

df %>% ggplot(aes(sample=res.lm)) +
   geom_qq() +
   geom_qq_line() +
   labs(x="theoretical quantiles", y="sample quantiles", title="Q-Q Graph")
```
:::

:::
:::


## Logistic Regression

:::{.columns}
:::{.column}
The regressed logistic function provides the parameter $x_0$ which identifies the value that separates an **equal amount of false positives and false negatives**

Furthermore, it is possible to identify the appropriate threshold to obtain a predetermined probability of false positives (or false negatives)

This is the simplest type of **machine learning**: a binomial classifier
:::

:::{.column}
```{r}
df.glm <- glm(OKv~val, family=binomial(), data=df)
df <- add_predictions(df, df.glm, var="pred.glm", type="response")
df <- add_residuals(df, df.glm, var="res.glm")

df %>% ggplot(aes(x=val, y=OKv)) +
   geom_point(aes(color=OK)) +
   geom_line(aes(x=val, y=pred.glm), color="blue") +
   geom_line(aes(y=pred.lm), color=gray(2/3), linetype=2) +
   coord_cartesian(ylim=c(0,1)) +
   labs(x="value", y="success", color="success:") +
   theme(legend.position = "bottom")
```


:::
:::


# Presentation of data
The concept of **confidence band** is essential in the graphical presentation of data from multiple series

## Graphic comparison of multiple series

```{r}
set.seed(0)
r <- 8
alpha <- 0.05
df.n <- tibble(
   x=seq(100, 900, 100),
   S1=c(300, 310, 318, 330, 290, 305, 358, 290, 180),
   S2=c(315, 315, 322, 340, 272, 315, 340, 285, 185),
   n=c(10, 10, 25, 10, 10, 7, 20, 10, 10)
) %>% mutate(S2=S2-15)

df <- expand.grid(
   r=1:r,
   x=df.n$x
) %>% mutate(
   S1=map_dbl(x, ~df.n[df.n$x==.,]$S1),
   S2=map_dbl(x, ~df.n[df.n$x==.,]$S2)
   ) %>%
   tibble() %>%
   pivot_longer(S1:S2, names_to="s", values_to="nominal", cols_vary = "slowest")

df <- df %>%
   group_by(s, x) %>%
   group_modify(~ {.x$noise=rnorm(8, sd=df.n[df.n$x==.y$x,]$n); .x}) %>%
   mutate(value=nominal+noise)

df.s <- df %>% group_by(s, x) %>% summarise(
   n=n(),
   ci=sd(value)/sqrt(n) * qt(alpha/2, n - 1, lower.tail = F),
   value=mean(value),
   lwr=value-ci,
   upr=value+ci
)
```

:::columns
:::column
Suppose we have a process whose value depends on a variable $x$

Suppose that a process parameter $S$ can affect the output value. For example:

* the value is the hardness of a metal, $x$ is the temperature, the parameter $S$ is the quantity of an alloying element
* the value is the productivity of a plant, $x$ is a quantitative process parameter, the parameter $S$ is the type of machine used

:::

:::column

```{r}
df %>% ggplot(aes(x=x)) +
   geom_point(aes(y=value, group=s, color=s, shape=s)) +
   labs(color="Series:", shape="Series:", y="value", title="Raw data") +
   coord_cartesian(xlim=c(50,950), ylim=c(150,400)) +
   scale_x_continuous(breaks=df.n$x) +
   theme(legend.position = "bottom")
```
:::
:::
Suppose we repeat `r r` times a measurement of the output value for the various combinations of $x$ and $S$, obtaining the results in the figure: **which differences are significant**?


## Without a reference model

:::columns
:::column
Without a reference model that expresses $v=f(x, S)$ **it makes no sense to perform a regression**

However, I can report, for each **treatment**

* the average value, joining the series with a broken line for the sole purpose of visually grouping the data
* the limits of the confidence interval for each series and for each treatment
* or, join the limits with a band representing **confidence about the mean**
:::

:::column
```{r}
df.s %>% ggplot(aes(x=x)) +
   geom_ribbon(aes(ymin=lwr, ymax=upr, group=s), alpha=1/6) +
   geom_errorbar(aes(ymin=lwr, ymax=upr, group=s, color=s), alpha=2/3) +
   geom_line(aes(y=value, group=s, color=s), linewidth=1) +
   geom_point(aes(x=x, y=value, group=s, color=s, shape=s), data=df) +
   labs(color="Series:", shape="Series:", y="value", title="95% Confidence") +
   coord_cartesian(xlim=c(50,950), ylim=c(150,400)) +
   scale_x_continuous(breaks=df.n$x) +
   theme(legend.position = "bottom")
```
:::
:::
**Areas where the bands overlap are statistically indistinguishable**

## With a model

:::columns
:::column
**Only if I have a model $v=f(x, S)$** I can perform a regression

Also in this case, the regression must be accompanied with confidence bands

Again, **areas where the bands overlap are statistically indistinguishable**

:::
:::column
```{r}
df %>% ggplot(aes(x=x)) +
   geom_smooth(aes(y=value, group=s, color=s)) +
   geom_point(aes(y=value, group=s, color=s, shape=s)) +
   labs(color="Series:", shape="Series:", y="value", title="95% Confidence") +
   coord_cartesian(xlim=c(50,950), ylim=c(150,400)) +
   scale_x_continuous(breaks=df.n$x) +
   theme(legend.position = "bottom")
```
:::
:::