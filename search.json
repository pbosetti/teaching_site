[
  {
    "objectID": "various.html",
    "href": "various.html",
    "title": "Various stuff",
    "section": "",
    "text": "Miroscic coding session 1: Seminar within the Miroscic PRIN project\nK-Fold Cross Validation: K-Fold Cross Validation document\nRcpp intro: Rcpp introduction"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#industrial-experiment-or-scientific-experiment",
    "href": "slides/DESANED/5-DoE.html#industrial-experiment-or-scientific-experiment",
    "title": "Design of Experiments",
    "section": "Industrial experiment or scientific experiment?",
    "text": "Industrial experiment or scientific experiment?\n\n\nA scientific experiment is generally conducted with the purpose of supporting or disproving a theory\n\nit is always based on a theoretical model to be verified\noften the model focuses on the effect of a limited number of explanatory variables\n\n\nIn the industrial field this is often not possible:\n\noften a theoretical model for subject of the experiment is not available due to scientific, technical, or practical reasons\nthe interaction between multiple explanatory variables is often what matters most\n\n\n\nFor practical purposes, the design of the experiment is all the more important the higher the complexity (i.e. the number of factors involved)"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#objectives-of-an-experiment",
    "href": "slides/DESANED/5-DoE.html#objectives-of-an-experiment",
    "title": "Design of Experiments",
    "section": "Objectives of an experiment",
    "text": "Objectives of an experiment\nIn general, an experiment serves to:\n\nconfirm a theoretical hypothesis (model): you want to verify the form \\(y=f(\\cdot)\\) of the theoretical model; the regression of the model is accompanied by the study of confidence intervals (analytical or bootstrap)\ncalibrate the parameters of a model: the shape is known and we want to obtain the parameter values; generally a regression is carried out, collecting data under realistic operating conditions\nidentify the explanatory variables that influence a process: the model may be unknown and we want to determine the list of independent variables that appear in the \\(y=f(\\cdot)\\); the objective is to build an approximate empirical model, which can possibly be used as a starting point for the formulation of a theoretical model"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#dimensionality-of-an-experiment",
    "href": "slides/DESANED/5-DoE.html#dimensionality-of-an-experiment",
    "title": "Design of Experiments",
    "section": "Dimensionality of an experiment",
    "text": "Dimensionality of an experiment\nIf the model of interest si simple (i.e. one predictor), the experiment consists of analyzing the response variable at a sequence of levels for the explanatory variable. The number of levels is correlated with the expected degree of the response: for a regression of degree \\(m\\) you need at least \\(l=m+1\\) levels\nBut if the model has multiple predictors, i.e. the output depends on \\(n\\) explanatory variables, and each variable is investigated on \\(l\\) levels, then the number of test conditions is \\(l^n\\)\nIf each test condition is then repeated \\(r\\) times (to average the results), the number of individual experiments is \\(rl^n\\)\nThis number can become large and economically unsustainable very quickly\n\n\nSuppose we have 4 parameters and 4 levels, repeating each test 3 times: the total number of tests is 768; if the parameters become 5, the number grows to 3072, i.e. 4 times as much, a ratio which is also valid for the cost"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#factorial-plan",
    "href": "slides/DESANED/5-DoE.html#factorial-plan",
    "title": "Design of Experiments",
    "section": "Factorial plan",
    "text": "Factorial plan\n\n\n\nTwo factors, \\(A\\) and \\(B\\)\nWe investigate 2 levels for each factor indicated as \\(X-\\) and \\(X+\\)\nLet’s change one level at a time\nWe evaluate each treatment only 1 time\nLet’s evaluate the effects of \\(A\\) and \\(B\\): \\[\n   \\begin{align}\n   A &= 50 - 20 = 30\\\\\n   B &= 30 - 20 = 10\n   \\end{align}\n   \\]"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#yates-notation",
    "href": "slides/DESANED/5-DoE.html#yates-notation",
    "title": "Design of Experiments",
    "section": "Yates notation",
    "text": "Yates notation\nWhen the levels of all factors are only two, the Yates order can be used to indicate the combinations of levels:\n\nFactors and effects of factors are indicated with capital letters\nTreatments are indicated with combinations of lowercase letters\n\nletter present means factor at high level\nabsent letter means factor at low level\nif all the letters are absent, write \\((1)\\)\n\n\n\n\nIn the example on the previous slide, the treatments are \\((1), a, b\\)"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#factorial-plan-1",
    "href": "slides/DESANED/5-DoE.html#factorial-plan-1",
    "title": "Design of Experiments",
    "section": "Factorial plan",
    "text": "Factorial plan\n\n\nChanging one factor at a time does not identify the interactions\nInteraction occurs when the effect of one factor depends on the level of another factor\nIn this second example we measure the response of treatments \\((1), a, b, ab\\)\nWe can estimate both the effects of \\(A\\) and \\(B\\) and the interaction \\(AB\\):\n\\[\n\\begin{align}\nA &= \\frac{a+ab}{2} - \\frac{(1) + b}{2} = 6\\\\\nB &= \\frac{b+ab}{2} - \\frac{(1) + a}{2} = -14 \\\\\nAB &= \\frac{a+b}{2} - \\frac{(1)+ab}{2} = -24\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#interaction-graph",
    "href": "slides/DESANED/5-DoE.html#interaction-graph",
    "title": "Design of Experiments",
    "section": "Interaction graph",
    "text": "Interaction graph\n\n\nThe concept of interaction is well illustrated by interaction graphs\n\nIf the two segments are parallel there is no interaction\nIf the two segments are crossed or convergent there is interaction\nIt is indifferent which factor is on the abscissa and which is on the series\n\n\n\nYield vs. AYield vs. B"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#response-surface",
    "href": "slides/DESANED/5-DoE.html#response-surface",
    "title": "Design of Experiments",
    "section": "Response surface",
    "text": "Response surface\n\n\nThe interaction graphs are nothing more than projections on the axis of one of the two factors of the response surface\nCoded units are generally used, rescaling the range of each factor to the range \\([-1,1]\\)\nThis way you have the same sensitivity regardless of the original scale range\n\n\n\n\n\n\n\n\n\n\n\nNOTE: Neither the response surface nor the interaction plots give any information on the statistical significance of the effects"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#l2-factorial-plans",
    "href": "slides/DESANED/5-DoE.html#l2-factorial-plans",
    "title": "Design of Experiments",
    "section": "\\(l^2\\) factorial plans",
    "text": "\\(l^2\\) factorial plans\nIn general, an experiment in which you have 2 factors each tested on \\(l\\) levels is a \\(l^2\\) factorial design, because the total number of treatments is \\(N=rl^2\\), where \\(r\\) is the number of repetitions for each treatment\nThe statistical model and regression model associated with the experiment are:\n\\[\ny_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk};\\quad \\hat y = \\mu + \\alpha x_1 + \\beta x_2 + (\\alpha\\beta)x_1 x_2\n\\] with \\(x_1\\) and \\(x_2\\) the values of the two factors in coded units\nAs such, the experiment can be studied with a two-factor ANOVA (or two-way ANOVA):\n\n\n\\[\n\\textrm{A)}~\\left\\{\n\\begin{align}\nH_0&: \\alpha_1 = \\alpha_2 = \\dots =\\alpha_a = 0 \\\\\nH_1&: \\alpha_i \\ne 0\\quad\\textrm{for at least one}~i\n\\end{align}\n\\right.\n\\]\n\\[\n\\textrm{B)}~\\left\\{\n\\begin{align}\nH_0&: \\beta_1 = \\beta_2 = \\dots =\\beta_b = 0 \\\\\nH_1&: \\beta_j \\ne 0\\quad\\textrm{for at least one}~j\n\\end{align}\n\\right.\n\\]\n\n\\[\n\\textrm{AB)}~\\left\\{\n\\begin{align}\nH_0&: (\\alpha\\beta)_{ij} = 0\\quad \\forall~(i,j) \\\\\nH_1&: (\\alpha\\beta)_{ij} \\ne 0\\quad \\textrm{for at least one}~(i,j)\n\\end{align}\n\\right.\n\\]"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#l2-factorial-plans-1",
    "href": "slides/DESANED/5-DoE.html#l2-factorial-plans-1",
    "title": "Design of Experiments",
    "section": "\\(l^2\\) factorial plans",
    "text": "\\(l^2\\) factorial plans\nThese pairs of hypotheses correspond to a decomposition of the corrected total square sum \\(SS_\\mathrm{tot}=SS_A + SS_B + SS_{AB} + SS_E\\)\nThe corresponding ANOVA table is:\n\n\n\n\n\n\n\n\n\n\n\nEffect\n\\(\\nu\\) (GdL)\n\\(SS\\)\n\\(MS\\)\n\\(F_0\\)\np-value\n\n\n\n\nA\n\\(a-1\\)\n\\(SS_A\\)\n\\(SS_A/\\nu_A\\)\n\\(MS_A/MS_E\\)\n\\(\\mathrm{CDF}^+(F_{0,A}, \\nu_A, \\nu_E)\\)\n\n\nB\n\\(b-1\\)\n\\(SS_B\\)\n\\(SS_B/\\nu_B\\)\n\\(MS_B/MS_E\\)\n\\(\\mathrm{CDF}^+(F_{0,B}, \\nu_B, \\nu_E)\\)\n\n\nAB\n\\((a-1)(b-1)\\)\n\\(SS_{AB}\\)\n\\(SS_{AB}/\\nu_{AB}\\)\n\\(MS_{AB}/MS_E\\)\n\\(\\mathrm{CDF}^+(F_{0,AB}, \\nu_{AB}, \\nu_E)\\)\n\n\nError\n\\(ab(n-1)\\)\n\\(SS_E\\)\n\\(SS_E/\\nu_E\\)\n—\n—\n\n\nTotal\n\\(abn-1\\)\n\\(SS_\\mathrm{tot}\\)\n\\(SS_\\mathrm{tot}/\\nu_\\mathrm{tot}\\)\n—\n—"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#example",
    "href": "slides/DESANED/5-DoE.html#example",
    "title": "Design of Experiments",
    "section": "Example",
    "text": "Example\nWe want to study the effect of cutting speed (factor \\(A\\)) and rake angle (factor \\(B\\)) of a turning tool on tool life. The two factors are both quantitative; we decide to investigate three levels for each factor, repeating each treatment twice: factorial plan \\(2\\cdot 3^2\\)\n\n\n\npreparation of the test grid\nrandomization of the operative sequence\nexecution of experiments and data collection\nformulation and verification of the statistical model\nanalysis of variance (ANOVA)\ncreation of the response surface\n\nPoints 4 and 5 are possibly repeated\n\n\n\n\n\n\nStdOrder\nAngle\nA\nSpeed\nB\nRepeat\nLife\n\n\n\n\n1\n15\n-1\n125\n-1\n1\nNA\n\n\n2\n20\n0\n125\n-1\n1\nNA\n\n\n3\n25\n1\n125\n-1\n1\nNA\n\n\n4\n15\n-1\n150\n0\n1\nNA\n\n\n5\n20\n0\n150\n0\n1\nNA\n\n\n6\n25\n1\n150\n0\n1\nNA\n\n\n7\n15\n-1\n175\n1\n1\nNA\n\n\n8\n20\n0\n175\n1\n1\nNA\n\n\n9\n25\n1\n175\n1\n1\nNA\n\n\n\n\n\n\n\n\nFor reasons of space, the table only reports the first half of the rows, corresponding to the first repetition"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#example-step-2.",
    "href": "slides/DESANED/5-DoE.html#example-step-2.",
    "title": "Design of Experiments",
    "section": "Example — step 2.",
    "text": "Example — step 2.\n\n\nIn the grid, a new column of randomly ordered integers \\(\\left&lt;1\\dots N\\right&gt;,~N=rl^n\\) is generated\nThe grid is rearranged according to the new column, usually called Run Order\n\n\n\n\n\n\nStdOrder\nRunOrder\nAngle\nA\nSpeed\nB\nRepeat\nLife\n\n\n\n\n4\n1\n15\n-1\n150\n0\n1\nNA\n\n\n5\n2\n20\n0\n150\n0\n1\nNA\n\n\n10\n3\n15\n-1\n125\n-1\n2\nNA\n\n\n2\n4\n20\n0\n125\n-1\n1\nNA\n\n\n12\n5\n25\n1\n125\n-1\n2\nNA\n\n\n15\n6\n25\n1\n150\n0\n2\nNA\n\n\n3\n7\n25\n1\n125\n-1\n1\nNA\n\n\n18\n8\n25\n1\n175\n1\n2\nNA\n\n\n17\n9\n20\n0\n175\n1\n2\nNA\n\n\n14\n10\n20\n0\n150\n0\n2\nNA\n\n\n8\n11\n20\n0\n175\n1\n1\nNA\n\n\n13\n12\n15\n-1\n150\n0\n2\nNA\n\n\n6\n13\n25\n1\n150\n0\n1\nNA\n\n\n1\n14\n15\n-1\n125\n-1\n1\nNA\n\n\n11\n15\n20\n0\n125\n-1\n2\nNA\n\n\n7\n16\n15\n-1\n175\n1\n1\nNA\n\n\n16\n17\n15\n-1\n175\n1\n2\nNA\n\n\n9\n18\n25\n1\n175\n1\n1\nNA"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#example-step-3.",
    "href": "slides/DESANED/5-DoE.html#example-step-3.",
    "title": "Design of Experiments",
    "section": "Example — step 3.",
    "text": "Example — step 3.\n\n\nThe experiments are carried out according to the run order\nBy performing the experiments according to the run order possible unknown and uncontrolled effects are randomly distributed over all treatments:\n\nglobal variance increases (\\(SS_\\mathrm{tot}\\))\nthe relative effect of the factors is not altered (\\(SS_X\\))\n\nNote: a repetition is a replica of the entire experiment, not of the measurement operation alone\n\n\n\n\n\n\nStdOrder\nRunOrder\nAngle\nA\nSpeed\nB\nRepeat\nLife\n\n\n\n\n4\n1\n15\n-1\n150\n0\n1\n-3\n\n\n5\n2\n20\n0\n150\n0\n1\n1\n\n\n10\n3\n15\n-1\n125\n-1\n2\n-1\n\n\n2\n4\n20\n0\n125\n-1\n1\n0\n\n\n12\n5\n25\n1\n125\n-1\n2\n0\n\n\n15\n6\n25\n1\n150\n0\n2\n6\n\n\n3\n7\n25\n1\n125\n-1\n1\n-1\n\n\n18\n8\n25\n1\n175\n1\n2\n-1\n\n\n17\n9\n20\n0\n175\n1\n2\n6\n\n\n14\n10\n20\n0\n150\n0\n2\n3\n\n\n8\n11\n20\n0\n175\n1\n1\n4\n\n\n13\n12\n15\n-1\n150\n0\n2\n0\n\n\n6\n13\n25\n1\n150\n0\n1\n5\n\n\n1\n14\n15\n-1\n125\n-1\n1\n-2\n\n\n11\n15\n20\n0\n125\n-1\n2\n2\n\n\n7\n16\n15\n-1\n175\n1\n1\n2\n\n\n16\n17\n15\n-1\n175\n1\n2\n3\n\n\n9\n18\n25\n1\n175\n1\n1\n0"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#example-step-4.",
    "href": "slides/DESANED/5-DoE.html#example-step-4.",
    "title": "Design of Experiments",
    "section": "Example — step 4.",
    "text": "Example — step 4.\n\n\nWe formulate the full quadratic regression model:\n\\[\\begin{align}\n\\hat y = & \\mu + \\alpha_1 x_1 + \\beta_1 x_2 + (\\alpha\\beta)_1x_1x_2 + \\\\\n          & + \\alpha_2x_1^2 + \\beta_2x_2^2 + (\\alpha\\beta)_{2,1}x_1^2x_2 + \\\\\n          & + (\\alpha\\beta)_{1,2}x_1x_2^2 + (\\alpha\\beta)_{2,2}x_1^2x_2^2\n      \n\\end{align}\\]\nwhere \\((\\alpha\\beta)\\) is not a product: it represents the coefficient of a product of factors \\(A\\) and \\(B\\)\nIt is necessary to evaluate normality and absence of pattern in the residuals\n\n\n\\(\\varepsilon(A)\\)\\(\\varepsilon(B)\\)\\(\\varepsilon(r)\\)Quantile-Quantile"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#example-step-5.",
    "href": "slides/DESANED/5-DoE.html#example-step-5.",
    "title": "Design of Experiments",
    "section": "Example — step 5.",
    "text": "Example — step 5.\n\n\nChoosing 5% as the threshold on the p-value, we observe that these effects are statistically insignificant:\n\n\\(B^2\\), corresponding to the \\(\\beta_2\\) term in the regression\n\\(A^2B\\), corresponding to the \\((\\alpha\\beta)_{2,1}\\) term in the regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neffect\n\\(\\nu\\)\n\\(SS\\)\n\\(MS\\)\n\\(F_0\\)\n\\(p\\mathrm{-value}\\)\n\n\n\n\n\\(A\\)\n1\n8.333333\n8.333333\n5.769231\n0.0397723\n\n\n\\(B\\)\n1\n21.333333\n21.333333\n14.769231\n0.0039479\n\n\n\\(A^2\\)\n1\n16.000000\n16.000000\n11.076923\n0.0088243\n\n\n\\(B^2\\)\n1\n4.000000\n4.000000\n2.769231\n0.1304507\n\n\n\\(AB\\)\n1\n8.000000\n8.000000\n5.538462\n0.0430650\n\n\n\\(A^2B\\)\n1\n2.666667\n2.666667\n1.846154\n0.2073056\n\n\n\\(AB^2\\)\n1\n42.666667\n42.666667\n29.538462\n0.0004137\n\n\n\\(A^2B^2\\)\n1\n8.000000\n8.000000\n5.538462\n0.0430650\n\n\n\\(\\varepsilon\\)\n9\n13.000000\n1.444444\nNA\nNA\n\n\n\n\n\n\nSo the regression equation becomes: \\[\\begin{align}\n\\hat y = & \\mu + \\alpha_1 x_1 + \\beta_1 x_2 + (\\alpha\\beta)_1x_1x_2 + \\\\\n          & + \\alpha_2x_1^2 + (\\alpha\\beta)_{1,2}x_1x_2^2 + \\\\\n          & + (\\alpha\\beta)_{2,2}x_1^2x_2^2\n      \n\\end{align}\\]"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#example-step-6.",
    "href": "slides/DESANED/5-DoE.html#example-step-6.",
    "title": "Design of Experiments",
    "section": "Example — step 6.",
    "text": "Example — step 6.\n\n\nThe response surface allows you to identify notable points and directions:\n\nPoint S is a saddle point, where the gradient is zero in any direction: stable point\nAt point P, the directions tangential to the isohypse are constant yield directions\nPoint M is a maximum of yield\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn general, a response surface is a hyper-surface in an \\(n+1\\)-dimensional space, where \\(n\\) is the number of factors"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#model-adequacy-analysis",
    "href": "slides/DESANED/5-DoE.html#model-adequacy-analysis",
    "title": "Design of Experiments",
    "section": "Model adequacy analysis",
    "text": "Model adequacy analysis\nAfter possibly excluding some effects (for example \\(B^2\\) and \\(A^2B\\)) it is necessary to:\n\nreformulate the model\nanalyze the residuals of the new model\nconfirm with a new ANOVA\n\nIn particular, the analysis of the residuals is called model adequacy check (Model Adequacy Check, MAC) and typically consists of:\n\nchecks for absence of patterns in the residuals depending on the factors\nchecks for absence of patterns in the residuals depending on the test order\nverification of normality of residuals (Q-Q graph and Shapiro-Wilk test)"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#n-factorial-plan",
    "href": "slides/DESANED/5-DoE.html#n-factorial-plan",
    "title": "Design of Experiments",
    "section": "\\(2^n\\) factorial plan",
    "text": "\\(2^n\\) factorial plan\n\n\nFactor plans in which all factors have two levels (low and high, -1 and +1 in coded units) are of particular interest\n\nallow you to regress models only of the first degree\nrequire minimal testing\nthey still allow to define the sensitivity of the process to the various factors, excluding non-significant factors\nare the starting point of any analysis of complex processes (i.e. with more than one explanatory variable)"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#n-factorial-plan-1",
    "href": "slides/DESANED/5-DoE.html#n-factorial-plan-1",
    "title": "Design of Experiments",
    "section": "\\(2^n\\) factorial plan",
    "text": "\\(2^n\\) factorial plan\n\n\n\nEach factor has two levels: low and high. In coded units -1 and +1 are valid, referred to for short as - and +\nThe treatments, i.e. combinations of levels for the \\(n\\) factors, are indicated with Yates notation\nthe design matrix is obtained by permuting all the factors between - and + with frequencies gradually halved\nthe project matrix is repeated for the \\(r\\) repetitions and also reports the random execution order column\nthe effects matrix is obtained by adding the columns for the interactions, calculated as the product of the relative signs\n\n\n\nDesign matrixEffects matrix\n\n\n\n\n\n\n\ntreatment\nrepetition\nA\nB\nC\norder\n\n\n\n\n(1)\n1\n-\n-\n-\n1\n\n\na\n1\n+\n-\n-\n7\n\n\nb\n1\n-\n+\n-\n5\n\n\nab\n1\n+\n+\n-\n6\n\n\nc\n1\n-\n-\n+\n8\n\n\nac\n1\n+\n-\n+\n4\n\n\nbc\n1\n-\n+\n+\n2\n\n\nabc\n1\n+\n+\n+\n3\n\n\n\n\n\n\n\n\n\n\n\n\ntreatment\nI\nA\nB\nAB\nC\nAC\nBC\nABC\n\n\n\n\n(1)\n+\n-\n-\n+\n-\n+\n+\n-\n\n\na\n+\n+\n-\n-\n-\n-\n+\n+\n\n\nb\n+\n-\n+\n-\n-\n+\n-\n+\n\n\nab\n+\n+\n+\n+\n-\n-\n-\n-\n\n\nc\n+\n-\n-\n+\n+\n-\n-\n+\n\n\nac\n+\n+\n-\n-\n+\n+\n-\n-\n\n\nbc\n+\n-\n+\n-\n+\n-\n+\n-\n\n\nabc\n+\n+\n+\n+\n+\n+\n+\n+"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#n-factorial-plan-2",
    "href": "slides/DESANED/5-DoE.html#n-factorial-plan-2",
    "title": "Design of Experiments",
    "section": "\\(2^n\\) factorial plan",
    "text": "\\(2^n\\) factorial plan\nThe effects matrix contains the information to calculate the effects and the quadratic sums \\(SS\\) which, in turn, are used to complete the ANOVA table\n\n\nFor the effects: \\[\n\\begin{align}\nA &= \\frac{-(1)+a-b+ab}{2r} \\\\\nB &= \\frac{-(1)-a+b+ab}{2r} \\\\\nAB &= \\frac{+(1)-a-b+ab}{2r}\n\\end{align}\n\\]\n\nFor quadratic sums: \\[\n\\begin{align}\nSS_A &=& \\frac{(-(1)+a-b+ab)^2}{4r} \\\\\nSS_B &=& \\frac{(-(1)-a+b+ab)^2}{4r} \\\\\nSS_{AB} &=& \\frac{(+(1)-a-b+ab)^2}{4r}\n\\end{align}\n\\]\n\nIn general: \\(\\mathrm{E}(X) = \\frac{2}{2^rn}\\mathrm{Contrast}(X)\\) and \\(\\mathit{SS}(X) = \\frac{1}{2 ^rn}\\mathrm{Contrast}(X)^2\\) where the contrast of the factor \\(\\mathrm{Contrast}(X)\\) is calculated using the signs of the relevant column \\(X\\) for the treatments in the order of Yates (e.g. \\(\\mathrm{Contrast}(AB)=+(1)-a-b+ab\\))"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#n-factorial-plan-statistical-model",
    "href": "slides/DESANED/5-DoE.html#n-factorial-plan-statistical-model",
    "title": "Design of Experiments",
    "section": "\\(2^n\\) factorial plan: statistical model",
    "text": "\\(2^n\\) factorial plan: statistical model\nThe statistical model of a \\(2^n\\) factorial plan is obviously a first-order linear model in all factors. For \\(n=2\\), for example:\n\\[\ny_{ijk} = \\mu + \\alpha_{i} + \\beta_{j} + (\\alpha\\beta)_{ij}+\\varepsilon_{ijk}\n\\] For \\(n=3\\): \\[\ny_{ijkl} = \\mu + \\alpha_{i} + \\beta_{j} + (\\alpha\\beta)_{ij}+ \\gamma_k + (\\alpha\\gamma)_{ik} + (\\beta\\gamma )_{jk} + (\\alpha\\beta\\gamma)_{ijk} + \\varepsilon_{ijkl}\n\\] and so on.\nIn R we will see that these models can be abbreviated as Y~A*B and Y~A*B*C respectively and used to calculate the ANOVA table\n\n\nExercise: derive the regression models corresponding to the two statistical models reported above"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#unreplicated-factorial-designs",
    "href": "slides/DESANED/5-DoE.html#unreplicated-factorial-designs",
    "title": "Design of Experiments",
    "section": "Unreplicated factorial designs",
    "text": "Unreplicated factorial designs\nAs the number of factors increases, the number of individual tests may become unsustainable\nThe simplest way to reduce the number of tests is to avoid repetitions for various treatments\nIf a treatment is not repeated, however, it is not possible to calculate the \\(SS_E\\) and therefore the ANOVA table cannot be completed with the \\(F_0\\) and the p-values\nThe solution was proposed by C. Daniel and is based on the hypothesis that at least one of the factors or interactions is not significant\nThis assumption is usually reasonable for more complex processes with large \\(n\\), precisely the cases where it is particularly important to reduce the number of tests\nThe idea is that non-significant effects are statistics calculated on different sub-samples of the same homogeneous sample, and therefore are normally distributed. Only the significant effects deviate from the normal distribution of the others"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#daniels-method",
    "href": "slides/DESANED/5-DoE.html#daniels-method",
    "title": "Design of Experiments",
    "section": "Daniel’s method",
    "text": "Daniel’s method\n\n\nWhich effects are probably significant can therefore be determined with a quantile-quantile graph of them\nThe graph is a first screening which must be conservative: it only serves to remove effects that are certainly not significant (i.e. very aligned with the diagonal) and allow the execution of the ANOVA\nThe ANOVA table must however be calculated on a reduced linear statistical model, in order to confirm the result of the graphic method or to remove further effects that are actually non-significant"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#daniels-method-1",
    "href": "slides/DESANED/5-DoE.html#daniels-method-1",
    "title": "Design of Experiments",
    "section": "Daniel’s method",
    "text": "Daniel’s method\n\n\nIn this case only the effects \\(A, C, D\\) and the interactions \\(AC\\) and \\(AD\\) are non-normal\nThe linear statistical model can then be reviewed as\n\\[\ny_{ijkl} = \\mu + \\alpha_{i} + \\gamma_{j} + (\\alpha\\gamma)_{ij}+ \\delta_k + (\\alpha\\delta)_{ik} + \\varepsilon_{ijkl}\n\\] That is, we can already rule out that \\(B\\) is in fact a factor. In this way, instead of an unreplicated \\(2^4\\) factorial plan we are dealing with a twice replicated \\(2^3\\) (i.e. \\(2\\cdot 2^3\\)), for which we can perform a normal ANOVA analysis"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#example-1",
    "href": "slides/DESANED/5-DoE.html#example-1",
    "title": "Design of Experiments",
    "section": "Example",
    "text": "Example\n\n\nLet’s consider the data represented in this graph\nIt doesn’t matter where the data comes from: let’s just build a linear model of the data\n\\[ y_{ij} = \\mu + x_i + \\varepsilon_{ij}\\]\nNotice how the residuals do not look normal and that there are some obvious patterns\n\n\nDataResid. vs. xResid. vs. \\(\\hat y\\)Q-Q"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#example-2",
    "href": "slides/DESANED/5-DoE.html#example-2",
    "title": "Design of Experiments",
    "section": "Example",
    "text": "Example\n\n\nIf we observe from the boxplot that the yield tends to increase more than linearly with \\(x\\), we can think of reformulating the model by transforming it through a squaring:\n\\[ y_{ij} = (\\mu + x_i + \\varepsilon_{ij})^2\\] The model no longer seems linear, but considering it can be rewritten as\n\\[ \\sqrt{y_{ij}} = \\mu + x_i + \\varepsilon_{ij}\\]\nit is clear that it is still a linear model in the coefficients.\n\n\nResiduals vs. xResiduals vs. \\(\\hat y\\)Q-Q\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFollowing this transformation it is therefore clear that the new model is more adequate, with pattern-free residuals and more compatible with a normal distribution"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#box-cox-transformations",
    "href": "slides/DESANED/5-DoE.html#box-cox-transformations",
    "title": "Design of Experiments",
    "section": "Box-Cox transformations",
    "text": "Box-Cox transformations\nBox and Cox proposed a method to identify the best transformation in the family of power transformations \\(y^* = y^\\lambda\\), with \\(y^*=\\ln(y)\\) when \\(\\lambda=0\\)\nWe calculate a graph of the logarithmic likelihood \\(\\mathcal{L}\\) (log-likelyhood) of the following \\(y^{(\\lambda)}\\):\n\\[\ny_i^{(\\lambda)} =\n\\begin{cases}\n\\frac{y_i^\\lambda-1}{\\lambda\\dot y^{\\lambda-1}} & \\lambda\\neq 0 \\\\\n\\dot y \\ln y_i & \\lambda = 0\n\\end{cases}, ~~~ \\dot y = \\exp\\left[(1/n)\\sum \\ln y_i\\right],~i=1, 2,\\dots,n\n\\] The likelihood \\(\\ln\\mathcal{L}(\\lambda|y)\\) is nothing more than the probability of extracting a sample \\(y\\) given a certain parameter \\(\\lambda\\). Its maximum coincides with the value of \\(\\lambda\\) which makes the sample \\(y\\) more normal."
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#box-cox-diagram",
    "href": "slides/DESANED/5-DoE.html#box-cox-diagram",
    "title": "Design of Experiments",
    "section": "Box-Cox diagram",
    "text": "Box-Cox diagram\n\n\nThe Box-Cox diagram also identifies an interval corresponding to a variation of less than 95%.\nAny value of \\(\\lambda\\) within this range is statistically equivalent\nYou then choose the value included in the range, which represents a “convenient” transformation\nFor example, if the optimal \\(\\lambda\\) was 0.58, we would still choose \\(\\lambda=0.5\\), which corresponds to the transformation \\(y^*=\\sqrt{y}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhenever a linear model is suspicious (in a regression or in factorial design analysis) it is a good idea to try a Box-Cox transformation. In FPs, sometimes the Box-Cox transformation can simplify the model (i.e. reduce the number of significant factors or interactions)"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#central-composite-design-ccd",
    "href": "slides/DESANED/5-DoE.html#central-composite-design-ccd",
    "title": "Design of Experiments",
    "section": "Central Composite Design (CCD)",
    "text": "Central Composite Design (CCD)\n\n\nIt would be automatic to extend a FP from \\(2^2\\) to \\(2^3\\) to evaluate quadratic effects\nIn this way, however, the sensitivity in the axial directions would be lower than the sensitivity in the diagonal directions, the evaluation interval being smaller in the first case\nIt is therefore preferable to perform centered FPs with rotational symmetry around the origin\nBy two factors, the axial points are extended a distance \\(\\sqrt{2}\\) from the origin; in the generic \\(n\\)-dimensional case the distance is \\((2^k)^{1/4}\\)\n\n\nTwo factorsThree factors"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#fractional-factor-plans-ffp",
    "href": "slides/DESANED/5-DoE.html#fractional-factor-plans-ffp",
    "title": "Design of Experiments",
    "section": "Fractional Factor Plans (FFP)",
    "text": "Fractional Factor Plans (FFP)\n\n\nSuppose we consider only the opposite vertices of the FP in the figure: \\((1), ab, ac, bc\\)\nWe are considering half of the original FP, which however includes all levels of the three factors\nSplitting certainly reduces the completeness of the model, but it saves a lot of testing\n\nHow to choose fractions for larger dimensions?\nWhat information do we lose?"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#choosing-fractions-defining-relationships",
    "href": "slides/DESANED/5-DoE.html#choosing-fractions-defining-relationships",
    "title": "Design of Experiments",
    "section": "Choosing fractions: defining relationships",
    "text": "Choosing fractions: defining relationships\n\n\nLooking at the effects matrix, we observe that the treatments \\((1), ab, ac, bc\\) correspond to the rows for which the relation \\(I=-ABC\\) holds. The other complementary half instead corresponds to \\(I=ABC\\)\nThese relations are called defining relationships because they define the FFP. It makes no difference whether you choose the positive or the negative half\n\n\n\n\n\n\ntreatment\nI\nA\nB\nAB\nC\nAC\nBC\nABC\n\n\n\n\n(1)\n+\n-\n-\n+\n-\n+\n+\n-\n\n\na\n+\n+\n-\n-\n-\n-\n+\n+\n\n\nb\n+\n-\n+\n-\n-\n+\n-\n+\n\n\nab\n+\n+\n+\n+\n-\n-\n-\n-\n\n\nc\n+\n-\n-\n+\n+\n-\n-\n+\n\n\nac\n+\n+\n-\n-\n+\n+\n-\n-\n\n\nbc\n+\n-\n+\n-\n+\n-\n+\n-\n\n\nabc\n+\n+\n+\n+\n+\n+\n+\n+\n\n\n\n\n\n\n\n\nThe effects matrix for a fractional FP \\(2^{n-1}\\) plus the defining relationship uniquely identify a fractional factorial plan"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#alias-and-information-loss",
    "href": "slides/DESANED/5-DoE.html#alias-and-information-loss",
    "title": "Design of Experiments",
    "section": "Alias and information loss",
    "text": "Alias and information loss\nIn a fractional FP \\(2^{3-1}\\) with defining relationship \\(I=ABC\\), we consider these effects: \\[\n\\begin{align}\nA &= (-(1)+a-b+ab-c+ac-bc+abc)/(2r) \\\\\nBC &= (\\underline{+(1)}+a-b\\underline{-ab}-c\\underline{-ac} \\underline{+bc}+abc)/(2r)\n\\end{align}\n\\] Since the underlined treatments are not tested, the \\(A\\) effect is indistinguishable from the \\(BC\\) effect\nIt is said that \\(A\\) is aliased with \\(BC\\)\nGiven a certain defining relation, the possible alias structures can be obtained from the relation itself using a dedicated algebra: \\(I\\cdot X=X\\), \\(X\\cdot X = X\\), \\(X\\cdot Y =XY\\)\nTherefore, it results in \\(A\\cdot I=A\\cdot ABC\\) i.e. \\(A=BC\\), and similarly \\(B=AC\\) and \\(C=AB\\)"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#alias-and-information-loss-1",
    "href": "slides/DESANED/5-DoE.html#alias-and-information-loss-1",
    "title": "Design of Experiments",
    "section": "Alias and information loss",
    "text": "Alias and information loss\nTherefore, by splitting a FP you lose information: you lose the ability to discriminate between aliased effects. It is clear that the longer the defining relationship, the higher the degree of alias interactions with the direct effects will be (e.g. \\(A=BCDEF\\))\nBy virtue of the principle of sparsity of effects, however, this loss of information is not dramatic. The principle says that in a process the significance of high-level interactions is gradually less likely as the number of factors that compose them increases\nConsequently, an alias \\(A=BCDEF\\) can be neglected by assuming the significance of \\(A\\) rather than that of \\(BCDEF\\) by virtue of said Principle"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#fractional-fp-of-type-2n-p",
    "href": "slides/DESANED/5-DoE.html#fractional-fp-of-type-2n-p",
    "title": "Design of Experiments",
    "section": "Fractional FP of type \\(2^{n-p}\\)",
    "text": "Fractional FP of type \\(2^{n-p}\\)\nIt is possible to split a plan more than once, reducing the number of treatments to \\(2^{n-p}\\)\nFor each fraction it is necessary to choose a new defining relationship\nFor example, for \\(2^{7-2}\\) you can choose the DRs \\(I=ABCDE\\) and \\(I=CDEFG\\)\nFor these two DRs there is a third, dependent one: \\(I=ABFG\\). Any two of these three DRs are equivalent\nMinimum aberration design\n\nAll possible \\(p\\)-tuples of defining relations are generated, completing them with the dependent one\nCount the number of letters in the \\((p+1)\\)-tuples (previous example: \\(\\left&lt;5,5,4\\right&gt;\\))\nthe design that minimizes the number of strings with minimum length is preferable because it has fewer aliases"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#minimum-aberration-design-example",
    "href": "slides/DESANED/5-DoE.html#minimum-aberration-design-example",
    "title": "Design of Experiments",
    "section": "Minimum Aberration Design (example)",
    "text": "Minimum Aberration Design (example)\n\n\nCompare these \\(2^{7-2}_{IV}\\) designs:\n\nDesign A\n\n\\(I = ABCF = BCDG~(= ADFG)\\);\n\nDesign B\n\n\\(I = ABCF = ADEG~(= BCDEFG)\\);\n\nDesign C\n\n\\(I=ABCDF=ABDEG~(=CEFG)\\)\n\n\nCarefully select the defining relations!\n\n\n2nd level aliases:\n\n\nDesign A\nDesign B\nDesign C\n\n\n\n\n\\(AB=CF\\)\n\\(AB=CF\\)\n\\(CE=FG\\)\n\n\n\\(AC=BF\\)\n\\(AC=BF\\)\n\\(CF=EG\\)\n\n\n\\(AD=FG\\)\n\\(AD=EG\\)\n\\(CG=EF\\)\n\n\n\\(AG=DF\\)\n\\(AE=DG\\)\n\n\n\n\\(BD=CG\\)\n\\(AF=BC\\)\n\n\n\n\\(BG=CD\\)\n\\(AG=DE\\)\n\n\n\n\\(AF=BC=DG\\)"
  },
  {
    "objectID": "slides/DESANED/5-DoE.html#to-make-a-good-fp",
    "href": "slides/DESANED/5-DoE.html#to-make-a-good-fp",
    "title": "Design of Experiments",
    "section": "To make a good FP",
    "text": "To make a good FP\n\nStart with \\(2^n\\) and then increase it\nEvaluate the opportunity for splitting and carefully choose the defining relationships\nalways perform the model fit check and refine the statistical model accordingly\nDiscuss interactions and effects only after refining the model\nEvaluate the effects of alias\n\nAdvanced themes\n\nBlocking\nMinimum Aberration Design\nModel transformations and Box-Cox transformations"
  },
  {
    "objectID": "slides/DESANED/3-regression.html#basics",
    "href": "slides/DESANED/3-regression.html#basics",
    "title": "Regression",
    "section": "Basics",
    "text": "Basics\n\nA model of a physical system can be expressed as: \\[\n   y=f(x_1, x_2, \\dots, x_n, c_1, c_2, \\dots, c_m)\n\\] where \\(x_i\\) are the physical (random) variables, called regressors or predictors, while the \\(c_i\\) are the parameters (constant) of the model, and \\(y\\) is the response or dependent variable\nIf \\(n=1\\) there is a single predictor and a single dependent variable we talk about simple regression\nFor example, \\(y=a+bx+cx^2\\) is a simple linear model with parameters \\(a,~b,~c\\)\nregressing the model means making measurements of \\(y\\) for different values of \\(x\\) and determining the values of the parameters \\(a,~b,~c\\) that minimize the distance between the model and experimental observations"
  },
  {
    "objectID": "slides/DESANED/3-regression.html#types-of-regression",
    "href": "slides/DESANED/3-regression.html#types-of-regression",
    "title": "Regression",
    "section": "Types of regression",
    "text": "Types of regression\nWe will consider three types of regression:\n\nLinear regression: the model is a linear combination of parameters\nGeneralized linear regression\nLeast squares regression: parameters are combined in a non-linear way"
  },
  {
    "objectID": "slides/DESANED/3-regression.html#basics-1",
    "href": "slides/DESANED/3-regression.html#basics-1",
    "title": "Regression",
    "section": "Basics",
    "text": "Basics\nThe statistical model of a process with \\(n\\) parameters to be regressed is defined as follows: \\[\ny_i=f(x_{1i}, x_{2i}, \\dots, y_{ni}) = \\hat{y_i} + \\varepsilon_{ij},~~~i=1,2,\\dots,N\n\\]\nwhere \\(i\\) is the observation index (\\(N\\geq n+1\\) in total), \\(\\hat{y_i}\\) is the regressed value, or prediction, at the observation \\(i\\), and \\(\\varepsilon_{ij}\\) are the residuals\n\nThe regressed value corresponds to the deterministic component\nThe residuals are the random component\nThe hypothesis of normality of the residuals is assumed, i.e. that \\(\\varepsilon_{ij} \\sim\\mathcal{N}(0, \\sigma^2)\\)\n\n\nIf the \\(f(\\cdot)\\) is an analytical function with one predictor and linear in the coefficients, then we can express it as \\(\\mathbf{A}\\mathbf{k}=\\mathbf{y}\\), where\n\n\\(\\mathbf{y}\\) is the vector of \\(y_i,~i=1\\dots N\\)\n\\(\\mathbf{k}\\) is the vector of parameters \\(c_j,~j=1\\dots n+1\\)\n\\(\\mathbf{A}\\) a matrix \\(N\\times (n+1)\\) composed as follows:\n\n\\[\n\\mathbf A = \\begin{bmatrix}\nx_1^{n-1} & x_1^{n-2} & \\dots & x_1 & 1 \\\\\nx_2^{n-1} & x_2^{n-2} & \\dots & x_2 & 1 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\nx_N^{n-1} & x_N^{n-2} & \\dots & x_N & 1 \\\\\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/DESANED/3-regression.html#basics-2",
    "href": "slides/DESANED/3-regression.html#basics-2",
    "title": "Regression",
    "section": "Basics",
    "text": "Basics\nThe linear equation can be solved with the pseudo-inverse method: \\[\n\\begin{align}\n\\mathbf A^T \\mathbf A\\cdot \\mathbf{k} &= \\mathbf A^T \\cdot \\mathbf{y} \\\\\n(\\mathbf A^T \\mathbf A)^{-1} \\mathbf A^T \\mathbf A\\cdot \\mathbf{k} &= (\\mathbf A^T \\mathbf A)^{-1} \\mathbf A ^T \\cdot \\mathbf{y} \\\\\n\\mathbf{k} &= (\\mathbf A^T \\mathbf A)^{-1} \\mathbf A^T \\cdot \\mathbf{y}\n\\end{align}\n\\] This relationship makes clear what is meant by linear regression: it has nothing to do with the degree of the regressed function (which is not required to be linear in the predictors!), but only with the equation linear in the parameters that represents the model\nNOTE:\n\nfrom the previous matrix equation it is clear that the regression can be performed if and only if \\(N\\geq n+1\\), i.e. if the number of observations is at least equal to the number of parameters\neven in the case of a \\(f(x_1,x_2,\\dots,x_n)\\), if it is linear in the coefficients it is always possible to express it as \\(\\mathbf{A}\\mathbf{k}=\\mathbf{y}\\) and therefore solve it with the pseudo-inverse method"
  },
  {
    "objectID": "slides/DESANED/3-regression.html#example",
    "href": "slides/DESANED/3-regression.html#example",
    "title": "Regression",
    "section": "Example",
    "text": "Example\n\n\nLet the model to be regressed be of the type \\(y_i=(ax_i + b) + \\varepsilon_i\\); then it can be represented as:\n\\[\n\\begin{bmatrix}\nx_1 & 1 \\\\\nx_2 & 1 \\\\\n\\vdots & \\vdots \\\\\nx_N & 1 \\\\\n\\end{bmatrix} \\cdot\n\\begin{bmatrix}\na \\\\\nb\n\\end{bmatrix} =\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_N\n\\end{bmatrix}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nThe figure shows the observations as points with coordinates \\((x_i, y_i)\\), the regressed model as a red line (linear model in the coefficients \\(a,~b\\) and first degree of the predictor \\(x\\)) and the residuals \\(\\varepsilon_i\\) as blue segments representing the difference between the \\(y_i\\) and the corresponding regressed value \\(\\hat{y_i}\\)"
  },
  {
    "objectID": "slides/DESANED/3-regression.html#least-squares-regression-1",
    "href": "slides/DESANED/3-regression.html#least-squares-regression-1",
    "title": "Regression",
    "section": "Least Squares Regression",
    "text": "Least Squares Regression\nThat is, we look for the set of parameter values that minimizes the distance between the model and the observations. This minimization can be achieved by defining an index of merit which represents the distance between the model and observations depending on the parameters: \\[\n\\Phi(c_1, c_2,\\dots,c_m)=\\sum_{i=1}^N \\left(y_i - f(x_{1i},x_{2i},\\dots,x_{ni}, c_1, c_2 ,\\dots,c_m) \\right)^2\n\\]\n\n\nIf the \\(f(\\cdot)\\) is analytic and differentiable, then we can minimize \\(\\Phi(\\cdot)\\) by differentiation, i.e. by solving the system of \\(m\\) equations \\[\n\\frac{\\partial\\Phi}{\\partial c_i}(c_1,c_2,\\dots,c_m) = 0\n\\]\nIf \\(f(\\cdot)\\) is not differentiable, the minimum of \\(\\Phi(\\cdot)\\) can still be calculated numerically (e.g. Newton-Raphson method)"
  },
  {
    "objectID": "slides/DESANED/3-regression.html#coefficient-of-determination",
    "href": "slides/DESANED/3-regression.html#coefficient-of-determination",
    "title": "Regression",
    "section": "Coefficient of Determination",
    "text": "Coefficient of Determination\n\n\n\nThe coefficient of merit most used to evaluate a regression is the coefficient of determination \\(R^2\\)\nIt is defined as \\(R^2 = 1 - \\frac{SS_\\mathrm{res}}{SS_\\mathrm{tot}}\\), where \\(SS_\\mathrm{res} = \\sum \\varepsilon_i^2\\) and \\(SS_ \\mathrm{tot} = \\sum(y_i - \\bar y)^2\\)\nIf the regressed values correspond to the observed values \\(y_i=\\hat{y_i}\\), then the residuals are all zero and \\(R^2 = 1\\)\nThe quality of the regression decreases as \\(R^2\\) decreases\n\n\n\nTwo datasetsSame set, two models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: As you can see, the values of the regressors do not need to be equally spaced!"
  },
  {
    "objectID": "slides/DESANED/3-regression.html#underfitting",
    "href": "slides/DESANED/3-regression.html#underfitting",
    "title": "Regression",
    "section": "Underfitting",
    "text": "Underfitting\n\n\nThere is underfitting when the model has a lower degree than the apparent behavior of the observations\nIt can be highlighted, in addition to a low \\(R^2\\), by studying the distribution of the residuals: if there is under-fitting the residuals can be non-normal and, above all, show trends, or patterns \nA pattern is a regular trend of the residuals as a function of the regressors\nFrom the number of maximums and minimums present in the possible pattern it is possible to estimate how many degrees are missing\n\n\nUnderfittingCorrect fit"
  },
  {
    "objectID": "slides/DESANED/3-regression.html#overfitting",
    "href": "slides/DESANED/3-regression.html#overfitting",
    "title": "Regression",
    "section": "Overfitting",
    "text": "Overfitting\n\n\nIf the degree of the model is excessive, the model tends to chase individual points\nThe value of \\(R^2\\) increases, reaching 1 when the degree equals the number of observations minus 1\nHowever, the model loses generality and is no longer able to correctly predict new values acquired at a later time (the red crosses in the figure)\nOverfitting has particularly dramatic effects in case of extrapolation, i.e. when evaluating the model outside the range into which it has been regressed"
  },
  {
    "objectID": "slides/DESANED/3-regression.html#prediction-bands",
    "href": "slides/DESANED/3-regression.html#prediction-bands",
    "title": "Regression",
    "section": "Prediction Bands",
    "text": "Prediction Bands\n\n\nIt is a band symmetric with respect to the regression within which the observations (present and future) have an assigned probability of falling\nIn general, for a sufficiently large number of observations (\\(&gt;50\\)) the 95% prediction band contains 95% of the observations"
  },
  {
    "objectID": "slides/DESANED/3-regression.html#confidence-bands",
    "href": "slides/DESANED/3-regression.html#confidence-bands",
    "title": "Regression",
    "section": "Confidence Bands",
    "text": "Confidence Bands\n\n\nIt is a band symmetric with respect to the regression within which the expected value of the model has an assigned probability of falling\nIt is always narrower than the prediction band\nIt is the multi-dimensional equivalent of the confidence interval for a T-test: as this is the interval, within which the value corresponding to the null hypothesis has an assigned probability of falling, here we can assume that the “true” model falls within the confidence band with a certain probability\n\n\n\n\n\n\n\n\n\n\n\nIt is obtained by calculating the confidence intervals on the regression parameters, then calculating—for each value of the predictor—the maximum and minimum value of the regression corresponding to the extreme values of the parameters in their confidence intervals"
  },
  {
    "objectID": "slides/DESANED/3-regression.html#basics-3",
    "href": "slides/DESANED/3-regression.html#basics-3",
    "title": "Regression",
    "section": "Basics",
    "text": "Basics\nIn the case of linear regression: \\[\n\\begin{align}\ny_i &= f(\\mathbf{x}_i, \\mathbf{k}) + \\varepsilon_i = \\eta_i + \\varepsilon_i \\\\\n\\varepsilon_i &\\sim \\mathcal{N}(0, \\sigma^2)\n\\end{align}\n\\] In the case of generalized linear regression:\n\\[\n\\begin{align}\ny_i &= \\eta_i + \\varepsilon_i \\\\\n\\varepsilon_i &\\sim D(p_1,p_2,\\dots,p_k)\n\\end{align}\n\\] where \\(D\\) is a generic distribution with \\(k\\) parameters belonging to the family of exponential distributions (normal, binomial, gamma, inverse normal, Poisson, quasinormal, quasibinomial and quasipoissonian)\nThe problem can be solved by introducing a link function that rescales the residuals by projecting them onto a normal distribution"
  },
  {
    "objectID": "slides/DESANED/3-regression.html#basics-4",
    "href": "slides/DESANED/3-regression.html#basics-4",
    "title": "Regression",
    "section": "Basics",
    "text": "Basics\nThe link function \\(g(\\cdot)\\) is such that:\n\\[\n\\begin{align}\ny_i &= \\eta_i + g(\\varepsilon_i) \\\\\n\\varepsilon_i &\\sim D(p_1,p_2,\\dots,p_k);~g(\\varepsilon_i)\\sim \\mathcal{N}(0, \\sigma^2)\n\\end{align}\n\\] The link functions for the most common distributions are:\n\n\n\n\n\nDistribution\nLink function\n\n\n\n\nNormal\n\\(g(x)=x\\)\n\n\nBinomial\n\\(g(x)=\\mathrm{logit}(x)\\)\n\n\nPoisson\n\\(g(x)=\\log(x)\\)\n\n\nRange\n\\(g(x)=1/x\\)\n\n\n\nIn particular, it holds: \\(\\mathrm{logit}(x)=\\frac{1}{1+e^{-p(x-x_0)}}\\)"
  },
  {
    "objectID": "slides/DESANED/3-regression.html#logistic-regression",
    "href": "slides/DESANED/3-regression.html#logistic-regression",
    "title": "Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\n\nThe typical case of logistic regression is the binomial event classifier\nwe consider a process that, depending on one or more predictors, can provide a result that can only be valid for one of two alternatives (success/failure, broken/intact, true/false, 1/0). We want to identify the threshold of predictors that switches the outcome\nA linear regression is not suitable for the situation: it is clear that the residuals are not normal and that the slope of the regression depends a lot on how many points are collected in the “safe” zones\n\n\nDataLinear model residuals"
  },
  {
    "objectID": "slides/DESANED/3-regression.html#logistic-regression-1",
    "href": "slides/DESANED/3-regression.html#logistic-regression-1",
    "title": "Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\n\nThe regressed logistic function provides the parameter \\(x_0\\) which identifies the value that separates an equal amount of false positives and false negatives\nFurthermore, it is possible to identify the appropriate threshold to obtain a predetermined probability of false positives (or false negatives)\nThis is the simplest type of machine learning: a binomial classifier"
  },
  {
    "objectID": "slides/DESANED/3-regression.html#graphic-comparison-of-multiple-series",
    "href": "slides/DESANED/3-regression.html#graphic-comparison-of-multiple-series",
    "title": "Regression",
    "section": "Graphic comparison of multiple series",
    "text": "Graphic comparison of multiple series\n\n\nSuppose we have a process whose value depends on a variable \\(x\\)\nSuppose that a process parameter \\(S\\) can affect the output value. For example:\n\nthe value is the hardness of a metal, \\(x\\) is the temperature, the parameter \\(S\\) is the quantity of an alloying element\nthe value is the productivity of a plant, \\(x\\) is a quantitative process parameter, the parameter \\(S\\) is the type of machine used\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose we repeat 8 times a measurement of the output value for the various combinations of \\(x\\) and \\(S\\), obtaining the results in the figure: which differences are significant?"
  },
  {
    "objectID": "slides/DESANED/3-regression.html#without-a-reference-model",
    "href": "slides/DESANED/3-regression.html#without-a-reference-model",
    "title": "Regression",
    "section": "Without a reference model",
    "text": "Without a reference model\n\n\nWithout a reference model that expresses \\(v=f(x, S)\\) it makes no sense to perform a regression\nHowever, I can report, for each treatment\n\nthe average value, joining the series with a broken line for the sole purpose of visually grouping the data\nthe limits of the confidence interval for each series and for each treatment\nor, join the limits with a band representing confidence about the mean\n\n\n\n\n\n\n\n\n\n\n\n\nAreas where the bands overlap are statistically indistinguishable"
  },
  {
    "objectID": "slides/DESANED/3-regression.html#with-a-model",
    "href": "slides/DESANED/3-regression.html#with-a-model",
    "title": "Regression",
    "section": "With a model",
    "text": "With a model\n\n\nOnly if I have a model \\(v=f(x, S)\\) I can perform a regression\nAlso in this case, the regression must be accompanied with confidence bands\nAgain, areas where the bands overlap are statistically indistinguishable"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#random-or-stochastic-variables",
    "href": "slides/DESANED/1-statistics.html#random-or-stochastic-variables",
    "title": "Intro to Statistics",
    "section": "Random (or stochastic) variables",
    "text": "Random (or stochastic) variables\nA stochastic variable is a variable that takes on random values at each observation, i.e. such that it is not possible to predict the exact value of the next observation, not even knowing the previous observations\n\nmeasurement is the process that leads to the objective evaluation of the measurand. The result of a measurement is called measure\n\n\nStochastic variables are of particular interest for engineering and industry in general, given that each measurement produces, as a result, a value that has a random content and can therefore be represented as a stochastic variable\n\n\nIn turn, the random contribution to a measurement is called uncertainty\n\n\nGiven that every production activity is inextricably linked to measurements, it is therefore clear how fundamental it is to treat random contributions to measurements in a coherent and robust manner"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#scale-effect",
    "href": "slides/DESANED/1-statistics.html#scale-effect",
    "title": "Intro to Statistics",
    "section": "Scale effect",
    "text": "Scale effect"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#scale-effect-1",
    "href": "slides/DESANED/1-statistics.html#scale-effect-1",
    "title": "Intro to Statistics",
    "section": "Scale effect",
    "text": "Scale effect"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#scale-effect-2",
    "href": "slides/DESANED/1-statistics.html#scale-effect-2",
    "title": "Intro to Statistics",
    "section": "Scale effect",
    "text": "Scale effect\nIn other words, the stochastic component of a measurement is affected by a scale effect: the smaller the ratio between the average value measured and the variability typical of the instrument (i.e. its precision), the less appreciable the random effect will be\nIt is therefore essential to precisely and effectively define intuitive concepts such as variability and average value that we have expressed above"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#populations",
    "href": "slides/DESANED/1-statistics.html#populations",
    "title": "Intro to Statistics",
    "section": "Populations",
    "text": "Populations\nIn statistics, a population is a set of values, objects or events of interest for some analysis or experiment.\nTo study the height of residents in the city of Trento, the population of interest is the entire population of Trento.\nTo study the mechanical behavior of the aluminum alloy produced by a certain plant, the population of interest can be the entire quantity of alloy produced by a certain batch of raw material\nBut to also study the effects of variability in raw materials (between one batch and another), environmental conditions, etc., it would be more appropriate to define a larger set as a population\nTherefore, the definition of the population of interest is often arbitrary"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#populations-1",
    "href": "slides/DESANED/1-statistics.html#populations-1",
    "title": "Intro to Statistics",
    "section": "Populations",
    "text": "Populations\nSo:\n\nthe definition of the population depends on the objective of the analysis\nthe size of a population is generally very large and potentially not limited\nconsequently it is often impractical to consider the entire population\nwe then work on subsets randomly extracted from the population, called samples\nbeing randomly drawn, the samples approximate the population, the more numerous they are, the better"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#population",
    "href": "slides/DESANED/1-statistics.html#population",
    "title": "Intro to Statistics",
    "section": "Population",
    "text": "Population\nBy observing a population we can identify a central value and a variability"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#moments-of-a-population",
    "href": "slides/DESANED/1-statistics.html#moments-of-a-population",
    "title": "Intro to Statistics",
    "section": "Moments of a population",
    "text": "Moments of a population\nThe central value is called expected value and the variability is called variance\n\nExpected value:\n\nfor discrete r.v.: \\(\\mu = E(x) := \\sum_{i = 1}^N x_i p(x_i)\\)\nfor continuous r.v.: \\(\\mu = E(x) := \\int_{-\\infty}^{+\\infty} x f(x)~\\mathrm{d}x\\)\n\nVariance:\n\nfor discrete r.v.: \\(\\sigma^2 = V(x) := \\sum_{i = 1}^N (x_i -\\mu)^2 p(x_i)\\)\nfor continuous r.v.: \\(\\sigma^2 = V(x) := \\int_{-\\infty}^{+\\infty} (x - \\mu)^2 f(x)~\\mathrm{d}x\\)\n\n\nwhere \\(E()\\) and \\(V()\\) are the expected value and variance operators, respectively; \\(x_i\\) (and \\(x\\)) is the generic observation of the r.v., and \\(p(x_i)\\) and \\(f(x)\\) are the probability and probability density of finding a given value\nThe properties of a population are indicated with Greek letters: \\(\\mu\\) and \\(\\sigma^2\\)\nNOTE: This results in \\(\\sigma^2 = E\\left[(x-\\mu)^2\\right]\\)"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#probability-and-probability-density",
    "href": "slides/DESANED/1-statistics.html#probability-and-probability-density",
    "title": "Intro to Statistics",
    "section": "Probability and probability density",
    "text": "Probability and probability density\n\nProbability (or frequency): for a discrete r.v., corresponds to the ratio between the number of observations of a given value and the total number of observations\nProbability density: for a continuous r.v., the probability of exactly finding a given value is zero, therefore we are referring to a probability of finding a value within a given interval. The probability density is the derivative of this value\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCumulative probability (Cumulated Distribution Function): \\(p(x_0) = P(x\\leq x_0)\\)\nProbability Density (Probability Density Function): \\(f(x) = \\frac{d}{dx} p(x)\\)\nAlso valid is \\(p(x_0) = \\int_{-\\infty}^{x_0}f(x) \\mathrm d x\\)"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#probability-and-probability-density-1",
    "href": "slides/DESANED/1-statistics.html#probability-and-probability-density-1",
    "title": "Intro to Statistics",
    "section": "Probability and probability density",
    "text": "Probability and probability density\nAlso, note that probability and frequency must add to 1: respectively: \\[\n\\begin{array}{l} \\sum_i p(x_i) = 1 \\\\\n\\int_{-\\infty}^\\infty f(x)~\\mathrm{d}x = 1 \\end{array}\n\\]\nThis is because obviously the probability of finding any value must be certain, i.e. 1"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#operator-properties",
    "href": "slides/DESANED/1-statistics.html#operator-properties",
    "title": "Intro to Statistics",
    "section": "Operator properties",
    "text": "Operator properties\nThe expected value and variance operators have the following properties:\n\\[\n\\begin{array}{l}\nE(c)&=&c\\\\\nE(x)&=&\\mu\\\\\nE(cx)&=&cE(x)=c\\mu\\\\\nV(c)&=&0\\\\\nV(x)&=&\\sigma^2\\\\\nV(cx)&=&c^2V(x)=c^2\\sigma^2\\\\\nE(x+y)&=&E(x)+E(y)=\\mu_x+\\mu_y\\\\\n\\mathrm{Cov}(x,y)&=&E[(x-\\mu_x)(y-\\mu_y)] \\label{eq:cov}\\\\\nV(x+y)&=&V(x)+V(y)+2\\textrm{ Cov}(x,y)\\\\\nV(x-y)&=&V(x)+V(y)-2\\textrm{ Cov}(x,y)\n\\end{array}\n\\]\nwhere \\(c\\) indicates a constant"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#correlation-and-covariance",
    "href": "slides/DESANED/1-statistics.html#correlation-and-covariance",
    "title": "Intro to Statistics",
    "section": "Correlation and covariance",
    "text": "Correlation and covariance\nThe covariance operator is an index of how interdependent two stochastic variables are\nMore useful than covariance (which is not limited) is correlation which has the advantage of being within the range \\([-1,1]\\):\n\\[\n\\mathrm{Corr}(x, y) = \\frac{E[(x-\\mu_x)(y-\\mu_y)]}{\\sigma_x\\sigma_y} = \\frac{\\mathrm{Cov}(x,y) }{\\sigma_x\\sigma_y}\n\\]\n\nclose to zero means no correlation\nclose to 1 means strong positive correlation (if \\(x\\) increases, \\(y\\) also increases)\nclose to -1 means strong negative correlation (if \\(x\\) increases, \\(y\\) decreases).\n\nCovariance and correlation are also referred to as \\(\\sigma_{xy}^2\\) and \\(\\rho_{xy}\\), respectively."
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#samples",
    "href": "slides/DESANED/1-statistics.html#samples",
    "title": "Intro to Statistics",
    "section": "Samples",
    "text": "Samples\nA population may be too large to be analyzed directly\nThen we analyze the subsets obtained by sampling, i.e. random extraction\nRandomly drawing a large enough sample does not alter the distribution properties of the population\nFrom a population of \\(N\\) elements it is possible to extract a number of different samples of size \\(n\\) described by the binomial coefficient: \\[\\binom{N}{n}=\\frac{N! }{(N-n)!n!},~N&gt;n\\]"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#estimators",
    "href": "slides/DESANED/1-statistics.html#estimators",
    "title": "Intro to Statistics",
    "section": "Estimators",
    "text": "Estimators\nFor each property of the population it is possible to define an estimator, or statistic, built on the sample\nSample mean and variance are defined\n\\[\n\\begin{eqnarray} \\bar x &=& \\frac{1}{n}\\sum_{i=1}^n x_i\\\\\nS^2 &=& \\frac{\\sum_{i=1}^n (x_i - \\bar x)^2}{n-1}\n\\end{eqnarray}\n\\]\nA particular value taken by an estimator is called estimate\nInstead of the variance \\(S^2\\) the standard deviation \\(S\\) is often used (same units of measurement)"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#estimators-1",
    "href": "slides/DESANED/1-statistics.html#estimators-1",
    "title": "Intro to Statistics",
    "section": "Estimators",
    "text": "Estimators\nFor each property of the population it is possible to define an estimator, or statistic, built on the sample\nSample mean and variance are defined\n\\[\n\\begin{eqnarray} \\bar x &=& \\frac{1}{n}\\sum_{i=1}^n x_i\\\\\nS^2 &=& \\frac{\\sum_{i=1}^n (x_i - \\bar x)^2}{n-1}\n\\end{eqnarray}\n\\]\nA particular value taken by an estimator is called estimate\nInstead of the variance \\(S^2\\) the standard deviation \\(S\\) is often used (same units of measurement)"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#estimates",
    "href": "slides/DESANED/1-statistics.html#estimates",
    "title": "Intro to Statistics",
    "section": "Estimates",
    "text": "Estimates\n\n\nSince each sample is drawn randomly, each estimate is a random variable\nThe larger the sample, the closer the estimate is to the corresponding property: convergence in distribution"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#expected-value-and-variance-of-the-sample-mean",
    "href": "slides/DESANED/1-statistics.html#expected-value-and-variance-of-the-sample-mean",
    "title": "Intro to Statistics",
    "section": "Expected value and variance of the sample mean",
    "text": "Expected value and variance of the sample mean\nExpected value of the average:\n\\[\n\\begin{eqnarray} \\mathrm E(\\bar x) &=& \\mathrm E(\\frac{x_1+x_2+\\dots+x_n}{n}) = \\frac{1}{n}\\left[\\mathrm E (x_1+x_2+\\dots+x_n) \\right]\\\\\n&=& \\frac{1}{n}\\left[\\mathrm E(x_1)+\\mathrm E(x_2)+\\dots+\\mathrm E(x_n) \\right] = \\frac{1}{n} n\\mathrm E(x) \\\\\n\\mathrm E(\\bar x)&=& \\mu\n\\end{eqnarray}\n\\]\nVariance of the mean:\n\\[\n\\begin{eqnarray}\n\\mathrm V(\\bar x) &=& \\mathrm V(\\frac{x_1+x_2+\\dots+x_n}{n}) = \\frac{1}{n^2}\\left[\\mathrm V(x_1+ x_2+\\dots+x_n) \\right]\\\\\n&=& \\frac{1}{n^2}\\left[\\mathrm V(x_1)+\\mathrm V(x_2)+\\dots+\\mathrm V(x_n) \\right] = \\frac{n\\mathrm V( x)}{n^2} = \\frac{\\mathrm V(x)}{n} \\\\\n\\mathrm V(\\bar x) &=& \\frac{\\sigma^2}{n}\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#degrees-of-freedom",
    "href": "slides/DESANED/1-statistics.html#degrees-of-freedom",
    "title": "Intro to Statistics",
    "section": "Degrees of freedom",
    "text": "Degrees of freedom\nThe degrees of freedom of a statistic (GdF or DoF) are the number of independent elements that appear in its definition. From the definition of the variance it follows that\n\\[\n\\sigma^2=E\\left(\\frac{\\sum(x_i - \\bar x)^2}{n-1}\\right)=E\\left(\\frac{SS}{\\nu}\\right)\n\\]\nThat is, the variance is the expected value of the Sum of Squares divided by its number of degrees of freedom \\(\\nu\\), i.e. of independent elements.\nThat the latter are \\(n-1\\) is demonstrated by the following relation: \\[\n\\sum_{i=1}^n(x_i-\\bar x) = \\sum_{i=1}^n(x_i)-n\\bar x=:0\n\\] therefore not all the \\(n\\) elements in the definition of \\(SS\\) can be independent, given that the value of one of them is predictable from the remaining \\(n-1\\) thanks to the definition of \\(\\bar x\\)"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#distributions-1",
    "href": "slides/DESANED/1-statistics.html#distributions-1",
    "title": "Intro to Statistics",
    "section": "Distributions",
    "text": "Distributions\n\n\nDiscrete distributions\n\n\n\n\n\n\n\n\n\n\nContinuous distributions"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#binomial-or-bernoulli-distribution",
    "href": "slides/DESANED/1-statistics.html#binomial-or-bernoulli-distribution",
    "title": "Intro to Statistics",
    "section": "Binomial or Bernoulli distribution",
    "text": "Binomial or Bernoulli distribution\n\n\n\nDescriptionDefinitionsExamples\n\n\nA Bernoulli process is a series of \\(n\\) events with outcomes \\(z_1, z_2, \\dots, z_n\\) such that:\n\nevents \\(z_i\\) are all independent\neach \\(z_i\\) can be represented with 0 or 1\nthe probability of success \\(p_s\\in[0,1]\\) of each event is constant\n\nThe binomial distribution describes the probability of getting \\(x\\in[0, n]\\) successes (out of \\(n\\) trials)\n\n\n\nWe say that \\(x\\sim\\mathrm{Binom}(n,p_s)\\) or even \\(x\\sim\\mathcal{B}(n,p_s)\\) when the PDF is: \\[\np(x)=\\binom{n}{x}p_s^x(1-p_s)^{n-x},~~~x\\in{0,1\\dots,n}\n\\]\nMoments: \\[\n\\mu=np_s,~~~\\sigma^2=np_s(1-p_s)\n\\]\n\n\n\n\nprobability of getting 8 heads by flipping a coin 10 times\nprobability of finding 3 defective items in a batch of 20, with a defect rate of 0.1\nprobability of 2 sixes in 10 dice rolls"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#poisson-distribution",
    "href": "slides/DESANED/1-statistics.html#poisson-distribution",
    "title": "Intro to Statistics",
    "section": "Poisson distribution",
    "text": "Poisson distribution\n\n\n\nDescriptionDefinitionsExamples\n\n\n\nProbability of having a number \\(x\\in\\mathbb{N}^+\\) of events that occur successively and independently in a given interval of time (or space…)\nOn average \\(\\lambda\\in\\mathbb{R}^+\\) events occur in the same interval\nIt is also known as law of rare events\n\n\n\n\nWe say that \\(x\\sim\\mathrm{Poisson}(\\lambda)\\) or \\(x\\sim\\mathcal{P}(\\lambda)\\) when the PDF is: \\[\np(x)=\\frac{e^{-\\lambda}\\lambda^x}{x!},~~~\\forall x\\in\\mathbb{N}^+\n\\]\nMoments: \\[\n\\mu=\\lambda,~~~\\sigma^2=\\lambda\n\\]\n\n\n\n\nprobability of finding a defect on 1 m of wire, when there are on average 9 defects every 100 m: \\(\\lambda=\\frac{9~\\text{def}}{100~\\text m} * 1~\\text m = 0.09~\\text{def}\\)\nprobability of receiving a phone call in the next 10 min in a switchboard which receives on average 250 calls per day: \\(\\lambda=\\frac{250~\\text{calls}}{24~\\text{h}} * 1/6~\\text{h} \\simeq 1.74~\\text{calls}\\)\nprobability of incoming patient in a hospital with an average of 1825 patients per year: \\(\\lambda=1825~\\text{patients}/365~\\text{days} \\simeq 5~\\text{patients}\\) (see chart on the right)"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#geometric-distribution",
    "href": "slides/DESANED/1-statistics.html#geometric-distribution",
    "title": "Intro to Statistics",
    "section": "Geometric distribution",
    "text": "Geometric distribution\n\n\n\nDescriptionDefinitionsExamples\n\n\nIt is the probability distribution of obtaining a success in a Bernoulli process after \\(x \\in \\mathbb{N}^+\\) failures (so, when \\(x=0\\) we have a success on first trial and \\(p(0)=p_s\\))\n\n\n\nIt is said that \\(x\\sim\\mathrm{Geom}(p_s)\\) or \\(x\\sim\\mathcal{G}(p_s)\\) when the PDF is: \\[\np(x)=p_s(1-p_s)^{x-1},~~~x \\in\\mathbb{N}^+\n\\]\nMoments: \\[\n\\mu=(1-p_s)/p_s,~~~\\sigma^2=(1-p_s)/p_s^2\n\\]\n\n\n\n\nThe probability of getting a sequence of ten tails in sequence (and then a head) by flipping a coin"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#uniform-distribution",
    "href": "slides/DESANED/1-statistics.html#uniform-distribution",
    "title": "Intro to Statistics",
    "section": "Uniform distribution",
    "text": "Uniform distribution\n\n\n\nDescriptionDefinitionsExamples\n\n\n\ndistribution in which all the values of the r.v. they have the same probability\ncan be both discrete and continuous\n\n\n\n\nWe say \\(x\\sim\\mathcal{U}(a,b)\\) when the PDF is: \\[\nf(x)=\\begin{cases} 1/(b-a),&x\\in[a, b] \\\\\n0,&\\textrm{otherwise} \\end{cases}\n\\]\nMoments: \\[\n\\mu=(b+a)/2,~~~\\sigma^2=\\frac{(b-a)^2}{12}\n\\]\n\n\n\n\nRolling a 6-sided die (fair)\nThe extraction of a bingo number (discrete)\nThe stopping angle of a freely rotating wheel (continuous)"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#normal-or-gaussian-distribution",
    "href": "slides/DESANED/1-statistics.html#normal-or-gaussian-distribution",
    "title": "Intro to Statistics",
    "section": "Normal or Gaussian distribution",
    "text": "Normal or Gaussian distribution\n\n\n\nDescriptionDefinitionsNote\n\n\n\nRepresents the case in which the probability of finding values that are progressively further from the expected value decreases asymptotically to 0\nThe probability of any value is never zero\n\n\n\n\nWe say that \\(x\\sim\\mathrm{Norm}(\\mu,\\sigma^2)\\) or \\(x\\sim\\mathcal{N}(\\mu,\\sigma^2)\\) when the PDF is: \\[\n   f(x) =\\frac{1}{\\sigma \\sqrt{2 \\pi}}e^{-\\frac{1}{2}\\left[\\frac{x-\\mu}{\\sigma}\\right ]^2}\n\\]\nMoments: coincide with the two parameters \\(\\mu\\) and \\(\\sigma^2\\)\n\n\n\nIf \\(x\\sim\\mathcal{N}(\\mu, \\sigma^2)\\) then \\[\n\\frac{x-\\mu}{\\sigma}\\sim\\mathcal{N}(0,1)\n\\] and the distribution \\(\\mathcal{N}(0,1)\\) is called standard normal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is a distribution that plays a central role, by virtue of the central limit theorem"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#central-limit-theorem-statement",
    "href": "slides/DESANED/1-statistics.html#central-limit-theorem-statement",
    "title": "Intro to Statistics",
    "section": "Central limit theorem (statement)",
    "text": "Central limit theorem (statement)\n\nTheorem 1 (Celtral limit) If \\(x_1, x_2, \\dots,x_n\\) are \\(n\\) independent and identically distributed (IID) random variables with \\(E(x_i)=\\mu\\) and \\(V(x_i)=\\sigma^2~ ~\\forall i=1,2,\\dots,n\\) (both finite), and \\(y=x_1+x_2+\\dots+x_n\\), then: \\[\nz_n=\\frac{y-n\\mu}{\\sqrt{n\\sigma^2}}\n\\] approximates a distribution \\(\\mathcal{N}(0,1)\\), in the sense that if \\(F_n(z)\\) is the distribution function of \\(z_n\\) and \\(\\Phi(z)\\) is the distribution function of \\(\\mathcal{N}(0,1)\\), then: \\[\n\\lim_{n\\rightarrow+\\infty}\\frac{F_n(z)}{\\Phi(z)}=1\n\\]"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#central-limit-theorem-meaning",
    "href": "slides/DESANED/1-statistics.html#central-limit-theorem-meaning",
    "title": "Intro to Statistics",
    "section": "Central limit theorem (meaning)",
    "text": "Central limit theorem (meaning)\n\nFundamental in the field of measurements:\n\na measurement is the sum of a series of events\neach event can have an unknown distribution\nby adding many distributions the result converges to the normal\ntherefore the result of a measurement is often normal\n\nConvergence is often very rapid (about ten elements)\nNote: adding or multiplying a distribution by a constant changes its moments but the distribution remains the same. Instead, operations between r.v. change the resulting distribution!"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#chi-square-distribution",
    "href": "slides/DESANED/1-statistics.html#chi-square-distribution",
    "title": "Intro to Statistics",
    "section": "Chi-square distribution",
    "text": "Chi-square distribution\n\n\n\nDescriptionDefinitionsNote\n\n\n\nIt is the distribution of a sum of standard normal distributions\nThat is, let \\(x = z_1^2+z_2^2+\\dots+z_k^2\\), with \\(z_i\\sim\\mathcal{N}(0,1)~~\\forall i=1, 2, \\dots ,k\\), then the distribution of \\(x\\) is a Chi-square\nThe number of summed normals \\(k\\) is the number of degrees of freedom of the distribution\n\n\n\n\nWe say that \\(x\\sim\\chi^2_k\\) when the PDF is: \\[\n   f(x)=\\frac{1}{2^{k/2}\\Gamma(k/2)}x^{k/2-1}e^{-y/2}\n\\]\nMoments: \\[\n   \\mu=k,~~~\\sigma^2=2k\n\\]\n\n\n\nConsidering the quadratic sum of a sample of \\(k\\) elements \\(y_i\\), each coming from a distribution \\(\\mathcal{N}(\\mu, \\sigma^2)\\), it turns out that: \\[\n\\frac{(y_i-\\bar y)}{\\sigma}\\sim \\mathcal{N}(0,1)~~\\forall i=1,2,\\dots,k\n\\] and therefore: \\[\n\\frac{\\mathit{SS}}{\\sigma^2}=\\frac{\\sum_{i=1}^k(y_i-\\bar y)^2}{\\sigma^2} \\sim \\mathcal{X }^2_{k-1}\n\\]"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#students-t-distribution",
    "href": "slides/DESANED/1-statistics.html#students-t-distribution",
    "title": "Intro to Statistics",
    "section": "Student’s T distribution",
    "text": "Student’s T distribution\n\n\n\nDescriptionDefinitionsNote\n\n\nLet there be two r.v. \\[\nz\\sim\\mathcal{N}(0,1),~x\\sim\\mathcal{X}^2_k\n\\] then their combination \\[\nx=\\frac{z}{\\sqrt{x/k}}\n\\] is distributed as a Student’s T\n\n\n\nIt is said that \\(x\\sim\\mathrm{T}_k\\) or \\(x\\sim\\mathcal{T}_k\\) when the PDF is: \\[\nf(x)=\\frac{\\Gamma\\left((k-1)/2\\right)}{\\sqrt{k\\pi}\\Gamma(k/2)}\\frac{1}{((x^ 2/k)+1)^{(k+1)/2}}\n\\]\nMoments: \\[\n\\mu = 0,~~~ \\sigma^2=k/(k-2)\n\\]\n\n\n\nStudent’s T is a special case of \\(\\mathcal{N}(0,1)\\): \\[\n\\lim_{k\\rightarrow+\\infty}t_k=\\mathcal{N}(0,1)\n\\] The convergence is very rapid: already for \\(k&gt;30\\) the difference between the two distribution functions becomes negligible\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe figure compares Student’s T with the standard normal (dashed)"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#distribution-f-by-snedecor",
    "href": "slides/DESANED/1-statistics.html#distribution-f-by-snedecor",
    "title": "Intro to Statistics",
    "section": "Distribution F by Snedecor",
    "text": "Distribution F by Snedecor\n\n\n\nDescriptionDefinitions\n\n\nLet there be two r.v. \\[\nx_u\\sim\\mathcal{X}^2_u,~~~x_v\\sim\\mathcal{X}^2_v\n\\] and \\(x\\) is defined as: \\[x=\\frac{x_u/u}{x_v/v}\\] then \\(x\\) is a r.v. distributed as an F of Snedecor\n\n\n\nIt is said that \\(x\\sim\\mathrm{F}_{u,v}\\) or \\(x\\sim\\mathcal{F}_{u,v}\\) when the PDF is: \\[\nf(x)=\\frac{\\Gamma\\left(\\frac{u+v}{2}\\right)\\left(\\frac{u}{v}\\right)^{u/2}x^{( u/2)-1}}{\\Gamma\\left( \\frac{u}{2} \\right)\\Gamma\\left( \\frac{v}{2} \\right) \\left(\\frac{u}{ v}x+1\\right)^{(u+v)/2}}\n\\]\nMoments: \\[\n\\mu = \\frac{v}{v-2},~~~\\sigma^2=\\frac{2v^2(u+v-2)}{u(v-2)^2(v-4) }\n\\]"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#distribution-functions",
    "href": "slides/DESANED/1-statistics.html#distribution-functions",
    "title": "Intro to Statistics",
    "section": "Distribution functions",
    "text": "Distribution functions\n\n\nA distribution is described by three interrelated functions:\n\ndistribution density function, PDF\ncumulative distribution function, CDF, is the progressive integral of the PDF: \\[\n   \\begin{array}{rcl}\n   \\mathrm{CDF}^-(x) &=& \\int_{-\\infty}^x \\mathrm{PDF}(x)~\\mathrm{d}x \\\\\n   \\mathrm{CDF}^+(x) &=& \\int^{+\\infty}_x \\mathrm{PDF}(x)~\\mathrm{d}x\n   \\end{array}\n   \\]\nquantile function, is the inverse of the CDF; it is defined only in \\([0,1]\\)\n\n\n\nPDFCDFQuantile"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#statistical-hypotheses",
    "href": "slides/DESANED/1-statistics.html#statistical-hypotheses",
    "title": "Intro to Statistics",
    "section": "Statistical hypotheses",
    "text": "Statistical hypotheses\n\nIt has been seen that the sample mean and variance are two estimators and represent random variables\nSo by taking two samples from a population the two estimates of mean and variance will always be different\nHow do I know if two samples with different means come from the same population?\nI can formulate a pair of alternative hypotheses: \\[\n\\begin{eqnarray}\nH_0:~&\\mu_1 = \\mu_2 \\\\\nH_1:~&\\mu_1 \\neq \\mu_2 \\\\\n\\end{eqnarray}\n\\]\n\n\n\n\\(H_0\\) is called the null hypothesis, or weak; \\(H_1\\) is called the alternative hypothesis, or strong"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#confusion-matrix",
    "href": "slides/DESANED/1-statistics.html#confusion-matrix",
    "title": "Intro to Statistics",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n\n\nA hypothesis test can have two types of errors:\n\nType I error: false positive, or false alarm\nType II error: false negative, or no alarm\n\nThe purpose of inferential statistics is to associate a probability with these errors\n\n\n\n\n\n\nNull hypothesis\ntrue\nfalse\n\n\n\n\naccepted\nOK\nType II error\n\n\nrejected\nType I error\nOK\n\n\n\n\n\n\nThe probability of a Type I Error is \\(\\alpha\\), the probability of a Type II Error is \\(\\beta\\).\nThe value \\(1-\\beta\\) is the power \\(P\\) of the test\n\n\nNote: \\(\\alpha \\neq 1-\\beta\\)"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#student-test",
    "href": "slides/DESANED/1-statistics.html#student-test",
    "title": "Intro to Statistics",
    "section": "Student Test",
    "text": "Student Test\n\n\n\n\n\nWilliam S. Gosset\n\n\n\n\nWilliam S. Gosset, known as Student, Guinness brewmaster in Dublin, ~1900\nProblem: how to decide if two different averages on process samples indicate two different processes?\nLet the two normal and independent samples be \\(y_{1,i},~i=1, 2, \\dots, n_1\\) and \\(y_{2,i},~i=1, 2, \\dots , n_2\\), will obviously be \\(\\bar{y_1}\\neq\\bar{y_2}\\), so how can we choose between \\(H_0\\) or \\(H_1\\)?\n\n\nObviously the answer is probabilistic: I can only associate an error probability \\(\\alpha\\) to the hypothesis test \\[\n\\begin{eqnarray}\nH_0:~&\\mu_1 = \\mu_2 \\\\\nH_1:~&\\mu_1 \\neq \\mu_2 \\\\\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#student-test-1",
    "href": "slides/DESANED/1-statistics.html#student-test-1",
    "title": "Intro to Statistics",
    "section": "Student Test",
    "text": "Student Test\n\n\n\nSamples C1 and C3 will not have common values: it is very likely that they come from different populations\nSamples C1 and C2, however, are more difficult to distinguish\nIntuitively, \\(H_1\\) is more probable the further apart the means are and the narrower the variances\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the graph, the vertical dashes at the bottom represent the values of 10 random samples drawn from the respective populations"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#student-test-2",
    "href": "slides/DESANED/1-statistics.html#student-test-2",
    "title": "Intro to Statistics",
    "section": "Student Test",
    "text": "Student Test\nFor the two samples \\(y_{1,i}\\) and \\(y_{2,i}\\) it is possible to define the variable: \\[\nt_0 = \\frac{\\bar{y_2} - \\bar{y_1}}{S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_s} }}\n\\] where \\(S_p^2\\) is called pooled variance and is: \\[\nS_p^2 = \\frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1+n_2-2}\n\\] The \\(t_0\\) is the ratio between a r.v. normal to the numerator and a r.v. \\(\\chi^2\\) in the denominator. Consequently it is itself a r.v. and is defined as a Student’s T.\n\nThe number of degrees of freedom is the same as \\(\\chi^2\\) and is \\(n_1+n_2-2\\)\n\\(t_0\\) is called test statistic and it holds that \\(t_0\\sim t_{n_1+n_2-2}\\)"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#student-test-3",
    "href": "slides/DESANED/1-statistics.html#student-test-3",
    "title": "Intro to Statistics",
    "section": "Student Test",
    "text": "Student Test\n\nIt is clear that cases like C1 vs. C3 in the previous graph will have a higher value of \\(|t_0|\\) than cases like C1 vs. C2 (relationship between distance and variance)\nBut given a pair of samples from the same population the probability of high values of \\(|t_0|\\) is very low\nSince we know the distribution of \\(t_0\\) we can therefore calculate the probability of finding a given value assuming that \\(H_1\\) is valid\nIn other words, the probability of rejecting \\(H_0\\) when it is true is equal to the probability of finding a value equal to or greater than \\(t_0\\) in the Student distribution\nIn reality, the sign of \\(t_0\\) is arbitrary, so we need to check the probability of finding a value outside the interval \\([-|t_0|, |t_0|]\\)"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#student-test-4",
    "href": "slides/DESANED/1-statistics.html#student-test-4",
    "title": "Intro to Statistics",
    "section": "Student Test",
    "text": "Student Test\nUltimately, the probability of a type I error in the Student test is: \\[\np\\mathrm{-value} = 2\\mathrm{CDF}^+_t(|t_0|,n_1+n_2-2)\n\\] where \\(\\mathrm{CDF}^{+}_{t}\\) is the cumulative distribution, upper tail, of a Student’s T with \\(n_1+n_2-2\\) DoF, and where the factor 2 takes into account the last point on the previous page\nThe probability of error of any statistical test is called p-value\n\nThe smaller the p-value, the more likely we are to reject \\(H_0\\)\nTypically, \\(H_0\\) is rejected when the p-value becomes less than 5%"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#student-test-5",
    "href": "slides/DESANED/1-statistics.html#student-test-5",
    "title": "Intro to Statistics",
    "section": "Student Test",
    "text": "Student Test\nOften in statistical tests \\(H_0\\) is accepted or rejected on the basis of a Type I error probability threshold, denoted by \\(\\alpha\\)\n\nAn \\(\\alpha\\) is set a priori based on the risk associated with the test\nCalculate the value of \\(t_{0,\\mathrm{max}}\\) which corresponds to \\(\\alpha\\)\nIf \\(|t_0| \\geq t_{0,\\mathrm{max}}\\) rejects \\(H_0\\) with an error probability less than \\(\\alpha\\)\n\nThe threshold value is calculated using the quantile function and \\(H_0\\) is rejected when: \\[\n|t_0| \\geq t_{0,\\mathrm{max}} = t_{\\alpha/2, n_1+n_2-2}\n\\] where \\(t_{\\alpha/2, n_1+n_2-2}\\) is precisely the quantile function, upper tail, of the Student’s T evaluated for the probability \\(\\alpha/2\\) and for \\(n_1+n_2-2\\) DoF."
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#student-test-6",
    "href": "slides/DESANED/1-statistics.html#student-test-6",
    "title": "Intro to Statistics",
    "section": "Student Test",
    "text": "Student Test"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#quantile-table",
    "href": "slides/DESANED/1-statistics.html#quantile-table",
    "title": "Intro to Statistics",
    "section": "Quantile table",
    "text": "Quantile table\nIn the absence of calculators, the Student Test was performed by predetermining \\(\\alpha\\) and deciding whether to reject \\(H_0\\) based on the quantile table:\n\n\n\n\n\n\ndof\n0.1\n0.05\n0.025\n0.01\n0.005\n0.0025\n0.001\n\n\n\n\n1\n3.078\n6.314\n12.706\n31.821\n63.657\n127.321\n318.309\n\n\n2\n1.886\n2.920\n4.303\n6.965\n9.925\n14.089\n22.327\n\n\n3\n1.638\n2.353\n3.182\n4.541\n5.841\n7.453\n10.215\n\n\n4\n1.533\n2.132\n2.776\n3.747\n4.604\n5.598\n7.173\n\n\n5\n1.476\n2.015\n2.571\n3.365\n4.032\n4.773\n5.893\n\n\n6\n1.440\n1.943\n2.447\n3.143\n3.707\n4.317\n5.208\n\n\n7\n1.415\n1.895\n2.365\n2.998\n3.499\n4.029\n4.785\n\n\n8\n1.397\n1.860\n2.306\n2.896\n3.355\n3.833\n4.501\n\n\n9\n1.383\n1.833\n2.262\n2.821\n3.250\n3.690\n4.297\n\n\n10\n1.372\n1.812\n2.228\n2.764\n3.169\n3.581\n4.144\n\n\n\n\n\n\nFor example, for a sample with 8 DoF and \\(\\alpha=10%\\) results in a critical value of \\(t_0=1.86\\): any value of \\(t_0\\) calculated higher than this value ( in module) involves the rejection of \\(H_0\\) with an error probability of less than 10%"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#variants-of-the-student-test",
    "href": "slides/DESANED/1-statistics.html#variants-of-the-student-test",
    "title": "Intro to Statistics",
    "section": "Variants of the Student test",
    "text": "Variants of the Student test\nThe Student test seen above is valid for the most generic case. There are test variations for the following conditions, which can combine to result in 4 different tests:\n\none or two sample test\none-sided or two-sided test (in \\(H_1\\) the inequality is replaced with a \\(&gt;\\) or a \\(&lt;\\))\n\nFurthermore, in the case of two-sample tests it is possible to assume that the samples are homoscedastic or not\n\n\nTwo samples are said to be homoscedastic when they come from populations with identical variance"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#variants-of-the-student-test-1",
    "href": "slides/DESANED/1-statistics.html#variants-of-the-student-test-1",
    "title": "Intro to Statistics",
    "section": "Variants of the Student test",
    "text": "Variants of the Student test\n\n\n\nOne sample test: \\[\n\\begin{eqnarray}\nH_0:~& \\mu = \\mu_0 \\\\\nH_1:~& \\mu \\neq \\mu_0\n\\end{eqnarray}\n\\] \\[\nt_0 = \\frac{\\mu_0 - \\bar y}{S/\\sqrt{n}}\n\\]\n\n\n\nOne-sided test: \\[\n\\begin{eqnarray}\nH_0:~& \\mu_1 = \\mu_2 \\\\\nH_1:~& \\mu_1 \\gtrless \\mu_2\n\\end{eqnarray}\n\\] \\[\n\\begin{eqnarray}\nt_0 &\\gtrless& \\pm t_{\\alpha, k} \\\\\np\\mathrm{-value} &=& CDF_t^{\\pm}(t_0, k)\n\\end{eqnarray}\n\\]\n\n\nNote: The one-sided test, at the same value of \\(t_0\\), has a lower rejection threshold of \\(H_0\\), and is therefore more powerful"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#homoscedasticity",
    "href": "slides/DESANED/1-statistics.html#homoscedasticity",
    "title": "Intro to Statistics",
    "section": "Homoscedasticity",
    "text": "Homoscedasticity\nThe two-sample test seen above assumes that the two samples are homoscedastic. If they are not, you need to revise the definition of \\(t_0\\) as follows (Welch’s test): \\[\nt_0 = \\frac{\\bar{y_1} - \\bar{y_2}}{\\sqrt{S_1^2/n_1 + S_2^2/n_2}};~~~\\nu=\\frac{(S_1^2/n_1 + S_2^2/n_2)^2}{\\frac{(S_1^2/n_1)^2}{n_1-1}+\\frac{(S_2^2/n_2)^2}{n_2-1}}\n\\] The homoscedasticity hypothesis must be preliminarily verified with a variance test: \\[\n\\begin{eqnarray}\nH_0 :~& \\sigma^2_1 = \\sigma^2_2 \\\\\nH_1 :~& \\sigma^2_1 \\neq \\sigma^2_2\n\\end{eqnarray},~~~F_0=\\frac{S_1}{S_2}\\sim\\mathcal{F}_{n_1-1, n_2-1}\n\\] That is, if the p-value associated with \\(F_0\\) is small, it is assumed that the samples are not homoscedastic and therefore the Welch test is performed; otherwise the Student test applies"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#the-paired-student-test",
    "href": "slides/DESANED/1-statistics.html#the-paired-student-test",
    "title": "Intro to Statistics",
    "section": "The paired Student test",
    "text": "The paired Student test\nIn the case of two-sample Student tests, when they have the same size and are collected two by two in very similar conditions, it is advisable to pair them and carry out the paired Student test\nEach measurement can be expressed as: \\[\ny_{ij}=\\mu_i+\\beta_j+\\varepsilon_{ij};\\hspace{9pt} \\left\\{ \\begin{array}{l}i=1,2\\\\j=1,2,\\dots ,n\\end{array} \\right.\n\\] If we define \\(d_j=y_{1j} - y_{2j}\\), remembering the properties of the operator \\(E(\\cdot)\\), it results in \\(\\mu_d = \\mu_1 - \\mu_2\\). So we can reformulate a pair of equivalent hypotheses: \\[\n\\begin{eqnarray}\nH_0 :~& \\mu_d = 0 \\\\\nH_1 :~& \\mu_d \\neq 0\n\\end{eqnarray}\n\\] So the paired test is a one-sample test, with the advantage that random effects between pairs of observations do not affect the test result"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#the-confidence-interval",
    "href": "slides/DESANED/1-statistics.html#the-confidence-interval",
    "title": "Intro to Statistics",
    "section": "The confidence interval",
    "text": "The confidence interval\nGiven unknown parameter \\(\\vartheta\\), we want to define two statistics \\(L\\) and \\(U\\), such that the probability: \\(P(L\\leq\\vartheta\\leq U)= 1-\\alpha\\). In this case the interval \\([L,U]\\) is called the confidence interval for the parameter \\(\\vartheta\\).\nLet’s consider a one-sample T-test. We know that: \\[\nt_0=\\frac{\\bar x - \\mu}{S/\\sqrt{n}}\\sim t_{n-1}\n\\] If \\(c\\) is \\(t_{\\alpha/2, n-1}\\), then, by definition: \\(P(-c\\leq t_0\\leq c) = 1-\\alpha\\)\nSubstituting \\(t_0\\) we get: \\[\nP(\\bar x - cS/\\sqrt{n} \\leq \\mu \\leq \\bar x + cS/\\sqrt{n}) = 1-\\alpha\n\\]"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#confidence-interval",
    "href": "slides/DESANED/1-statistics.html#confidence-interval",
    "title": "Intro to Statistics",
    "section": "Confidence interval",
    "text": "Confidence interval\nIn practice, it means that the expected value of the population (which is unknown!) has the probability \\(1-\\alpha\\) of falling in the interval \\([\\bar x - cS/\\sqrt{n}, \\bar x + cS /\\sqrt{n}]\\); that probability is called confidence\nIf the \\(\\mu_0\\) of the corresponding test lies outside the confidence interval, then we can reject \\(H_0\\) with an error less than \\(\\alpha\\).\nFor a two-sample test it results: \\[\nP\\left( -t_{\\alpha/2,n_1+n_2-2}\\leqslant\\frac{(\\bar y_1-\\bar y_2)-(\\mu_1-\\mu_2)}{S_p\\sqrt{\\frac{1 }{n_1}+\\frac{1}{n_2}}}\\leqslant t_{\\alpha/2,n_1+n_2-2} \\right)=1-\\alpha\n\\] So the confidence interval \\([L,U]\\) is defined by: \\[\n\\left[(\\bar y_1-\\bar y_2)-t_{\\alpha/2,n_1+n_2-2}S_p\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}},~\n(\\bar y_1-\\bar y_2)+t_{\\alpha/2,n_1+n_2-2}S_p\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}~\\right]\n\\]"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#outliers",
    "href": "slides/DESANED/1-statistics.html#outliers",
    "title": "Intro to Statistics",
    "section": "Outliers",
    "text": "Outliers\nThe collection of data relating to a sample may be affected by errors:\n\nin the measurement operation\nin the data transcription operation\n\nThese errors obviously have a high probability of not agreeing with the rest of the data, i.e. of being too far from the mean in relation to the typical variance of the process.\nThese errors are called outlier and it is good practice to identify and eliminate them immediately after completing data collection\n\n\nIn Italian outliers are known as anomalie"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#outliers-graphical-method",
    "href": "slides/DESANED/1-statistics.html#outliers-graphical-method",
    "title": "Intro to Statistics",
    "section": "Outliers: graphical method",
    "text": "Outliers: graphical method\nThe most common graphical method for identifying outliers is the box-plot"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#outliers-statistical-tests",
    "href": "slides/DESANED/1-statistics.html#outliers-statistical-tests",
    "title": "Intro to Statistics",
    "section": "Outliers: statistical tests",
    "text": "Outliers: statistical tests\n\nChauvenet criterionGrubb Test\n\n\nIt is not a real test but a criterion based on which to eliminate the point furthest from the average (only one!)\nGiven \\(\\left&lt;x_1, x_2,\\dots,x_n\\right&gt;\\) with normal distribution, the maximum absolute difference is calculated \\(s_0=\\underset{i=1,\\dots,n}{\\max} \\left(\\frac{|x_i - \\bar x|}{S_x}\\right)\\)\nDue to the normality of the \\(x_i\\) it is evident that \\(|x_i-\\bar x|/S_x\\sim\\mathcal{N}(0,1)\\). We can then calculate the probability of a value greater than or equal to \\(s_0\\) from the upper tail distribution function: \\[\nP_s=CDF_{\\mathcal{N}}^+(s_0)\n\\] On \\(n\\) normal observations we expect \\(nP_s\\) values larger than \\(s_0\\). So, if \\(nP_s &lt; 0.5\\), while we have a suspicious point, then we can discard the outlier\n\n\nThe Grubb test is a real statistical test, based on a pair of hypotheses (\\(H_1\\) leads to the rejection of the suspected outlier), on a test statistic and on the calculation of a p- value.\nIn short: \\[\n\\begin{eqnarray}\nG_0 &=& \\frac{\\underset{i=1,\\dots,n}{\\max}|y_i-\\bar y|}{S_x}\\\\\nG_0 &&gt;& \\frac{n-1}{n}\\sqrt{\\frac{t^2_{\\alpha/(2n),n-2}}{n-2+t^2_{\\alpha/(2n ),n-2}}} \\Rightarrow \\neg H_0\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#normality-analysis",
    "href": "slides/DESANED/1-statistics.html#normality-analysis",
    "title": "Intro to Statistics",
    "section": "Normality analysis",
    "text": "Normality analysis\nThe tests seen so far always assume that the samples being operated on derive from a normal distribution, despite the unknown parameters\nBefore carrying out any test, therefore, it is necessary to verify the hypothesis of normality\nAs with the outliers analysis, also in this case it is possible to use both graphical methods and statistical tests\nIn general, statistical tests are always preferable, because\n\nare less subject to personal interpretation\nallow automation of choice within algorithms"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#normality-analysis-histogram",
    "href": "slides/DESANED/1-statistics.html#normality-analysis-histogram",
    "title": "Intro to Statistics",
    "section": "Normality analysis: histogram",
    "text": "Normality analysis: histogram\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the number of bars, or bins, use the Sturges formula: \\(K = \\lceil\\log_2 n \\rceil + 1\\) or the Scott formula: \\(K=3.49 s / \\sqrt[3]{n}\\)"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#normality-analysis-quantile-quantile-plot",
    "href": "slides/DESANED/1-statistics.html#normality-analysis-quantile-quantile-plot",
    "title": "Intro to Statistics",
    "section": "Normality analysis: quantile-quantile plot",
    "text": "Normality analysis: quantile-quantile plot\n\n\n\n\n\n\n\n\n\ni\nx\nf\nq\n\n\n\n\n1\n-1.540\n0.061\n-1.547\n\n\n2\n-0.929\n0.159\n-1.000\n\n\n3\n-0.326\n0.256\n-0.655\n\n\n4\n-0.295\n0.354\n-0.375\n\n\n5\n-0.006\n0.451\n-0.123\n\n\n6\n0.415\n0.549\n0.123\n\n\n7\n1.263\n0.646\n0.375\n\n\n8\n1.272\n0.744\n0.655\n\n\n9\n1.330\n0.841\n1.000\n\n\n10\n2.405\n0.939\n1.547\n\n\n\n\n\n\n\n\n\nColumn x is the sorted observations; the sample cumulative distribution, column f, is corrected with the Bloom’s formula: \\(f=(1-3/8)/(n+1-3/4)\\); the q column represents the standard normal quantiles of f.\nThe diagonal on the graph passes through the two points of the first and third quartiles"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#normality-analysis-quantile-quantile-plot-1",
    "href": "slides/DESANED/1-statistics.html#normality-analysis-quantile-quantile-plot-1",
    "title": "Intro to Statistics",
    "section": "Normality analysis: quantile-quantile plot",
    "text": "Normality analysis: quantile-quantile plot"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#normality-analysis-chi-square-test",
    "href": "slides/DESANED/1-statistics.html#normality-analysis-chi-square-test",
    "title": "Intro to Statistics",
    "section": "Normality analysis: Chi-Square test",
    "text": "Normality analysis: Chi-Square test\nIt is a statistical test to test the null hypothesis that a given sample comes from a given (chosen!) distribution\n\nWe group \\(\\left&lt;x_1, x_2, \\dots, x_n\\right&gt;\\) into \\(k\\) classes, such that each class has at least 4–5 elements. The width of each interval can be different\nLet \\(O_i,~i=1,2,\\dots,k\\) be the number of observations in each class, and \\(E_i\\) be the number of expected observations (expected) in each class for the hypothesized distribution\nThe differences between \\(O_i\\) and \\(E_i\\) will be the greater the higher the probability of \\(H_1\\). Then the test statistic is defined:\n\n\\[\nX_0^2 = \\sum_{i=1}^k \\frac{(O_i-E_i)^2}{E_i} \\sim \\chi_{k-p-1}^2\n\\] where \\(p\\) is the number of parameters of the hypothesized distribution (2 for the normal)"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#normality-analysis-shapiro-wilk-test",
    "href": "slides/DESANED/1-statistics.html#normality-analysis-shapiro-wilk-test",
    "title": "Intro to Statistics",
    "section": "Normality analysis: Shapiro-Wilk test",
    "text": "Normality analysis: Shapiro-Wilk test\nAmong the tests of normality it is the one with the greatest power for a given level of significance\nThe test statistic is based on an anonymous distribution and known only numerically\nFor large enough samples, the test is so powerful that it highlights even small deviations from normality, due for example to outliers. In these cases it is appropriate to accompany it with a Quantile-Quantile plot to discriminate these effects"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#correlation-check",
    "href": "slides/DESANED/1-statistics.html#correlation-check",
    "title": "Intro to Statistics",
    "section": "Correlation check",
    "text": "Correlation check\n\n\n\nThe T-test is valid for normal and independently distributed samples (IID)\nIt is necessary to test this hypothesis\nIf I plot the values of the two samples in a graph in the order in which they were acquired I can observe correlation (s1 vs. s3) or lack of correlation (s1 vs. s2)\n\n\n\n\n\n\n\n\n\n\n\n\nIt is clear that \\(\\rho_{s_1s_2}\\) will be close to 0, while \\(\\rho_{s_1s_3}\\) will be close to 1\nThere is a Pearson correlation test which provides a p-value associated with the alternative hypothesis that the two samples are correlated."
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#factors",
    "href": "slides/DESANED/1-statistics.html#factors",
    "title": "Intro to Statistics",
    "section": "Factors",
    "text": "Factors\n\nA factor is a parameter that determines the outcome of a process\nIt is possible to have quantitative or qualitative factors\n\nQuantitative factors take on levels that can be represented by real numbers\nQualitative factors take on non-ordered and non-measurable levels\n\nCombinations of different levels for different factors are called treatments\nThe value measured as the output of the process is called yield"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#single-factor-and-multi-level-experiments",
    "href": "slides/DESANED/1-statistics.html#single-factor-and-multi-level-experiments",
    "title": "Intro to Statistics",
    "section": "Single-factor and multi-level experiments",
    "text": "Single-factor and multi-level experiments\nExample: tensile strength of a yarn as a function of the percentage of cotton fibers (quantitative factor)\n\nOne factor, 5 levels, each treatment is repeated 5 times\nThe boxplot is useful as an overview but for such small samples the indication of outliers is not reliable\n\n\n\n\n\n\n\n\n% Cotton\n#1\n#2\n#3\n#4\n#5\n\n\n\n\n15\n7\n7\n15\n11\n9\n\n\n20\n12\n17\n12\n18\n18\n\n\n25\n14\n18\n18\n19\n19\n\n\n30\n19\n25\n22\n19\n23\n\n\n35\n7\n10\n11\n15\n11"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#single-factor-and-multi-level-experiments-1",
    "href": "slides/DESANED/1-statistics.html#single-factor-and-multi-level-experiments-1",
    "title": "Intro to Statistics",
    "section": "Single-factor and multi-level experiments",
    "text": "Single-factor and multi-level experiments\nThe question is: do the treatments produce statistically significant changes in yield (strength)?\nEach individual yield value can be expressed as (averages model): \\[\ny_{ij}=\\mu_i+\\varepsilon_{ij},~~\\left\\{\\begin{array}{rcl}\ni &=& 1, 2, \\dots, a \\\\\nj &=& 1, 2, \\dots, n\n\\end{array}\\right.\n\\] where \\(i\\) are the treatments, \\(j\\) are the repetitions, \\(\\varepsilon_{ij}\\) are the residuals, i.e. the purely stochastic and zero-mean component of the random variable \\(y_{ij}\\), while \\(\\mu_i\\) is the deterministic component\nWe can separate \\(\\mu_i = \\mu + \\tau_i\\), where \\(\\tau_i\\) is the contribution of the treatment to the overall mean \\(\\mu\\) (effects model): \\[\ny_{ij}=\\mu + \\tau_i+\\varepsilon_{ij},~~\\left\\{\\begin{array}{rcl}\ni &=& 1, 2, \\dots, a \\\\\nj &=& 1, 2, \\dots, n\n\\end{array}\\right.\n\\]"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#single-factor-and-multi-level-experiments-2",
    "href": "slides/DESANED/1-statistics.html#single-factor-and-multi-level-experiments-2",
    "title": "Intro to Statistics",
    "section": "Single-factor and multi-level experiments",
    "text": "Single-factor and multi-level experiments\n\nWe can exploit the previous models to study the significance of the factor\nUsing the means model we have the pair of hypotheses:\n\n\\[\n\\begin{eqnarray}\nH_0&:&~\\mu_1=\\mu_2=\\dots =\\mu_a \\\\\nH_1&:&~\\mu_i\\neq\\mu_j~~~\\textrm{for at least one pair }(i, j)\n\\end{eqnarray}\n\\]\n\nEquivalently, using the effects model:\n\n\\[\n\\begin{eqnarray}\nH_0&:&~\\tau_1=\\tau_2=\\dots =\\tau_a = 0 \\\\\nH_1&:&~\\tau_i\\neq0~~~\\textrm{for at least one }i\n\\end{eqnarray}\n\\]\n\nTo dispel the hypothesis we must formulate an appropriate test statistic"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#variance-decomposition",
    "href": "slides/DESANED/1-statistics.html#variance-decomposition",
    "title": "Intro to Statistics",
    "section": "Variance decomposition",
    "text": "Variance decomposition\nDefinitions:\n\nShorthand notation for sums:\n\n\\[\n\\begin{eqnarray}\ny_{i.}&=&\\sum_{j=1}^n y_{ij},~\\bar y_{i.}=y_{i.}/n,~i=1, 2, \\dots, a \\\\\ny_{..}&=&\\sum_{i=1}^a\\sum_{j=1}^n y_{ij},~\\bar y_{..}=y_{..}/N,~N=na\n\\end{eqnarray}\n\\]\n\nWe introduce the total corrected quadratic sum: \\[\n\\begin{equation}\nSS_T=\\sum_{i=1}^a\\sum_{j=1}^n (y_{ij}-\\bar y_{..})^2 = (N-1)V(y_{ij})\n\\end{equation}\n\\]"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#variance-decomposition-1",
    "href": "slides/DESANED/1-statistics.html#variance-decomposition-1",
    "title": "Intro to Statistics",
    "section": "Variance decomposition",
    "text": "Variance decomposition\nConsidering that: \\[\nSS_T=\\sum_{i=1}^a\\sum_{j=1}^n (y_{ij}-\\bar y_{..})^2 = \\sum_{i=1}^a\\sum_{j =1}^n [(\\bar y_{i.}-\\bar y_{..})+(y_{ij}-\\bar y_{i.})]^2\n\\]\nthat is to say:\n\\[\nSS_T= n\\sum_{i=1}^a(\\bar y_{i.}-\\bar y_{..})^2 + \\sum_{i=1}^a\\sum_{j=1}^n (y_{ij}-\\bar y_{i.})^2 + 2\\sum_{i=1}^a\\sum_{j=1}^n(\\bar y_{i.}-\\bar y_{. .})(y_{ij}-\\bar y_{i.})\n\\]\nand since \\(\\sum_{j=1}^n (y_{ij}-\\bar y_{i.})=y_{i.}-n\\bar y_{i.}=0\\), it follows that the \\(SS_T\\) can be decomposed as:\n\\[\nSS_T=\\sum_{i=1}^a\\sum_{j=1}^n (y_{ij}-\\bar y_{..})^2=n\\sum_{i=1}^a(\\bar y_{i.}-\\bar y_{..})^2 + \\sum_{i=1}^a\\sum_{j=1}^n (y_{ij}-\\bar y_{i.})^2=SS_{tr}+SS_E\n\\]"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#variance-decomposition-2",
    "href": "slides/DESANED/1-statistics.html#variance-decomposition-2",
    "title": "Intro to Statistics",
    "section": "Variance decomposition",
    "text": "Variance decomposition\nWe have decomposed the \\(SS_T\\) (which is a close relative of the variance) into mean square sums:\n\n\\(SS_{tr}\\), which measures the variability between one treatment and another\n\\(SS_E\\), which measures variability within treatments\n\nRemembering what was said in the definition of the distribution \\(\\mathcal{X}^2\\):\n\\[\n\\frac{\\mathit{SS}}{\\sigma^2}=\\frac{\\sum_{i=1}^n(y_i-\\bar y)^2}{\\sigma^2} \\sim \\mathcal{X}^2_{n-1}\n\\]\nit seems that:\n\\[\n\\sum_{i=1}^{a}\\sum_{j=1}^{n}\\frac{(y_{ij}-\\bar y_{..})^2}{\\sigma^2} = \\frac{SS_T}{\\sigma^2} \\sim \\chi^2_{N-1},~~~\\frac{SS_E}{\\sigma^2} \\sim \\chi^2_{N-a},~~~\\frac{SS_{tr}}{\\sigma^2} \\sim \\chi^2_{a-1}\n\\]"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#variance-decomposition-3",
    "href": "slides/DESANED/1-statistics.html#variance-decomposition-3",
    "title": "Intro to Statistics",
    "section": "Variance decomposition",
    "text": "Variance decomposition\n\nThe ratio between a quadratic sum and its number of DoF is equal to the variance\nTherefore, if \\(H_0\\) is valid it must be \\(E(SS_{tr}/(a-1)) = E(SS_E/(N-a))\\), given that it is the expected value of several samples drawn from the same group of observations, all from the same population (\\(H_0\\) means that the treatments are not significant)\nBut then we can write:\n\n\\[\nF_0 = \\frac{SS_{tr}/(a-1)}{SS_E/(N-a)}=\\frac{MS_{tr}}{MS_E} \\sim F_{a-1,N-a}\n\\] Where \\(MS_\\cdot\\) are called mean sum of squares\nBased on the last equation we can calculate the p-value associated with the hypothesis \\(H_1\\): for small values we can state that at least one treatment has a statistically significant effect"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#cochrans-theorem",
    "href": "slides/DESANED/1-statistics.html#cochrans-theorem",
    "title": "Intro to Statistics",
    "section": "Cochran’s theorem",
    "text": "Cochran’s theorem\nThe definition of \\(F_0\\) assumes that \\(MS_{tr}\\) and \\(MS_E\\) are independent. Since they are the result of the decomposition of \\(SS_T\\) this is not obvious. However, the theorem holds true:\n\nTheorem 2 (Cochran’s) Let \\(Z_i\\sim \\mathcal{N}(0,1),~i=1,2,\\dots,\\nu\\) independent samples, with \\(\\sum_{i=1}^\\nu Z_i^2=Q_1+Q_2+\\dots+Q_s\\) where \\(s\\leqslant \\nu\\) and \\(Q_i\\) has \\(\\nu_i\\) degrees of freedom (\\(i=1, 2,\\dots,s\\)). Then, \\(Q_1,Q_2,\\dots,Q_s\\) are independent random variables distributed as \\(\\mathcal{X}^2\\) with \\(\\nu_1,\\nu_2,\\dots,\\nu_s\\) degrees of freedom, if and only if \\[\n\\nu=\\nu_1+\\nu_2+\\dots+\\nu_s\n\\] Since \\((N-a)+(a-1)=(N-1)\\), it follows that \\(SS_{tr}/\\sigma^2\\) and \\(SS_E/\\sigma^2\\) are independent random variables distributed like \\(\\mathcal {X}^2_{a-1}\\) and \\(\\mathcal{X}^2_{N-a}\\), respectively."
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#anova-analysis-of-variance",
    "href": "slides/DESANED/1-statistics.html#anova-analysis-of-variance",
    "title": "Intro to Statistics",
    "section": "ANOVA (ANalysis Of VAriance)",
    "text": "ANOVA (ANalysis Of VAriance)\nReturning to the example of the resistance of mixed yarns, we can apply the variance decomposition obtaining:\n\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nCotton\n4\n475.76\n118.94\n14.75682\n9.1e-06\n\n\nResiduals\n20\n161.20\n8.06\nNA\nNA\n\n\n\n\n\nThe table above is called ANOVA table\nThe very low p-value allows us to reject the null hypothesis and state that at least one treatment has statistically significant effects\nHowever, we do not know how many and which treatments are significant. To study this aspect we use the Tukey Test"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#tukey-test",
    "href": "slides/DESANED/1-statistics.html#tukey-test",
    "title": "Intro to Statistics",
    "section": "Tukey test",
    "text": "Tukey test\nThe test evaluates simultaneously all the following pairs of hypotheses: \\[\n\\left.\n\\begin{eqnarray}\nH_0&:&~\\mu_i=\\mu_j \\\\\nH_1&:&~\\mu_i\\neq \\mu_j\n\\end{eqnarray}\n\\right\\} ~~\\forall i\\neq j\n\\] For each pair \\((i,j),~i\\neq j\\) we have the test statistic: \\[\nq_{0,ij}=\\frac{|\\bar y_{i.}-\\bar y_{j.}|}{S_{p,ij}\\sqrt{2/n}}\\sim\\mathcal{Q} _{a,k}\n\\] where \\(n\\) is the size of the samples, the same for all, \\(a\\) is the number of treatments and \\(k\\) is the number of degrees of freedom of \\(MS_E\\), i.e. \\(N-a=an-a\\).\nFor each pair we then calculate the p-value from the CDF of the studentized range distribution, \\(\\mathcal{Q}\\), and calculate the confidence interval: \\[\n\\bar y_{i.}-\\bar y_{j.}-q_{\\alpha,a,N-a}\\frac{S_{p,ij}}{\\sqrt{n}} \\leqslant \\mu_i-\\mu_j\n\\leqslant \\bar y_{i.}-\\bar y_{j.}+q_{\\alpha,a,N-a}\\frac{S_{p,ij}}{\\sqrt{n}}\n\\]"
  },
  {
    "objectID": "slides/DESANED/1-statistics.html#tukey-test-1",
    "href": "slides/DESANED/1-statistics.html#tukey-test-1",
    "title": "Intro to Statistics",
    "section": "Tukey test",
    "text": "Tukey test\n\n\nGenerally, the result of Tukey’s test is reported in a plot\nCouples for which the confidence interval crosses 0 have no significant differences, and vice versa\nNote: carrying out as many separate Student tests would increase the overall error probability, which would result from the combination of the error probabilities of the individual tests\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: What happens to confidence intervals going from a 95% interval to a 99% confidence level?"
  },
  {
    "objectID": "slides/ADAS/index.html",
    "href": "slides/ADAS/index.html",
    "title": "Analisi Dati e Statistica",
    "section": "",
    "text": "Introduzione al corso\nStatistica descrittiva e inferenziale\nIntroduzione a R\nRegressione\nBootstrap\nMisura, incertezza, taratura\nDesign of Experiments\nSerie temporali\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAggornamenti continui!\n\n\n\nLe slide qui presentate possono essere frequentemente aggiornate per correzioni e piccole aggiunte: assicuratevi di verificare sempre l’ultima versione (basta ricaricare la pagina!)"
  },
  {
    "objectID": "slides/ADAS/index.html#contenuti-del-corso",
    "href": "slides/ADAS/index.html#contenuti-del-corso",
    "title": "Analisi Dati e Statistica",
    "section": "",
    "text": "Introduzione al corso\nStatistica descrittiva e inferenziale\nIntroduzione a R\nRegressione\nBootstrap\nMisura, incertezza, taratura\nDesign of Experiments\nSerie temporali\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAggornamenti continui!\n\n\n\nLe slide qui presentate possono essere frequentemente aggiornate per correzioni e piccole aggiunte: assicuratevi di verificare sempre l’ultima versione (basta ricaricare la pagina!)"
  },
  {
    "objectID": "slides/ADAS/index.html#uso-delle-slide",
    "href": "slides/ADAS/index.html#uso-delle-slide",
    "title": "Analisi Dati e Statistica",
    "section": "Uso delle slide",
    "text": "Uso delle slide\nLe slide sono interattive. I comandi più utili sono:\n\nF per presentare a tutto schermo\nE per la modalità esportazione (poi stampare come PDF dal browser); E di nuovo per uscire dalla modalità esportazione\nTasti cursore per navigare avanti e indietro\nESC per passare alla modalità overview\n? per altri comandi"
  },
  {
    "objectID": "slides/ADAS/index.html#repository-del-codice",
    "href": "slides/ADAS/index.html#repository-del-codice",
    "title": "Analisi Dati e Statistica",
    "section": "Repository del codice",
    "text": "Repository del codice\nIl codice R sviluppato durante le lezioni è disponibile su GitHub: github.com/pbosetti/adas-24."
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#statistica-delle-serie-temporali",
    "href": "slides/ADAS/7-serie_temporali.html#statistica-delle-serie-temporali",
    "title": "Serie Temporali",
    "section": "Statistica delle serie temporali",
    "text": "Statistica delle serie temporali\nTutti i metodi di regressione visti fin ora sono basati sull’assunzione che la variabile aleatoria sia \\(x\\overset{IID}\\sim \\mathcal{N}(\\mu, \\sigma^2)\\). Cioè tutte le osservazioni devono essere non-autocorrelate\nQuest’assunzione è, tra l’altro, alla base della raccomandazione di casualizzazione della sequenza operativa\nSupponiamo di poter considerare una misura come un segnale tempo-dipendente. È evidente che riducendo l’intervallo di campionamento del segnale prima o poi ogni campione sarà correlato al precedente\nEsiste quindi una frequenza di campionamento massima al di sopra della quale ogni misura risulta essere autocorrelata, cioè le osservazioni di \\(x\\) non sono più IID\nQuesta situazione sussiste quando la dinamica propria dello strumento di misura o del misurando stesso—che sono sempre finite—sono più lente dell’intervallo temporale in cui si effettuano le misure"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#statistica-delle-serie-temporali-1",
    "href": "slides/ADAS/7-serie_temporali.html#statistica-delle-serie-temporali-1",
    "title": "Serie Temporali",
    "section": "Statistica delle serie temporali",
    "text": "Statistica delle serie temporali\n\n\nConsideriamo ad esempio la serie temporale in figura che riporta la differenza tra la temperatura media delle terre emerse e il corrispondente valore medio nel periodo 1951–1980\nÈ evidente che osservazioni vicine sono più correlate di osservazioni lontane\nInoltre è evidente (ed è di interesse) valutare la dipendenza della v.a. considerata dal tempo allo scopo di effettuare delle previsioni future"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#autocovarianza-e-autocorrelazione",
    "href": "slides/ADAS/7-serie_temporali.html#autocovarianza-e-autocorrelazione",
    "title": "Serie Temporali",
    "section": "Autocovarianza e autocorrelazione",
    "text": "Autocovarianza e autocorrelazione\nAbbiamo visto come gli operatori covarianza e correlazione servano a stimare l’indipendenza di due campioni\nConsiderando un segnale tempo-dipendente \\(x=x(t)\\), è interessante considerare la covarianza del segnale con se stesso, traslato nel tempo\nDefiniamo la funzione autocovarianza \\(\\gamma(s,t)\\) come la funzione che valuta la covarianza di un segnale temporale con se stesso valutato iniziando ai tempi \\(s\\) e \\(t\\): \\[\n\\gamma_x(s, t) = \\sigma_{x_s, x_t} = E[(x_s-\\mu_s)(x_t-\\mu_t)]\n\\]\nÈ evidente che \\(\\gamma_x(s,s)=\\sigma^2(x_s)\\)\nLa funzione di autocorrelazione (ACF), di conseguenza, è definita come: \\[\n\\rho_x(s, t) = \\frac{\\gamma_x(s,t)}{\\sqrt{\\gamma_x(s,s)\\gamma_x(t,t)}}\n\\] ed ha il vantaggio di essere sempre compresa in \\([-1,1]\\). È inoltre evidente che \\(\\rho_x(s,s)=1\\)"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#autocorrelazione",
    "href": "slides/ADAS/7-serie_temporali.html#autocorrelazione",
    "title": "Serie Temporali",
    "section": "Autocorrelazione",
    "text": "Autocorrelazione\nSe campioniamo un segnale continuo a intervalli fissi \\(\\Delta t\\) per una durata complessiva \\(T\\), otteniamo una serie temporale finita di \\(N=T/\\Delta t\\) osservazioni: \\(x_{0}=\\left&lt;x_1, x_2, \\dots, x_N\\right&gt;\\)\nPossiamo estendere la definizione stabilendo che sia \\(s=t_0 = 0\\) l’istante iniziale della serie e che sia \\(t=s+\\tau\\) un generico momento successivo tale per cui \\[\n\\tau = \\ell \\Delta t\n\\] dove \\(\\ell\\) è il ritardo o lag, e allora l’autocovarianza e l’autocorrelazione per una s.t. finita risultano: \\[\n\\gamma_x(\\ell) = \\frac{\\sum_{i=1}^{N-\\ell} (x_i - \\bar x_0) (x_{i+\\ell}-\\bar x_\\ell) }{N-\\ell-2},\\quad \\rho_x(\\ell) = \\frac{\\gamma_x(\\ell)}{\\sqrt{\\sigma_x(0)\\sigma_x(\\ell)}}\n\\] dove \\[\n\\bar x_0=\\frac{1}{N-\\ell}\\sum\\nolimits_{i=1}^{N-\\ell} x_i,\\quad \\bar x_\\ell=\\frac{1}{N-\\ell}\\sum\\nolimits_{i=\\ell}^{N} x_i\n\\]"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#autocorrelogramma-acf",
    "href": "slides/ADAS/7-serie_temporali.html#autocorrelogramma-acf",
    "title": "Serie Temporali",
    "section": "Autocorrelogramma (ACF)",
    "text": "Autocorrelogramma (ACF)\n\n\n\n\n\n\n\n\n\n\n\n\nPer costruire il grafico ACF di una s.t. si trasla l’ascissa di \\(\\ell\\) campioni trascurando i primi \\(\\ell\\) campioni nella s.t. non traslata e gli ultimi \\(\\ell\\) campioni in quella traslata\nPoi si calcola l’autocorrelazione tra le due serie\nIl processo viene ripetuto per \\(\\ell=0, 1, \\dots, n\\), con \\(n\\) scelto in funzione della dimensione della serie, tipicamente pari ad almeno 50 e comunque non oltre la metà della lunghezza della s.t."
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#autocorrelogramma-acf-1",
    "href": "slides/ADAS/7-serie_temporali.html#autocorrelogramma-acf-1",
    "title": "Serie Temporali",
    "section": "Autocorrelogramma (ACF)",
    "text": "Autocorrelogramma (ACF)\n\n\nLa funzione di autocorrelazione \\(\\rho_x(\\ell)\\) che è una funzione a valori discreti, può essere messa in grafico per studiare il lag massimo al di sopra del quale la serie storica \\(x\\) non è più autocorrelata\nIn figura il segnale di un microfono che registra il suono “AAHH”. La serie è evidentemente periodica ogni 0.01 s\nL’autocorrelogramma mostra autocorrelazione elevata fino a \\(\\ell=4\\). Poi l’andamento è periodico, a confermare che la s.t. è autocorrelata con se stessa ogni circa 10 lag\n\n\nSerie storicaACF\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nACF sta per Auto-Correlation Function; la banda evidenziata in blu è l’intervallo di confidenza al 95%. Inoltre, in generale vale sempre \\(\\rho_x(0)=1\\)"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#autocorrelogramma-acf-2",
    "href": "slides/ADAS/7-serie_temporali.html#autocorrelogramma-acf-2",
    "title": "Serie Temporali",
    "section": "Autocorrelogramma (ACF)",
    "text": "Autocorrelogramma (ACF)\n\n\nConsideriamo gli stessi dati della s.t. precedente, ma campionati in istanti casuali\nAllora \\(y\\overset{IID}\\sim \\mathcal{N}(\\mu, \\sigma^2)\\) e quindi \\(\\rho_y(\\ell) = 0~\\forall \\ell&gt;0\\)\nCome atteso, l’unico valore della ACF fuori dall’initervallo di confidenza è \\(\\rho_x(0)\\)\nIn questo caso si dice anche che la s.t. è un random walk\n\n\nSerie storicaACF"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#serie-temporali-stazionarie",
    "href": "slides/ADAS/7-serie_temporali.html#serie-temporali-stazionarie",
    "title": "Serie Temporali",
    "section": "Serie temporali stazionarie",
    "text": "Serie temporali stazionarie\nUna s.t. può essere stazionaria o meno. Si definiscono:\n\nSerie temporale stazionaria in senso ampio\n\nÈ una serie temporale per cui il comportamento probabilistico di una qualsiasi collezione di valori \\(\\left&lt;x_{t_1}, x_{t_2},\\dots, x_{t_k}\\right&gt;\\) è identico a quello della collezione traslata \\(\\left&lt;x_{t_1+h}, x_{t_2+h},\\dots, x_{t_k+h}\\right&gt;\\), cioè: \\[\n\\mathrm{Pr}(x_{t_1}\\leq c_1, \\dots, x_{t_k}\\leq c_k) = \\mathrm{Pr}(x_{t_1+h}\\leq c_1, \\dots, x_{t_k+h}\\leq c_k).\n\\]\n\nSerie temporale stazionaria in senso stretto\n\nÈ una serie temporale per cui il valor medio della serie temporale è costante (tempo-indipendente) e la funzione di autocovarianza \\(\\gamma(s,t)\\) dipende da \\(s\\) e \\(t\\) solo tramite la loro differenza \\(|s-t|\\)\n\n\nPer una serie stazionaria in senso ampio si può assumere \\(\\sigma_x(0)=\\sigma_x(\\ell)=\\sigma_x\\) e \\(\\bar x_0=\\bar x_\\ell\\) e, quindi, \\(\\rho_x(\\ell) = \\gamma_x(\\ell)/\\sigma_x\\)"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#stabilizzazione",
    "href": "slides/ADAS/7-serie_temporali.html#stabilizzazione",
    "title": "Serie Temporali",
    "section": "Stabilizzazione",
    "text": "Stabilizzazione\n\nLe ST stazionarie almeno in senso ampio sono più semplici da trattare\nÈ quindi utile cercare di stabilizzare la ST separandola in un termine di tendenza (trend) \\(x_t\\) più un termine stazionario \\(x_s\\)\nla stabilizzazione può essere fatta in due modi:\n\ndetrending mediante regressione lineare: \\(x_t = x_{l,t} + x_{s,t}\\), dove \\(x_{l,t} = a + bt\\) e, quindi, \\(x_{s,t}\\) risulta essere la serie dei residui della regressione lineare di \\(x_t\\)\ndetrending per differenziazione\n\nLa ST stabilizzata può poi essere analizzata e quindi ri-trasformata mediante l’operazione inversa:\n\nsomma del termine di tendenza\nintegrazione (cioè somma cumulativa)"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#stabilizzazione-esempio",
    "href": "slides/ADAS/7-serie_temporali.html#stabilizzazione-esempio",
    "title": "Serie Temporali",
    "section": "Stabilizzazione (esempio)",
    "text": "Stabilizzazione (esempio)"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#operatori-di-differenziazione",
    "href": "slides/ADAS/7-serie_temporali.html#operatori-di-differenziazione",
    "title": "Serie Temporali",
    "section": "Operatori di differenziazione",
    "text": "Operatori di differenziazione\nIn generale, la stabilizzazione per differenziazione dà risultati migliori ed è anche più pratica: se la ST differenziata non è stabile, è possibile aumentare l’ordine di differenziazione fino a raggiungere la stabilità\nCome si differenzia una ST?\nSi definiscono:\n\nOperatore backshift: è l’operatore \\(B^n\\) tale per cui \\(B^n x_t := x_{t-n}\\)\nOperatore differenza: è l’operatore \\(\\nabla\\) tale per cui \\(\\nabla x_t := x_t - x_{t-1}=(1-B)x_t\\). Risulta quindi che \\(\\nabla^dx_t=(1-B)^dx_t\\), e quindi ad esempio \\(\\nabla^2x_t=(1-B)^2x_t=x_t-2x_{t-1}+x_{t-2}\\)\n\nQuindi ad esempio la differenziazione \\(\\nabla^2x_t\\) è l’equivalente discreto della derivata seconda \\(\\frac{d^2}{dt^2}x(t)\\) per la funzione continua \\(x(t)\\)"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#stabilizzazione-e-acf",
    "href": "slides/ADAS/7-serie_temporali.html#stabilizzazione-e-acf",
    "title": "Serie Temporali",
    "section": "Stabilizzazione e ACF",
    "text": "Stabilizzazione e ACF"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#stabilizzazione-e-acf-1",
    "href": "slides/ADAS/7-serie_temporali.html#stabilizzazione-e-acf-1",
    "title": "Serie Temporali",
    "section": "Stabilizzazione e ACF",
    "text": "Stabilizzazione e ACF\n\n\n\nLa ACF di una serie non stabilizzata mostra sempre una correlazione anche a lag elevati\nLa ACF della serie dei residui di una regressione lineare decresce più rapidamente, ma mantiene comunque una correlazione anche a lag elevati\nLa ACF della serie differenziata, inoltre, si smorza molto rapidamente (i lag 1, 2, 3, … dovrebbero essere esponenziali), dopodiché mostra oscillazioni armoniche, indice di una periodicità nella ST originale\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPossiamo quindi dire che la serie storica del prezzo del pollo mostra un trend che si stabilizza al primo ordine di differenziazione, ha una dinamica che mostra autocorrelazione fino ai tre punti precedenti, e un andamento periodico con periodo pari a 12 lag."
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#modelli-ar-definizione",
    "href": "slides/ADAS/7-serie_temporali.html#modelli-ar-definizione",
    "title": "Serie Temporali",
    "section": "Modelli AR — definizione",
    "text": "Modelli AR — definizione\nUn modello Auto-Regressivo (AR) esprime una determinata osservazione \\(x_t\\) al tempo \\(t\\) come combinazione lineare di \\(p\\) valori precedenti \\(x_{t-1}, x_{t-2},\\dots,x_{t-p}\\). Un modello AR di ordine \\(p\\), abbreviato in \\(\\mathrm{AR}(p)\\), ha la forma: \\[\nx_t=\\phi_1 x_{t-1} + \\phi_2 x_{t-2}+\\dots+\\phi_p x_{t-p} + w_t \\label{eq:AR}\n\\]\n\n\\(x_t\\) è stazionaria in senso ampio e \\(w_t\\sim\\mathcal{N}(0, \\sigma^2_w)\\)\n\\(\\phi_1, \\phi_2,\\dots,\\phi_p\\) sono costanti e \\(\\phi_p\\neq0\\)\n\nSe la media di \\(x_t\\) non è nulla, si sostituisce \\(x_t\\) con \\(x_t - \\mu\\) per ottenere: \\[\n\\begin{align}\nx_t-\\mu&=\\phi_1( x_{t-1}-\\mu) + \\phi_2 (x_{t-2}-\\mu)+\\dots+\\phi_p (x_{t-p}-\\mu) + w_t \\\\\nx_t&=\\alpha + \\phi_1 x_{t-1} + \\phi_2 x_{t-2}+\\dots+\\phi_p x_{t-p} + w_t,~~~\\alpha=\\mu(1-\\phi_1-\\phi_2-\\dots-\\phi_p)\n\\end{align}\n\\]\n\n\nLe \\(w_t\\) sono anche chiamate innovazioni, dato che sono l’unico contributo nuovo di un punto rispetto ai precedenti."
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#modelli-ar-regressione",
    "href": "slides/ADAS/7-serie_temporali.html#modelli-ar-regressione",
    "title": "Serie Temporali",
    "section": "Modelli AR — regressione",
    "text": "Modelli AR — regressione\nRicordando la definizione dell’operatore backshift, la definizione di \\(w_t\\) può essere scritta come:\n\\[\n(1-\\phi_1 B - \\phi_2 B^2 -\\dots-\\phi_p B^p)x_t=w_t\n\\] o ancora più concisamente come:\n\\[\n\\Phi_p(B)x_t=w_t\n\\] dove \\(\\Phi_p(B):=1-\\phi_1B-\\phi_2B^2-\\dots-\\phi_pB^p\\) è detto operatore autoregressivo.\nEffettuare la regressione di un modello \\(\\mathrm{AR}(p)\\) su una serie storica \\(x_t\\) significa quindi adattare il modello \\(\\Phi_p(B)\\hat{ x_t}=w_t\\) identificando i coefficienti di \\(\\Phi_p(B)\\) che minimizzano i residui quadratici medi, essendo i residui \\(\\varepsilon_t = x_t - \\hat{ x_t} = x_t - \\Phi_p^{-1}(B)w_t\\) (ammesso che esista l’inversa \\(\\Phi_p^{-1}(B)\\))."
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#modelli-ma-definizione",
    "href": "slides/ADAS/7-serie_temporali.html#modelli-ma-definizione",
    "title": "Serie Temporali",
    "section": "Modelli MA — definizione",
    "text": "Modelli MA — definizione\nAlternativamente, è possibile immaginare il caso in cui la generica osservazione \\(x_t\\) è espressa come combinazione lineare del disturbo agente sulle \\(q\\) osservazioni precedenti. Un modello MA di ordine \\(q\\), abbreviato come \\(\\mathrm{MA}(q)\\) è definito come \\[\nx_t = w_t + \\theta_1 w_{t-1} + \\theta_2 w_{t-2} + \\dots + \\theta_q w_{t-q}\n\\]\n\n\\(w_t\\sim\\mathcal{N}(0, \\sigma_w^2)\\)\n\\(\\theta_1, \\theta_2,\\dots,\\theta_q\\) sono parametri costanti con \\(\\theta_q\\neq0\\)"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#modelli-ma-regressione",
    "href": "slides/ADAS/7-serie_temporali.html#modelli-ma-regressione",
    "title": "Serie Temporali",
    "section": "Modelli MA — regressione",
    "text": "Modelli MA — regressione\nAnalogamente al caso \\(\\mathrm{AR}(p)\\), per \\(\\mathrm{MA}(q)\\) è possibile definire l’operatore media mobile di ordine \\(q\\): \\[\n\\Theta_q(B)=(1+\\theta_1 B+\\theta_2 B^2+\\dots+\\theta_q B^q)\n\\] tale per cui la equazione per \\(x_t\\) può essere scritta come:\n\\[\nx_t=\\Theta_q(B)w_t\n\\]\nCome sopra, regredire un modello \\(\\mathrm{MA}(q)\\) ad una serie storica \\(x_t\\) significa identificare i termini di \\(\\Theta_q(B)\\) che minimizzano i residui quadratici medi, definiti come \\(\\varepsilon_t = x_t - \\hat{x_t} = x_t - \\Theta_q(B)\\) (si noti che questa volta non c’è l’inversa!)"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#modelli-ma-e-acf",
    "href": "slides/ADAS/7-serie_temporali.html#modelli-ma-e-acf",
    "title": "Serie Temporali",
    "section": "Modelli MA e ACF",
    "text": "Modelli MA e ACF\n\n\nUna ST \\(MA(q)\\) ha una memoria che si estende fino al lag \\(q\\), nel senso che le innovazioni a distanze superiori a \\(q\\) non hanno alcun effetto sull’ultima osservazione\nQuindi, data una serie temporale di tipo \\(MA(q)\\) si può spesso identificare l’ordine dalla sua ACF, contando i picchi dopo quello a lag 0:\n\ntre picchi fuori dalla banda di confidenza significano un modello \\(MA(3)\\)\ni segni dei picchi corrispondono ai segni dei coefficienti"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#modelli-ma-e-acf-1",
    "href": "slides/ADAS/7-serie_temporali.html#modelli-ma-e-acf-1",
    "title": "Serie Temporali",
    "section": "Modelli MA e ACF",
    "text": "Modelli MA e ACF\n\n\nSe invece la serie temporale è di tipo \\(AR(p)\\), ogni osservazione dipende dall’innovazione e da tutte le osservazioni precedenti, in modo ricorsivo\nIn questo caso l’autocorrelogramma riporterà un decadimento esponenziale seguito eventualmente da oscillazioni armoniche"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#modelli-ar-e-pacf",
    "href": "slides/ADAS/7-serie_temporali.html#modelli-ar-e-pacf",
    "title": "Serie Temporali",
    "section": "Modelli AR e PACF",
    "text": "Modelli AR e PACF\nPer i modelli di tipo \\(AR(p)\\) è comunque possibile identificare l’ordine mediante la funzione di autocorrelazione parziale o PACF, così definita:\n\\[\n\\begin{align}\n\\mathrm{PACF}_1 &= \\mathrm{ACF}(z_{t+1}, z_t) \\\\\n\\mathrm{PACF}_k &= \\mathrm{ACF}(z_{t+k} - \\hat{z_{t+k}}, z_t - \\hat{z_t}),~~~k\\geq2 \\\\\n\\end{align}\n\\] in cui \\(\\hat{z_{t+k}}\\) e \\(\\hat{z_t}\\) sono combinazioni lineari di \\(\\{z_{t+1}, z_{t+2},\\dots,z_{t+k-1}\\}\\) che minimizzano l’errore quadratico medio di \\(z_{t+k}\\) e \\(z_t\\), rispettivamente"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#modelli-ar-e-pacf-1",
    "href": "slides/ADAS/7-serie_temporali.html#modelli-ar-e-pacf-1",
    "title": "Serie Temporali",
    "section": "Modelli AR e PACF",
    "text": "Modelli AR e PACF\n\n\nIn generale, quindi, se la ACF mostra una memoria infinita (modello AR) e la PACF mostra pochi picchi, il numero di picchi è l’ordine del modello AR\nAttenzione: non si considerano i picchi dopo il primo cut-off, cioè il lag in corrispondenza del quale l’autocorrelazione scende sotto il limite di confidenza per la prima volta\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttenzione: in questi grafici ACF e PACF il lag comincia da 1!"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#modelli-ma-e-pacf",
    "href": "slides/ADAS/7-serie_temporali.html#modelli-ma-e-pacf",
    "title": "Serie Temporali",
    "section": "Modelli MA e PACF",
    "text": "Modelli MA e PACF\n\n\nConfrontando sia ACF che PACF\n\npuro rumore: ACF e PACF sono nulle per lag maggiore di 0\n\\(AR(p)\\): ACF decresce lentamente, PACF non-nulla per lag minori o uguali a \\(p\\), nulla altrimenti\n\\(MA(q)\\): ACF mostra \\(q\\) picchi; se \\(PACF(1) &gt; 0\\), PACF oscilla a 0, altrimenti decade geometricamente a 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpesso, in pratica queste indicazioni sono difficili da riscontrare ed è quindi difficile individuare \\(p,q\\) in maniera certa e univoca"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#modelli-arma",
    "href": "slides/ADAS/7-serie_temporali.html#modelli-arma",
    "title": "Serie Temporali",
    "section": "Modelli ARMA",
    "text": "Modelli ARMA\nL’ovvia estensione risulta dalla combinazione dei modelli \\(\\mathrm{AR}(p)\\) e \\(\\mathrm{MA}(q)\\)\nUn modello ARMA di ordine \\(p,q\\), abbreviato come \\(\\mathrm{ARMA}(p,q)\\) è definito come:\n\\[\nx_t=\\phi_1 x_{t-1} + \\phi_2 x_{t-2}+\\dots+\\phi_p x_{t-p} + w_t + \\theta_1 w_{t-1} + \\theta_2 w_{t-2} + \\dots + \\theta_q w_{t-q}\n\\] con \\(\\phi_p\\) e \\(\\theta_q\\) non nulli, ovvero, più brevemente: \\[\n\\Phi_p(B)x_t=\\Theta_q(B)w_t\n\\]"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#modelli-ma-e-pacf-1",
    "href": "slides/ADAS/7-serie_temporali.html#modelli-ma-e-pacf-1",
    "title": "Serie Temporali",
    "section": "Modelli MA e PACF",
    "text": "Modelli MA e PACF\n\n\nConfrontando sia ACF che PACF\n\npuro rumore: ACF e PACF sono nulle per lag maggiore di 0\n\\(AR(p)\\): ACF decresce lentamente, PACF non-nulla per lag minori o uguali a \\(p\\), nulla altrimenti\n\\(MA(q)\\): ACF mostra \\(q\\) picchi; se \\(PACF(1) &gt; 0\\), PACF oscilla a 0, altrimenti decade geometricamente a 0\n\\(ARMA(p,q)\\): ACF mostra \\(q\\) picchi; PACF decade geometricamente a 0 solo dopo lag \\(p\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpesso, in pratica queste indicazioni sono difficili da riscontrare ed è quindi difficile individuare \\(p,q\\) in maniera certa e univoca"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#modelli-arima",
    "href": "slides/ADAS/7-serie_temporali.html#modelli-arima",
    "title": "Serie Temporali",
    "section": "Modelli ARIMA",
    "text": "Modelli ARIMA\nI processi \\(\\mathrm{ARMA}(p,q)\\) sono adatti solo a descrivere serie stazionarie in senso ampio\nAbbiamo visto però che un processo non stazionario può essere reso tale per differenziazione di un opportuno grado \\(d\\)\nUn processo \\(x_t\\) è detto \\(\\mathrm{ARIMA}(p,d,q)\\) quando \\(\\nabla^d x_t = (1-B)^d x_t\\) 2è un processo \\(\\mathrm{ARMA}(p,q)\\)\nIn generale, quando \\(E(\\nabla^dx_t)=0\\) il processo \\(\\mathrm{ARIMA}(p,d,q)\\) può essere scritto come: \\[\n\\Phi_p(B)\\nabla^dx_t=\\Theta_q(B)w_t\n\\] Se invece il valore atteso \\(E(\\nabla^dx_t)=\\mu\\), allora: \\[\n\\Phi_p(B)\\nabla^dx_t=\\delta + \\Theta_q(B)w_t,~~~\\delta=\\mu(1-\\phi_1-\\dots-\\phi_p)\n\\]"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#stabilità-e-unicità-dei-modelli",
    "href": "slides/ADAS/7-serie_temporali.html#stabilità-e-unicità-dei-modelli",
    "title": "Serie Temporali",
    "section": "Stabilità e unicità dei modelli",
    "text": "Stabilità e unicità dei modelli\nConsideriamo un modello \\(\\mathrm{AR}(1)\\): possiamo sviluppare la formula ricorsiva come: \\[\n\\begin{align}\nx_t =& \\phi x_{t-1}+w_t = \\phi(\\phi x_{t-2}+w_{t-1}) + w_t \\\\\n=& \\sum_{j=0}^{+\\infty}\\phi^j w_{t-j}\n\\end{align}\n\\] È quindi evidente che la serie temporale \\(x_t\\) è stabile solo se \\(|\\phi|&lt;1\\)\nNel caso generale del modello \\(\\mathrm{AR}(p)\\), si ha la condizione di stabilità: \\[\n\\left|\\sum_{i=1}^p \\phi_i\\right| &lt; 1 \\label{eq:ARstab}\n\\] Un modello AR non stabile è detto anche anti-causale, perché si può dimostrare che per essere stabilizzato richiede la conoscenza delle osservazioni future"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#stabilità-e-unicità-dei-modelli-1",
    "href": "slides/ADAS/7-serie_temporali.html#stabilità-e-unicità-dei-modelli-1",
    "title": "Serie Temporali",
    "section": "Stabilità e unicità dei modelli",
    "text": "Stabilità e unicità dei modelli\nPer un modello \\(\\mathrm{MA}(1)\\), invece, consideriamo due modelli: \\[\n\\begin{align}\nx_t &= w_t + \\theta w_{t-1}, ~w_t\\sim\\mathcal{N}(0, 1) \\\\\ny_t &= \\nu_t + 1/\\theta \\nu_{t-1}, ~\\nu_t\\sim\\mathcal{N}(0, \\theta^2)\n\\end{align}\n\\] È evidente che \\(x_t\\) e \\(\\nu_t\\) hanno la stessa ACF e sono indistinguibili, dato che noi non conosciamo le innovazioni \\(w_t\\) e \\(\\nu_t\\) ma solo le due serie. Si può dimostrare che il risultato è generale (cioè non dipende dell’ordine \\(q\\)).\nÈ quindi necessario scegliere una delle due forme alternative. Per individuare quale, riscriviamo la serie come \\(w_t=-\\theta x_{t-1}+x_t\\), che ha la stessa forma della \\(\\mathrm{AR}(1)\\). Possiamo quindi scegliere l’alternativa che rispetta lo stesso criterio di stabilità: \\[\n\\left|\\sum_{i=1}^q \\theta_i\\right| &lt; 1\n\\] Tale alternativa si dice invertibile"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#stabilità-e-unicità-dei-modelli-2",
    "href": "slides/ADAS/7-serie_temporali.html#stabilità-e-unicità-dei-modelli-2",
    "title": "Serie Temporali",
    "section": "Stabilità e unicità dei modelli",
    "text": "Stabilità e unicità dei modelli\nIn generale, si può dimostrare che i criteri visti sopra corrispondono a imporre il requisito che le radici complesse dei polinomi \\[\n\\begin{align}\n\\Phi_p(z)&=1-\\phi_1 z-\\phi_2 z^2 - \\dots -\\phi_p z^p \\\\\n\\Theta_q(z)&=1+\\theta_1 z+\\theta_2 z^2 + \\dots +\\theta_q z^q\n\\end{align}\n\\] siano tutte strettamente fuori dal cerchio unitario sul piano complesso\nAd esempio: \\(\\mathrm{ARMA}(2,2)\\) con \\(\\Phi_2(z)= 1 + 0.9z + 0.1z^2\\) e \\(\\Theta_q(z) = 1 + 2z + 15z^2\\):\n\nabs(polyroot(c(1, -0.9, -0.1)))\n\n[1]  1 10\n\nabs(polyroot(c(1, 2, 15)))\n\n[1] 0.2581989 0.2581989\n\n\nCioè il termine AR non è causale e il termine MA non è invertibile (ma lo sarebbe \\(\\theta_q(z) = 1 + 1/2z + 1/15z^2\\))"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#ridondanza",
    "href": "slides/ADAS/7-serie_temporali.html#ridondanza",
    "title": "Serie Temporali",
    "section": "Ridondanza",
    "text": "Ridondanza\nConsideriamo il processo \\(x_t = w_t\\), con \\(w_t\\sim\\mathcal{N}(0, 1)\\): si tratta ovviamente di puro rumore casuale\nMoltiplichiamo entrambi i lati per \\(1-0.5B\\) per ottenere: \\[\nx_t = 0.5x_{t-1}-0.5w_{t-1} + w_t\n\\] che sembra un processo \\(\\mathrm{ARMA}(1,1)\\) ma ovviamente è sempre lo stesso rumore casuale. Come distinguere questi casi?\n\nsi scompongono in fattori i polinomi \\(\\Phi_p\\) e \\(\\Theta_q\\)\nsi eliminano i fattori comuni\nsi sicompongono i fattori per ottenere il modello non ridondante\n\nIn R si può ancora usare polyroot()"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#ridondanza-1",
    "href": "slides/ADAS/7-serie_temporali.html#ridondanza-1",
    "title": "Serie Temporali",
    "section": "Ridondanza",
    "text": "Ridondanza\nAd esempio, consideriamo il processo \\(x_t=0.4x_{t-1}+0.45x_{t-2}+w_t + w_{t-1}+0.25w_{t-2}\\), che usando l’operatore \\(B\\) diventa: \\[\n(1-0.4B-0.45B^2)x_t = (1+B+0.25B^2)w_t.\n\\]\nIn questa forma il processo sembra \\(\\mathrm{ARMA}(2,2)\\), tuttavia possiamo scomporre i due polinomi in fattori, usando polyroot() per calcolare le radici:\n\n# per Phi:\n-1/polyroot(c(1, -0.4, -0.45))\n\n[1] -0.9-6.887768e-21i  0.5+2.125854e-21i\n\n# per Theta:\n-1/polyroot(c(1, 1, 0.25))\n\n[1] 0.5-5.551115e-17i 0.5+5.551115e-17i\n\n\n\n\nNota: se \\(z_i,~ i=1,\\dots,n\\) sono le radici del polinomio \\(p(z)\\) di grado \\(n\\), allora il polinomio può essere scomposto nei fattori \\((1-1/z_1 z)\\cdot(1-1/z_2 z)\\dots(1-1/z_n z)\\)."
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#ridondanza-2",
    "href": "slides/ADAS/7-serie_temporali.html#ridondanza-2",
    "title": "Serie Temporali",
    "section": "Ridondanza",
    "text": "Ridondanza\nCome si vede, si può scrivere \\[\n\\begin{align}\n\\Phi_p(B)&=(1-0.9B)(1+0.5B) \\\\\n\\Theta_q(B)&=(1+0.5B)^2\n\\end{align}\n\\]\nEliminando il fattore comune \\((1+0.5B)\\) otteniamo il modello \\[\nx_t=0.9x_{t-1}+0.5w_{t-1}+w_t\n\\] che è un \\(\\mathrm{ARMA}(1,1)\\).\nQuindi ai criteri di causalità e di invertibilità si aggiunge il criterio di non ridondanza dei parametri, che si verifica eliminando ogni fattore comune dalla scomposizione in fattori dei polinomi \\(\\Phi_p(B)\\) e \\(\\Theta_q(B)\\)."
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#regressione-sarima",
    "href": "slides/ADAS/7-serie_temporali.html#regressione-sarima",
    "title": "Serie Temporali",
    "section": "Regressione (S)ARIMA",
    "text": "Regressione (S)ARIMA\nPer quanto detto sopra, un processo, o serie temporale, \\(x_t\\) può essere regredita mediante un modello ARIMA identificandone i parametri per minimizzazione degli scarti quadratici\nTuttavia, prima della regressione è necessario definire gli indici \\(p, d,q\\) adeguati, tali che non si abbia né sotto- né sovra-adattamento\nIn certi casi, inoltre, le serie sono periodiche: oltre ad un possibile trend sono soggette anche a ciclici andamenti oscillanti. In questi casi:\n\nsi rende la ST stazionaria per differenziazione\nsi separa un contributo a bassa frequenza, chiamato stagionale, da un contributo ad alta frequenza\nsi effettua separatamente la regressione del contributo stagionale e del contributo non stagionale come due processi ARIMA distinti e sovrapposti: tale regressione si chiama Seasonal ARIMA o SARIMA"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#regressione-sarima-1",
    "href": "slides/ADAS/7-serie_temporali.html#regressione-sarima-1",
    "title": "Serie Temporali",
    "section": "Regressione (S)ARIMA",
    "text": "Regressione (S)ARIMA\nInoltre, abbiamo visto che i modelli ARIMA si basano sull’ipotesi di serie temporali stazionarie in senso ampio\nQuindi, è necessario che sia il valor medio che la varianza siano costanti nel tempo\n\nLa media si stabilizza per differenziazione\nLa varianza si può stabilizzare mediante trasformazioni Box-Cox\n\nQuindi, nel caso più generale il modello è \\(\\mathrm{SARIMA}(p, d, q, p_s, d_s, q_s, \\lambda)\\)\nPrima di eseguire una regressione è quindi necessario definire i valori dei sette parametri, evitando sovra- e sotto-adattamento\n\n\\(d, d_s\\) possono essere identificati per tentativi, aumentando gradualmente i valori (prima di \\(d\\) e poi di \\(d_s\\)) finché la ACF è soddisfacente\ngli altri termini possono essere individuati verificando la bontà della regressione su una griglia di combinazioni e valutando un opportuno indice di merito"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#akaike-information-criterion",
    "href": "slides/ADAS/7-serie_temporali.html#akaike-information-criterion",
    "title": "Serie Temporali",
    "section": "Akaike Information Criterion",
    "text": "Akaike Information Criterion\nL’indice di merito più usato nella regressione SARIMA è l’Akaike Information Criterion, o AIC\nIn genere la qualità di una regressione con \\(k\\) parametri è misurata dalla somma quadratica dei residui, \\(SS_E(k)\\), normalizzata per la dimensione del campione \\(n\\): \\[\n\\hat{\\sigma_k^2} = \\frac{SS_E(k)}{n}\n\\] Questo indicatore si chiama Maximum Likelihood Estimator (MLE). Più questo valore è piccolo, meglio il modello si adatta ai dati\nTuttavia aumentando \\(k\\) si ha una diminuzione di MLE, a rischio di sovra-adattamento. Per questo motivo Akaike ha proposto di penalizzare MLE con il numero di parametri: \\[\n\\mathrm{AIC}=\\log(\\hat{\\sigma_k^2})+\\frac{n+2k}{n}\n\\] L’AIC va ovviamente minimizzato"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#altri-criteri-di-informazione",
    "href": "slides/ADAS/7-serie_temporali.html#altri-criteri-di-informazione",
    "title": "Serie Temporali",
    "section": "Altri criteri di informazione",
    "text": "Altri criteri di informazione\nOltre all’AIC esistono anche l’AIC corretto e il Bayesian Information Criterion, o BIC:\n\nAIC corretto: \\(\\mathrm{AIC}_c = \\log(\\hat{\\sigma_k^2})+\\frac{n+k}{n-k-2}\\)\nBIC: \\(\\mathrm{BIC} = \\log(\\hat{\\sigma_k^2})+\\frac{k\\log(n)}{n}\\)\n\nIl BIC penalizza maggiormente la dimensione del modello, per cui è preferito per campioni molto grandi (migliaia di osservazioni), per i quali AIC e AICc tenderebbero a favorire modelli inutilmente complessi (troppi parametri, sovra-adattamento)\nCome l’AIC, anche AICc e BIC vanno minimizzati"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#regressione",
    "href": "slides/ADAS/7-serie_temporali.html#regressione",
    "title": "Serie Temporali",
    "section": "Regressione",
    "text": "Regressione\nLa libreria R forecast fornisce la funzione auto.arima() che valuta gli indicatori su una griglia di combinazioni dei sette parametri e fornisce la regressione migliore:\n\n\n\nforecast::auto.arima(AirPassengers, lambda=\"auto\", trace=T)\n\n\n ARIMA(2,1,2)(1,1,1)[12]                    : -890.0522\n ARIMA(0,1,0)(0,1,0)[12]                    : -845.0766\n ARIMA(1,1,0)(1,1,0)[12]                    : -885.6939\n ARIMA(0,1,1)(0,1,1)[12]                    : -896.9901\n ARIMA(0,1,1)(0,1,0)[12]                    : -860.1426\n ARIMA(0,1,1)(1,1,1)[12]                    : -895.2944\n ARIMA(0,1,1)(0,1,2)[12]                    : -895.3558\n ARIMA(0,1,1)(1,1,0)[12]                    : -889.5331\n ARIMA(0,1,1)(1,1,2)[12]                    : Inf\n ARIMA(0,1,0)(0,1,1)[12]                    : -880.0685\n ARIMA(1,1,1)(0,1,1)[12]                    : -896.1031\n ARIMA(0,1,2)(0,1,1)[12]                    : -895.698\n ARIMA(1,1,0)(0,1,1)[12]                    : -893.2768\n ARIMA(1,1,2)(0,1,1)[12]                    : -894.0835\n\n Best model: ARIMA(0,1,1)(0,1,1)[12]                    \n\n\nSeries: AirPassengers \nARIMA(0,1,1)(0,1,1)[12] \nBox Cox transformation: lambda= -0.2947046 \n\nCoefficients:\n          ma1     sma1\n      -0.4355  -0.5847\ns.e.   0.0908   0.0725\n\nsigma^2 = 5.856e-05:  log likelihood = 451.59\nAIC=-897.18   AICc=-896.99   BIC=-888.55\n\n\n\n\nSerie temporaleStabilizzata"
  },
  {
    "objectID": "slides/ADAS/7-serie_temporali.html#previsione",
    "href": "slides/ADAS/7-serie_temporali.html#previsione",
    "title": "Serie Temporali",
    "section": "Previsione",
    "text": "Previsione\n\n\nUna volta selezionato il modello più adatto (quello con AIC minimo) si può procedere a regressione e predizione\nLa predizione può essere ottenuta con la funzione forecast()\nTale funzione riporta anche le bande di confidenza al 95%\n\n\nPredizioneDettaglio"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#incertezza",
    "href": "slides/ADAS/5-misura.html#incertezza",
    "title": "Misura, Incertezza, Taratura",
    "section": "Incertezza",
    "text": "Incertezza\n\n\nIncertezza, precisione, accuratezza\n\nL’incertezza è l’inverso della precisione, a sua volta sinonimo di ripetibilità\nL’accuratezza è sinonimo di mancata polarizzazione\n\nÈ meglio avere uno strumento preciso o accurato?"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#norma-uni4546",
    "href": "slides/ADAS/5-misura.html#norma-uni4546",
    "title": "Misura, Incertezza, Taratura",
    "section": "Norma UNI4546",
    "text": "Norma UNI4546\nIntrodotta nel 1984, dà alcune definizioni:\n\n\nDefinizioni\n\n\nMisurazione: l’atto del misurare, assegnare valori numerici a grandezze fisiche\nParametro: grandezza di un sistema fisico alla quale assegnare valori numerici\nMisurando: un parametro sottoposto a misurazione\nMisura: il risultato di una misurazione\nMetrologia: disciplina che riguarda la qualità delle misure"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#norma-uni4546-1",
    "href": "slides/ADAS/5-misura.html#norma-uni4546-1",
    "title": "Misura, Incertezza, Taratura",
    "section": "Norma UNI4546",
    "text": "Norma UNI4546\nUna misura, che è il risultato di una misurazione, rappresenta un parametro di un sistema considerato in un determinato stato\nÈ una terna costituita da:\n\nun valore\nun’incertezza\nun’unità di misura\n\nAd esempio: la mia statura è 183±0.5 cm\nEsistono due tipi di grandezze:\n\nestensive, per cui vale la somma (lunghezze, correnti elettriche, velocità)\nintensive, esprimono un ordine e non vale la somma (pressioni, temperatura)\n\n\n\nL’incertezza è parte integrante di una misura; senza incertezza la misura non è valida"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#norma-uni4546-2",
    "href": "slides/ADAS/5-misura.html#norma-uni4546-2",
    "title": "Misura, Incertezza, Taratura",
    "section": "Norma UNI4546",
    "text": "Norma UNI4546\n\nMisurazione diretta: ottenuta per confronto diretto con un campione noto\nMisurazione indiretta: si misurano grandezze secondarie collegate a quella di interesse mediante un modello\n\nNel caso di misurazione indiretta, lo strumento di misura è basato su un trasduttore: un dispositivo che trasforma una grandezza fisica in ingresso in un’altra grandezza (es. forza in carica elettrica)\nUn modello è un insieme di relazioni tra parametri descrivente le interazioni (m. statico) e possibilmente l’evoluzione nel tempo (m. dinamico) di un sistema\nUn modello può essere analitico o numerico\n\n\nIl modello di misura deve essere costruito sulla base della misura stessa: non deve essere necessariamente esaustivo né basato sulla comprensione fisica del sistema (cioè può anche essere empirico)"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#norma-gum-isoiec-98-32008",
    "href": "slides/ADAS/5-misura.html#norma-gum-isoiec-98-32008",
    "title": "Misura, Incertezza, Taratura",
    "section": "Norma GUM (ISO/IEC 98-3:2008)",
    "text": "Norma GUM (ISO/IEC 98-3:2008)\nGuide to the expression of Uncertainty in Measurement è il riferimento normativo per la terminologia e i metodi di calcolo dell’incertezza nelle misurazioni\nSecondo la GUM il risultato di una misurazione è una variabile aleatoria e va trattato in quanto tale\nLa GUM ha abolito i termini di errore (sostituito con incertezza) e di valore vero (sostituito con stima)\nCioè si assume che il valore vero di una grandezza non sia conoscibile: anche aumentando all’infinito la precisione di uno strumento, prima o poi si arriva a livello atomico, per cui vale il principio di indeterminazione di Pauli\nQuindi, se non ha senso parlare di valore vero, allora non ha senso nemmeno parlare di errore di misura"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#norma-gum-isoiec-98-32008-1",
    "href": "slides/ADAS/5-misura.html#norma-gum-isoiec-98-32008-1",
    "title": "Misura, Incertezza, Taratura",
    "section": "Norma GUM (ISO/IEC 98-3:2008)",
    "text": "Norma GUM (ISO/IEC 98-3:2008)\nLa GUM definisce:\n\n\nDefinizioni\n\n\nL’incertezza standard (standard uncertainty) \\(u_x\\) della variabile \\(x\\) come la deviazione standard del valor medio di \\(x\\): \\[\nu_x=\\frac{s_x}{\\sqrt{n}}=\\sqrt{\\frac{\\sum_{i=1}^n(x_i-\\bar x)^2}{n(n-1)}}\n\\]\nL’incertezza relativa come rapporto tra l’incertezza standard e la media della variabile; è adimensionale e utile per confronti: \\[\nu_{x\\mathrm{rel}}=\\frac{u_x}{|\\bar x|}\n\\]"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#norma-gum-isoiec-98-32008-2",
    "href": "slides/ADAS/5-misura.html#norma-gum-isoiec-98-32008-2",
    "title": "Misura, Incertezza, Taratura",
    "section": "Norma GUM (ISO/IEC 98-3:2008)",
    "text": "Norma GUM (ISO/IEC 98-3:2008)\n\n\nDefinizioni\n\nRicordando che l’intervallo di confidenza è definito come \\[\nP\\left(-t_{n-1,\\alpha/2}\\leq\\frac{|\\bar x - \\mu_0|}{s_x/\\sqrt{n}}\\leq t_{n-1,\\alpha/2}\\right) = 1-\\alpha\n\\] risulta che l’intervallo \\(\\bar x \\pm t_{\\alpha/2,n-1}u_x\\) contiene il valore atteso di \\(x\\) con una confidenza \\(1-\\alpha\\)\nQuindi la GUM definisce anche:\n\nL’incertezza estesa come la semi-ampiezza dell’intervallo di confidenza su \\(\\bar x\\):\n\n\\[\nU_x=t_{\\alpha/2,n-1}\\frac{s_x}{\\sqrt{n}}=t_{\\alpha/2,n-1}u_x = k_{n-1}u_x\n\\]\nIl termine \\(k_{n-1}\\) è chiamato fattore di copertura, e dipende solo dalla dimensione del campione (è il quantile della distribuzione T di Student)."
  },
  {
    "objectID": "slides/ADAS/5-misura.html#fattore-di-copertura",
    "href": "slides/ADAS/5-misura.html#fattore-di-copertura",
    "title": "Misura, Incertezza, Taratura",
    "section": "Fattore di copertura",
    "text": "Fattore di copertura\n\n\nRicordare che per \\(n&gt;50\\) i quantili della normale e della T di Student sono indistinguibili a livello pratico\nPer questo motivo, per campioni sufficientemente grandi la GUM consente di adottare i fattori di copertura ricavati dai quantili di \\(\\mathcal{N}(0,1)\\)\nSalvo indicazioni, per esprimere una misura si usa sempre l’incertezza standard (27.5±0.1mm, in cui \\(u_x=0.1\\))\n\n\n\n\n\n\nLivello di confidenza\n\\(\\alpha\\)\nFattore di copertura\n\n\n\n\n68.27%\n0.3173\n1.0000\n\n\n90%\n0.1000\n1.6449\n\n\n95%\n0.0500\n1.9600\n\n\n95.45%\n0.0455\n2.0000\n\n\n99%\n0.0100\n2.5758\n\n\n99.73%\n0.0027\n3.0000\n\n\n\n\n\n\nSe si usa l’incertezza estesa è necessario accompagnarla dal livello di confidenza (27.5±0.1mm al 95%)\n\n\nPer campioni con meno di 50 elementi è opportuno calcolare il fattore di copertura come \\(t_{\\alpha/2,n-1}\\)"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#modello-statico-di-uno-strumento-di-misura",
    "href": "slides/ADAS/5-misura.html#modello-statico-di-uno-strumento-di-misura",
    "title": "Misura, Incertezza, Taratura",
    "section": "Modello statico di uno strumento di misura",
    "text": "Modello statico di uno strumento di misura\nNel caso di misurazioni indirette, è fondamentale disporre di un modello che descriva il comportamento del trasduttore, cioè la relazione tra uscita e ingresso\nOgni modello dipende da uno o più parametri numerici che devono essere identificati\nQuesta operazione di identificazione dei parametri del modello di misura si chiama taratura\nLa taratura punta a definire la correlazione \\(y=f(m)\\) tra l’ingresso misurando \\(m\\) e l’uscita del trasduttore \\(y\\), e la relativa incertezza. La \\(f(\\cdot)\\) è detta caratteristica statica dello strumento\nD’ora in avanti considereremo solo il caso di sistemi statici, cioè in stato stazionario, per i quali il tempo non è una variabile di modello\nI sistemi dinamici verranno presi in considerazione nel secondo modulo di questo corso\n\n\nSi parla di taratura statica o dinamica per una operazione di misurazione statica o dinamica, rispettivamente"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#taratura",
    "href": "slides/ADAS/5-misura.html#taratura",
    "title": "Misura, Incertezza, Taratura",
    "section": "Taratura",
    "text": "Taratura\n\n\nUno strumento fornisce la misura mediante inversione della caratteristica statica: \\(m=f^{-1}(y)\\)\nPerché la \\(f(\\cdot)\\) sia nota è necessario identificarne i parametri mediante regressione\nLa regressione viene effettuata a partire da una serie di coppie \\((m_i, y_i)\\) ottenute:\n\nda una serie di misurandi noti \\(m_i\\)\nda una serie di misurazioni \\(y_i\\) ottenute con uno strumento di qualità migliore di quello in taratura\n\n\n\nMisurazioneTaratura\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPer “strumento migliore” si intende uno strumento con una precisione migliore di almeno un ordine di grandezza"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#taratura-2",
    "href": "slides/ADAS/5-misura.html#taratura-2",
    "title": "Misura, Incertezza, Taratura",
    "section": "Taratura",
    "text": "Taratura\nUna taratura statica si sviluppa quindi su quattro passaggi:\n\nsviluppo del modello dello strumento: mediante analisi dei principi fisici si definisce la caratteristica statica come relazione (analitica, numerica o mista) tra ingresso e uscita. Idealmente, un modello completo comprende anche gli ingressi di disturbo\nraccolta dei dati di taratura: una campagna sperimentale fornisce le coppie \\((m_i, y_i)\\), che vanno raccolte con \\(m\\) in ordine casuale\nregressione: si identificano i parametri del modello\nvalidazione del modello: si verifica l’adeguatezza del modello regredito mediante analisi dei residui\n\nLa taratura deve anche definire l’incertezza dello strumento, dovuta:\n\nal modello (la forma della caratteristica statica)\nai parametri del modello\nalla stima del misurando (dovuta a ingressi di disturbo)"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#ingressi-di-disturbo",
    "href": "slides/ADAS/5-misura.html#ingressi-di-disturbo",
    "title": "Misura, Incertezza, Taratura",
    "section": "Ingressi di disturbo",
    "text": "Ingressi di disturbo\n\n\nGli ingressi fonte di incertezza possono essere:\n\ningressi modificanti: modificano la caratteristica statica, per cui ad uno stesso valore di \\(m\\) possono corrispondere diversi valori di \\(y\\) per via di un cambiamento della forma di \\(f(\\cdot)\\) o del valore dei parametri (ad es. effetto della temperatura)\ningressi interferenti: si sommano direttamente all’uscita dello strumento, per cui \\(y=f(m)+y_d\\), e hanno tipicamente un carattere stocastico (ad es. vibrazioni, disturbi elettrici)\n\n\n\n\n\n\nL’incertezza si riferisce solo alla componente aleatoria e non prevedibile della misura; si assume che ogni componente di deviazione sistematica sia già stato corretto"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#esempio-dinamometro-a-mensola",
    "href": "slides/ADAS/5-misura.html#esempio-dinamometro-a-mensola",
    "title": "Misura, Incertezza, Taratura",
    "section": "Esempio: dinamometro a mensola",
    "text": "Esempio: dinamometro a mensola\n\n\nÈ uno strumento per la misura della forza peso che sfrutta:\n\nl’elasticità di una trave snella per convertire una forza in una deformazione\nun estensimetro per convertire una deformazione in una variazione di resistenza\nun circuito elettrico con voltmetro per convertire la variazione di resistenza in variazione di corrente, mediante amplificazione\n\nIl modello dello strumento fornisce:\n\n\nDinamometroAmplificatore\n\n\n\n\n\n\n\n\n\n\n\\[\nV=3/2GV_i\\frac{lG_F}{EBH^2}F+V_0 = V_0+ KF\n\\]"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#esempio-dinamometro-a-mensola-1",
    "href": "slides/ADAS/5-misura.html#esempio-dinamometro-a-mensola-1",
    "title": "Misura, Incertezza, Taratura",
    "section": "Esempio: dinamometro a mensola",
    "text": "Esempio: dinamometro a mensola\nLa caratteristica statica dello strumento dipende, oltre che dal misurando \\(F\\), da altri 8 parametri\nPer effettuare la taratura si tengono il più possibile costanti tutti i parametri, eccetto il misurando\nI parametri che non è possibile mantenere costanti si accetta che fluttuino, ripetendo le misurazioni e mediando i risultati: si parla di controllo statistico del processo di taratura, che mitiga l’effetto degli ingressi interferenti\nÈ la condizione in cui si applica il teorema del limite centrale\nGli ingressi modificanti sono più complessi da trattare e richiedono una modifica del modello e il passaggio da uno strumento di misura a un sistema di misura\nAd esempio, la temperatura può influire su tutti i parametri della caratteristica statica: se affianco uno strumento di misura della temperatura e arricchisco il modello con la dipendenza dalla temperatura, posso compensare l’effetto degli ingressi modificanti\n\n\nUn sistema di misura è costituito da più strumenti che lavorano assieme"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#casualizzazione-della-sequenza-di-taratura",
    "href": "slides/ADAS/5-misura.html#casualizzazione-della-sequenza-di-taratura",
    "title": "Misura, Incertezza, Taratura",
    "section": "Casualizzazione della sequenza di taratura",
    "text": "Casualizzazione della sequenza di taratura\nCompensare l’effetto degli ingressi modificanti durante la taratura può non essere semplice o economico\nSe però raccolgo le coppie \\((m_y, y_i)\\) in ordine casuale anziché in ordine di \\(m_i\\), ottengo il risultato di distribuire casualmente l’effetto degli ingressi modificanti\nIn questo modo trasformo gli ingressi modificanti in ingressi interferenti, dei quali posso mitigare l’effetto mediante controllo statistico\nConfrontiamo ora la taratura del dinamometro a mensola effettuata senza casualizzazione e con casualizzazione, nel caso di un ingresso modificante (la temperatura) non preso in considerazione dall’operatore"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#taratura-del-dinamometro-senza-casualizzazione",
    "href": "slides/ADAS/5-misura.html#taratura-del-dinamometro-senza-casualizzazione",
    "title": "Misura, Incertezza, Taratura",
    "section": "Taratura del dinamometro (senza casualizzazione)",
    "text": "Taratura del dinamometro (senza casualizzazione)\n\n\nNel laboratorio di taratura, dopo 4 ore dall’inizio del processo di taratura, il termostato viene modificato e la temperatura comincia a variare tra 20°C e 25°C\nL’addetto alla calibrazione non si accorge del cambiamento né registra la temperatura"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#taratura-del-dinamometro-senza-casualizzazione-1",
    "href": "slides/ADAS/5-misura.html#taratura-del-dinamometro-senza-casualizzazione-1",
    "title": "Misura, Incertezza, Taratura",
    "section": "Taratura del dinamometro (senza casualizzazione)",
    "text": "Taratura del dinamometro (senza casualizzazione)\n\n\nL’addetto registra la tensione in uscita in corrispondenza di 50 valori di forza applicati, variabili tra 10 N e 100 N, in ordine crescente della forza\nDato che il modello dello strumento prevede una relazione lineare \\(V=V_0+KF\\), l’addetto effettua una regressione lineare\nLo studio dei residui però evidenzia un pattern con un minimo, per cui l’addetto sospetta un sotto-adattamento e quindi tenta una seconda regressione con un modello quadratico\n\n\nRegressioneResidui"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#taratura-del-dinamometro-senza-casualizzazione-2",
    "href": "slides/ADAS/5-misura.html#taratura-del-dinamometro-senza-casualizzazione-2",
    "title": "Misura, Incertezza, Taratura",
    "section": "Taratura del dinamometro (senza casualizzazione)",
    "text": "Taratura del dinamometro (senza casualizzazione)\n\n\nL’addetto verifica quindi una regressione con un modello di secondo grado\nL’analisi dei residui lo soddisfa e quindi accetta la nuova caratteristica statica tarata come un polinomio di secondo grado\nNota: questa caratteristica ha perso la relazione con la fisica del problema ed è quindi puramente empirica\nTuttavia, dato che la variazione di temperatura non è stata registrata né notata, l’addetto non ha modo di accorgersi che la caratteristica così tarata è ovviamente sbagliata e in pratica inutilizzabile\n\n\nRegressioneResidui\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI residui non sono del tutto privi di pattern (andamento a “M”). Tuttavia, aumentando il grado della regressione si può raggiungere un livello (grado 6) in cui i residui sono effettivamente privi di pattern, ma ciò tuttavia non invalida quanto sopra detto"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#taratura-del-dinamometro-con-casualizzazione",
    "href": "slides/ADAS/5-misura.html#taratura-del-dinamometro-con-casualizzazione",
    "title": "Misura, Incertezza, Taratura",
    "section": "Taratura del dinamometro (con casualizzazione)",
    "text": "Taratura del dinamometro (con casualizzazione)\n\n\nRivediamo cosa sarebbe successo raccogliendo le coppie \\((m_i,y_i)\\) in ordine casuale\nIn questo caso l’effetto di aumento della temperatura è nullo sulle coppie raccolte (con valori casuali di \\(m_i\\)!) prima di 4 ore; successivamente questo effetto si distribuisce casualmente su tutti i valori del misurando (diventa un ingresso interferente)\nLa regressione lineare di primo grado questa volta è adatta, anche se la varianza non è costante\nTuttavia, osservando i residui in funzione del tempo si nota che a circa 4 ore inizia a succedere qualcosa che fa aumentare i residui\n\n\nRegressioneResidui (F)Residui (t)"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#taratura-del-dinamometro-con-casualizzazione-1",
    "href": "slides/ADAS/5-misura.html#taratura-del-dinamometro-con-casualizzazione-1",
    "title": "Misura, Incertezza, Taratura",
    "section": "Taratura del dinamometro (con casualizzazione)",
    "text": "Taratura del dinamometro (con casualizzazione)\n\n\nSe variamo il colore dei punti in funzione del tempo in cui sono state effettuate le singole prove, osserviamo che c’è una fascia di punti in basso con andamento lineare e varianza costante, tutti raccolti a meno di 4 ore dall’inizio\nScartando dalla regressione tutti i punti raccolti dopo 4 ore, otteniamo una regressione lineare con residui stretti e regolari\nLa caratteristica statica così identificata rappresenta correttamente il comportamento dello strumento a 20°C\n\n\nRegressioneRegressione \\(t&lt;4~\\mathrm{h}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecondo la Norma ISO 1, tutte le misure vanno riferite alla temperatura di 20°C"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#basi",
    "href": "slides/ADAS/5-misura.html#basi",
    "title": "Misura, Incertezza, Taratura",
    "section": "Basi",
    "text": "Basi\nSupponiamo di voler misurare la velocità media di un veicolo in un tratto di strada\nPossiamo esprimere la velocità come rapporto tra la distanza percorsa \\(d\\) e il tempo impiegato \\(t\\): \\[\nv=\\frac{d}{t}\n\\] Sia la misura di distanza che quella di tempo sono accompagnate da una loro incertezza standard\nQual è l’incertezza standard sulla misura indiretta di velocità?\nSecondo la GUM può essere determinata in due modi:\n\nanalitico, mediante la legge di propagazione dell’incertezza\nnumerico, mediante il metodo Monte Carlo"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#metodo-analitico",
    "href": "slides/ADAS/5-misura.html#metodo-analitico",
    "title": "Misura, Incertezza, Taratura",
    "section": "Metodo analitico",
    "text": "Metodo analitico\nSia \\(y=f(x_1,x_2,\\dots,x_n)\\): vogliamo esprimere l’incertezza tipo combinata note che siano le \\(u_1,u_2,\\dots,u_n\\)\nEsprimiamo la variazione di \\(y\\) in un intorno di \\(x_0\\) mediante sviluppo in serie di Taylor per una sola variabile \\(x\\): \\[\nf(x_0+\\Delta x)=f(x_0)+\\frac{df}{dx}\\Delta x + \\frac{d^2f}{dx^2}\\frac{\\Delta x^2}{2!} + \\frac{d^3f}{dx^3}\\frac{\\Delta x^2}{3!}+o(\\Delta x^3)\n\\] Nel caso di \\(n\\) variabili, e arrestandosi al termine di primo grado: \\[\n\\begin{align}\ny =&f(x_1,x_2,\\dots,x_n) \\\\\n\\simeq&f\\left( \\bar x_1 + \\Delta x_1, \\bar x_2 + \\Delta x_2, \\dots, \\bar x_n + \\Delta x_n  \\right) \\\\\n=& f(\\bar x_1, \\bar x_2,\\dots,\\bar x_n) + \\sum_{i=1}^n \\frac{\\partial f}{\\partial x_i}(x_i-\\bar x_i)\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#metodo-analitico-1",
    "href": "slides/ADAS/5-misura.html#metodo-analitico-1",
    "title": "Misura, Incertezza, Taratura",
    "section": "Metodo analitico",
    "text": "Metodo analitico\nSi noti che: \\[\n\\begin{align}\nE(y) &= E\\left(f(\\bar x_1, \\bar x_2, \\dots, \\bar x_k) + \\sum_{i=1}^k \\frac{\\partial f}{\\partial x_i} (x_i - \\bar x_i)\\right) \\\\\n&= E(f(\\bar x_1, \\bar x_2, \\dots, \\bar x_k)) + \\sum_{i=1}^k \\frac{\\partial f}{\\partial x_i} E(x_i - \\bar x_i) \\\\\n&= f(\\bar x_1, \\bar x_2, \\dots, \\bar x_k)\n\\end{align}\n\\] cioè il valore medio di \\(f(\\cdot)\\) è la stessa funzione applicata ai valori medi: \\(\\bar y=f(\\bar x_1, \\bar x_2,\\dots,\\bar x_k)\\)"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#metodo-analitico-2",
    "href": "slides/ADAS/5-misura.html#metodo-analitico-2",
    "title": "Misura, Incertezza, Taratura",
    "section": "Metodo analitico",
    "text": "Metodo analitico\nPossiamo quindi scrivere che: \\[\n\\begin{align}\nE(y - f(\\bar x_1, \\bar x_2, \\dots, \\bar x_k)) &= E\\left(\\sum_{i=1}^k \\frac{\\partial f}{\\partial x_i} (x_i - \\bar x_i) \\right) \\\\\nu^2_y=E((y-\\bar y)^2) &= E\\left(\\left(\\sum_{i=1}^k \\frac{\\partial f}{\\partial x_i}(x_i - \\bar x_i)\\right)^2\\right) \\\\\nu_y^2 &= \\sum_{j=1}^k\\sum_{i=1}^k \\frac{\\partial f}{\\partial x_i}\\frac{\\partial f}{\\partial x_j}u_{i,j}^2\n\\end{align}\n\\] dove \\(u_{i,j}\\) è la covarianza \\(u_{i,j}=\\textrm{Cov}(x_i, x_j)=E((x_i-\\bar x_j)(x_j - \\bar x_i))\\)"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#metodo-analitico-3",
    "href": "slides/ADAS/5-misura.html#metodo-analitico-3",
    "title": "Misura, Incertezza, Taratura",
    "section": "Metodo analitico",
    "text": "Metodo analitico\nNel caso in cui le \\(x_i\\) siano tutte indipendenti, cioè \\(u_{i,j}=0~\\forall i\\neq j\\), vale la relazione semplificata: \\[\nu_y = \\sqrt{\\sum_{i=1}^n \\left(\\frac{\\partial f}{\\partial x_i}\\right)^2 u_i^2}\n\\] nota come legge di propagazione delle incertezze\nLe derivate parziali che compaiono nella LPI sono dette coefficienti di sensibilità e vanno valutati nel valore medio della rispettiva variabile \\(x_i\\).\nIl valore dei coefficienti di sensibilità consente di determinare quale delle misure combinande contribuisce maggiormente all’incertezza della misura combinata\nVolendo migliorare la misura combinata conviene investire soprattutto sulle misure combinande con un maggiore coefficiente di sensibilità\n\n\nL’ipotesi che le \\(x_i\\) siano indipendenti va verificata, ad esempio con un test di correlazione"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#esempio-volume-di-un-cilindro",
    "href": "slides/ADAS/5-misura.html#esempio-volume-di-un-cilindro",
    "title": "Misura, Incertezza, Taratura",
    "section": "Esempio: volume di un cilindro",
    "text": "Esempio: volume di un cilindro\nSiccome \\(V=\\pi r^2l\\) vogliamo calcolare l’incertezza standard sul volume di un cilindro, note che siano le incertezze standard su raggio, \\(u_r\\), e lunghezza, \\(u_l\\)\nEntrambe le incertezze sono calcolate da un campione di 20 elementi, entrambi con deviazione standard 2.24 mm: \\(u=s/\\sqrt{n} = 2.24/\\sqrt{20} = 0.5~\\mathrm{mm}\\)\n\\[\nu_V=\\sqrt{\\left(\\frac{\\partial V}{\\partial r}\\right)^2u_r^2 + \\left(\\frac{\\partial V}{\\partial l}\\right)^2u_l^2 }=\n\\pi \\bar r \\sqrt{4\\bar l^2 u_r^2+\\bar r^2 u_l^2}\n\\]\nNel caso in cui sia \\(r=\\) 120.0±0.5 mm e \\(l=\\) 450.0±0.5, l’incertezza tipo combinata risulta pari a: \\[\n\\begin{align}\nu_V&=&\\pi 120\\sqrt{4\\cdot 450^2\\cdot 0.5^2+120^2\\cdot 0.5^2} \\\\\n&=&171\\times10^3~\\mathrm{ mm^3}\\simeq2 \\times 10^5~\\mathrm{mm^3}\n\\end{align}\n\\] e quindi il volume misurato è: \\[\nV=\\pi \\cdot 120^2 \\cdot 450 = (20.4\\pm0.2)\\times 10^6~\\mathrm{mm^3}= (20.4\\pm0.2)~\\mathrm{l}\n\\]"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#metodo-monte-carlo",
    "href": "slides/ADAS/5-misura.html#metodo-monte-carlo",
    "title": "Misura, Incertezza, Taratura",
    "section": "Metodo Monte Carlo",
    "text": "Metodo Monte Carlo\nSe la relazione \\(y=f(x_1,x_2,\\dots, x_n)\\) non è differenziabile oppure non è nota in forma analitica (ma solo numerica), allora la LPI non è applicabile\nIn questo caso la GUM prevede l’applicazione di un metodo numerico noto come Monte Carlo, perché, come alla roulette, prevede la generazione di numeri casuali per simulare una distribuzione:\n\nsi genera un elevato (\\(\\geq10000\\)) numero di n-uple \\(x_1,x_2,\\dots, x_n\\), generando per ogni \\(x_i\\) un numero casuale da una distribuzione rappresentativa del caso reale\nsi applica la \\(y=f(x_1,x_2,\\dots, x_n)\\) a ciascuna n-upla, generando altrettanti valori di \\(y\\)\nsi studia la distribuzione empirica delle \\(y\\) così generate, calcolando l’intervallo di confidenza mediante il metodo dei quantili\n\n\n\nNota: il metodo non fa alcuna assunzione sulla distribuzione delle \\(x_i\\), anzi, la sua efficacia sta proprio nel prescindere da qualsiasi ipotesi di normalità"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#esempio-volume-di-un-cilindro-1",
    "href": "slides/ADAS/5-misura.html#esempio-volume-di-un-cilindro-1",
    "title": "Misura, Incertezza, Taratura",
    "section": "Esempio: volume di un cilindro",
    "text": "Esempio: volume di un cilindro\n\n\nPrimo passo: generazione dei campioni delle misure di raggio e lunghezza\nDopo uno studio della tecnologia di produzione del cilindro, si accerta che\n\nla distribuzione del raggio è simmetrica, con media 120.0 mm e deviazione standard 0.5 mm\nla distribuzione della lunghezza è gobba, con media 450.0 mm e deviazione standard 0.5 mm\n\nSi generano due campioni da 10 000 elementi ciascuno\n\n\nRaggioLunghezza\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDato che generiamo un’elevata quantità di campioni, secondo la GUM assumiamo che l’incertezza standard coincida con la deviazione standard"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#esempio-volume-di-un-cilindro-2",
    "href": "slides/ADAS/5-misura.html#esempio-volume-di-un-cilindro-2",
    "title": "Misura, Incertezza, Taratura",
    "section": "Esempio: volume di un cilindro",
    "text": "Esempio: volume di un cilindro\n\n\nSecondo passo: calcolo del campione di misure derivate di volume\nSi applica semplicemente la \\(V=\\pi r^2l\\) a ciascuna coppia \\((r,l)\\) generando altrettanti valori di \\(V\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSi noti, nuovamente, come la composizione di più distribuzioni, anche differenti, porti sempre a distribuzioni tendenti alla normale"
  },
  {
    "objectID": "slides/ADAS/5-misura.html#esempio-volume-di-un-cilindro-3",
    "href": "slides/ADAS/5-misura.html#esempio-volume-di-un-cilindro-3",
    "title": "Misura, Incertezza, Taratura",
    "section": "Esempio: volume di un cilindro",
    "text": "Esempio: volume di un cilindro\n\n\nTerzo passo: valutazione della distribuzione e calcolo dell’incertezza derivata\nLa media e la deviazione standard dei volumi calcolati corrispondono al valore atteso e all’incertezza standard sulla misura di volume\nConsiderando un’unica cifra significativa per l’incertezza e arrotondando la media alla stessa risoluzione, si ottiene \\[\nV=(20.4\\pm0.2)~\\mathrm{l}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNota: un ulteriore vantaggio del metodo Monte Carlo è che consente di analizzare anche la forma della distribuzione della misura derivata (ad esempio mediante un diagramma Q-Q)"
  },
  {
    "objectID": "slides/ADAS/3-regressione.html#basi",
    "href": "slides/ADAS/3-regressione.html#basi",
    "title": "Regressione",
    "section": "Basi",
    "text": "Basi\n\nUn modello di un sistema fisico può essere espresso come: \\[\ny=f(x_1, x_2, \\dots, x_n, c_1, c_2, \\dots, c_m)\n\\] dove \\(x_i\\) sono le variabili (aleatorie) fisiche, dette regressori o predittori, mentre le \\(c_i\\) sono i parametri (costanti) del modello\nSe \\(n=1\\) c’è un unico predittore e il modello si dice semplice\nAd esempio, \\(y=a+bx+cx^2\\) è un modello semplice con parametri \\(a,~b,~c\\)\nregredire il modello significa effettuare delle misurazioni di \\(y\\) per diversi valori di \\(x\\) e determinare i valori dei parametri \\(a,~b,~c\\) che minimizzano la distanza tra il modello e le osservazioni sperimentali"
  },
  {
    "objectID": "slides/ADAS/3-regressione.html#tipi-di-regressione",
    "href": "slides/ADAS/3-regressione.html#tipi-di-regressione",
    "title": "Regressione",
    "section": "Tipi di regressione",
    "text": "Tipi di regressione\nPrenderemo in considerazione tre tipi di regressione:\n\nRegressione lineare: il modello è una combinazione lineare dei parametri\nRegressione lineare generalizzata\nRegressione ai minimi quadrati: i parametri sono combinati in modo non lineare"
  },
  {
    "objectID": "slides/ADAS/3-regressione.html#basi-1",
    "href": "slides/ADAS/3-regressione.html#basi-1",
    "title": "Regressione",
    "section": "Basi",
    "text": "Basi\n\n\nDefinizioni\n\nSi definisce il modello statistico del processo da regredire come segue: \\[\ny_i=f(x_{1i}, x_{2i}, \\dots, x_{ni}, c_1, c_2, \\dots, c_{n+1}) + \\varepsilon_i=\\hat{y_i} + \\varepsilon_i,~i=1,2,\\dots,N\n\\]\ndove \\(i\\) è l’indice di osservazione (\\(N\\geq n+1\\) in totale), \\(\\hat{y_i}\\) è il valore regredito, o predizione, in corrispondenza dell’osservazione \\(i\\), e \\(\\varepsilon_i\\) sono i residui\n\nIl valore regredito corrisponde alla componente deterministica\nIl residuo è la componente aleatoria\nSi assume l’ipotesi di normalità dei residui, cioè che \\(\\varepsilon_i \\sim\\mathcal{N}(0, \\sigma^2)\\)\n\n\n\n\nSe la \\(f(\\cdot)\\) è una funzione analitica e lineare nei coefficienti, allora possiamo esprimerla come \\(\\mathbf{A}\\mathbf{k}=\\mathbf{y}\\), dove\n\n\\(\\mathbf{y}\\) è il vettore delle \\(y_i,~i=1\\dots N\\)\n\\(\\mathbf{k}\\) è il vettore dei parametri \\(c_j,~j=1\\dots n+1\\)\n\\(\\mathbf{A}\\) una matrice \\(N\\times (n+1)\\) così composta:\n\n\n\\[\n\\mathbf A = \\begin{bmatrix}\nx_1^{n-1} & x_1^{n-2} & \\dots & x_1 & 1 \\\\\nx_2^{n-1} & x_2^{n-2} & \\dots & x_2 & 1 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\nx_N^{n-1} & x_N^{n-2} & \\dots & x_N & 1 \\\\\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/ADAS/3-regressione.html#basi-2",
    "href": "slides/ADAS/3-regressione.html#basi-2",
    "title": "Regressione",
    "section": "Basi",
    "text": "Basi\nL’equazione lineare può essere risolta con il metodo della pseudo-inversa: \\[\n\\begin{align}\n\\mathbf A^T \\mathbf A\\cdot \\mathbf{k} &= \\mathbf A^T \\cdot \\mathbf{y} \\\\\n(\\mathbf A^T \\mathbf A)^{-1} \\mathbf A^T \\mathbf A\\cdot \\mathbf{k} &= (\\mathbf A^T \\mathbf A)^{-1} \\mathbf A^T \\cdot \\mathbf{y} \\\\\n\\mathbf{k} &= (\\mathbf A^T \\mathbf A)^{-1} \\mathbf A^T \\cdot \\mathbf{y}\n\\end{align}\n\\] Questa relazione rende evidente cosa si intende per regressione lineare: non ha nulla a che fare con il grado della funzione regredita, ma solo con l’equazione lineare nei parametri che rappresenta il modello\nNOTE:\n\ndalla equazione matriciale precedente è evidente che la regressione può essere eseguita se e solo se \\(N\\geq n+1\\), cioè se il numero di osservazioni è almeno pari al numero di parametri\nanche nel caso di una \\(f(\\cdot)\\) con più predittori, se essa è lineare nei coefficienti è sempre possibile esprimerla come \\(\\mathbf{A}\\mathbf{k}=\\mathbf{y}\\) e quindi risolverla con il metodo della pseudo-inversa"
  },
  {
    "objectID": "slides/ADAS/3-regressione.html#esempio",
    "href": "slides/ADAS/3-regressione.html#esempio",
    "title": "Regressione",
    "section": "Esempio",
    "text": "Esempio\n\n\nSia il modello da regredire del tipo \\(y_i=(ax_i + b) + \\varepsilon_i\\); allora può essere rappresentato come:\n\\[\n\\begin{bmatrix}\nx_1 & 1 \\\\\nx_2 & 1 \\\\\n\\vdots & \\vdots \\\\\nx_N & 1 \\\\\n\\end{bmatrix} \\cdot\n\\begin{bmatrix}\na \\\\\nb\n\\end{bmatrix} =\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_N\n\\end{bmatrix}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nLa figura mostra le osservazioni come punti di coordinate \\((x_i, y_i)\\), il modello regredito come una retta rossa (modello lineare nei coefficienti \\(a,~b\\) e di primo grado del predittore \\(x\\)) e i residui \\(\\varepsilon_i\\) come segmenti blu che rappresentano la differenza tra le \\(y_i\\) e il corrispondente valore regredito \\(\\hat{y_i}\\)"
  },
  {
    "objectID": "slides/ADAS/3-regressione.html#regressione-ai-minimi-quadrati-1",
    "href": "slides/ADAS/3-regressione.html#regressione-ai-minimi-quadrati-1",
    "title": "Regressione",
    "section": "Regressione ai Minimi Quadrati",
    "text": "Regressione ai Minimi Quadrati\nSi cerca cioè l’insieme di valori dei parametri che minimizza la distanza tra il modello e le osservazioni. Questa minimizzazione può essere realizzata definendo un indice di merito che rappresenta la distanza tra modello e osservazioni in funzione dei parametri: \\[\n\\Phi(c_1, c_2,\\dots,c_m)=\\sum_{i=1}^N \\left(y_i - f(x_{1i},x_{2i},\\dots,x_{ni}, c_1, c_2,\\dots,c_m) \\right)^2\n\\]\n\n\nSe la \\(f(\\cdot)\\) è analitica e differenziabile, allora possiamo minimizzare \\(\\Phi(\\cdot)\\) per derivazione, cioè risolvendo il sistema di \\(m\\) equazioni \\[\n\\frac{\\partial\\Phi}{\\partial c_i}(c_1,c_2,\\dots,c_m) = 0\n\\]\nSe la \\(f(\\cdot)\\) non è differenziabile, il minimo di \\(\\Phi(\\cdot)\\) può comunque essere calcolato per via numerica (ad es. metodo di Newton-Raphson)"
  },
  {
    "objectID": "slides/ADAS/3-regressione.html#coefficiente-di-determinazione",
    "href": "slides/ADAS/3-regressione.html#coefficiente-di-determinazione",
    "title": "Regressione",
    "section": "Coefficiente di Determinazione",
    "text": "Coefficiente di Determinazione\n\n\n\nIl coefficiente di merito più utilizzato per valutare una regressione è il coefficiente di determinazione \\(R^2\\)\nÈ definito come \\(R^2 = 1 - \\frac{SS_\\mathrm{res}}{SS_\\mathrm{tot}}\\), dove \\(SS_\\mathrm{res} = \\sum \\varepsilon_i^2\\) e \\(SS_\\mathrm{tot} = \\sum(y_i - \\bar y)^2\\)\nSe i valori regrediti corrispondono ai valori osservati \\(y_i=\\hat{y_i}\\), allora i residui sono tutti nulli e vale \\(R^2 = 1\\)\nLa qualità della regressione diminuisce al diminuire di \\(R^2\\)\n\n\n\nDue set di datiStesso set, due modelli\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNota: come si osserva, non è necessario che i valori dei regressori siano equispaziati!"
  },
  {
    "objectID": "slides/ADAS/3-regressione.html#sotto-adattamento",
    "href": "slides/ADAS/3-regressione.html#sotto-adattamento",
    "title": "Regressione",
    "section": "Sotto-adattamento",
    "text": "Sotto-adattamento\n\n\nSi ha sotto-adattamento (o under-fitting) quando il modello ha un grado inferiore all’apparente comportamento delle osservazioni\nPuò essere evidenziato, oltre che da un \\(R^2\\) basso, studiando la distribuzione dei residui: se c’è sotto-adattamento i residui possono essere non-normali e, soprattutto, mostrare degli andamenti, o pattern\nUn pattern è un andamento regolare dei residui in funzione dei regressori\nDal numero di massimi e minimi presenti nell’eventuale pattern è possibile stimare quanti gradi mancano\n\n\nSotto-adattamentoAdattamento corretto"
  },
  {
    "objectID": "slides/ADAS/3-regressione.html#sovra-adattamento",
    "href": "slides/ADAS/3-regressione.html#sovra-adattamento",
    "title": "Regressione",
    "section": "Sovra-adattamento",
    "text": "Sovra-adattamento\n\n\nSe il grado del modello è eccessivo, il modello tende a inseguire i singoli punti\nIl valore di \\(R^2\\) cresce, raggiungendo 1 quando il grado è uguale al numero di osservazioni meno 1\nTuttavia il modello perde di generalità e non riesce più a predirre correttamente nuovi valori acquisiti in un secondo momento (le crocette rosse in figura)\nIl sovra-adattamento ha effetti particolarmente drammatici in caso di estrapolazione, cioè quando si valuta il modello al di fuori dell’intervallo in cui è stato regredito\n\n\n\n\n\n\n\n\n\n\n\n\n\nIl metodo più robusto per identificare il sovra-adattamento è detto K-fold cross-validation, vedi anche https://paolobosetti.quarto.pub/kfold.html"
  },
  {
    "objectID": "slides/ADAS/3-regressione.html#bande-di-predizione",
    "href": "slides/ADAS/3-regressione.html#bande-di-predizione",
    "title": "Regressione",
    "section": "Bande di Predizione",
    "text": "Bande di Predizione\n\n\nÈ una banda simmetrica rispetto alla regressione all’interno della quale le osservazioni (presenti e future) hanno una probabilità assegnata di ricadere\nIn generale, per un numero di osservazioni sufficientemente grande (\\(&gt;50\\)) la banda di predizione al 95% contiene il 95% delle osservazioni"
  },
  {
    "objectID": "slides/ADAS/3-regressione.html#bande-di-confidenza",
    "href": "slides/ADAS/3-regressione.html#bande-di-confidenza",
    "title": "Regressione",
    "section": "Bande di Confidenza",
    "text": "Bande di Confidenza\n\n\nÈ una banda simmetrica rispetto alla regressione all’interno della quale il valore atteso del modello ha una probabilità assegnata di ricadere\nÈ sempre più stretta rispetto alla banda di predizione\nÈ l’equivalente multi-dimensionale dell’intervallo di confidenza per un T-test: come questo è l’intervallo all’interno del quale ha una assegnata probabilità di rientrare il valore corrispondente all’ipotesi nulla, qui possiamo assumere che il modello “vero” rientri con una certa probabilità nella banda di confidenza\n\n\n\n\n\n\n\n\n\n\n\nÈ ottenuto calcolando gli intervalli di confidenza sui parametri della regressione, calcolando poi—per ogni valore del predittore—il valore massimo e minimo della regressione corrispondente ai valori estremi dei parametri nei loro intervalli di confidenza"
  },
  {
    "objectID": "slides/ADAS/3-regressione.html#basi-3",
    "href": "slides/ADAS/3-regressione.html#basi-3",
    "title": "Regressione",
    "section": "Basi",
    "text": "Basi\nNel caso della regressione lineare: \\[\n\\begin{align}\ny_i &= f(\\mathbf{x}_i, \\mathbf{k}) + \\varepsilon_i = \\hat y_i + \\varepsilon_i \\\\\n\\varepsilon_i &\\sim \\mathcal{N}(0, \\sigma^2)\n\\end{align}\n\\] Nel caso della regressione lineare generalizzata:\n\\[\n\\begin{align}\ny_i &=  \\hat y_i + \\varepsilon_i \\\\\n\\varepsilon_i &\\sim D(p_1,p_2,\\dots,p_k)\n\\end{align}\n\\] dove \\(D\\) è una generica distribuzione a \\(k\\) parametri facente parte della famiglia delle distribuzioni esponenziali (normale, binomiale, gamma, normale inversa, Poisson, quasinormale, quasibinomiale e quasipoissoniana)\nIl problema può essere risolto con l’introduzione di una funzione di collegamento che riscala i residui proiettandoli su una ditribuzione normale"
  },
  {
    "objectID": "slides/ADAS/3-regressione.html#basi-4",
    "href": "slides/ADAS/3-regressione.html#basi-4",
    "title": "Regressione",
    "section": "Basi",
    "text": "Basi\nLa funzione di collegamento (link function) \\(g(\\cdot)\\) è tale per cui:\n\\[\n\\begin{align}\ny_i &=  \\hat y_i + g(\\varepsilon_i) \\\\\n\\varepsilon_i &\\sim D(p_1,p_2,\\dots,p_k);~g(\\varepsilon_i)\\sim \\mathcal{N}(0, \\sigma^2)\n\\end{align}\n\\]\n\n\nLe funzioni di collegamento per le distribuzioni più comuni sono:\n\n\n\nDistribuzione\nFunzione di collegamento\n\n\n\n\nNormale\n\\(g(x)=x\\)\n\n\nBinomiale\n\\(g(x)=\\mathrm{logit}(x)\\)\n\n\nPoisson\n\\(g(x)=\\log(x)\\)\n\n\nGamma\n\\(g(x)=1/x\\)\n\n\n\nIn particolare, vale: \\(\\mathrm{logit}(x)=\\frac{1}{1+e^{-p(x-x_0)}}\\)"
  },
  {
    "objectID": "slides/ADAS/3-regressione.html#regressione-logistica",
    "href": "slides/ADAS/3-regressione.html#regressione-logistica",
    "title": "Regressione",
    "section": "Regressione Logistica",
    "text": "Regressione Logistica\n\n\nIl caso tipico di regressione logistica è il classificatore di eventi binomiali\nconsideriamo un processo che, in funzione di uno o più predittori, possa fornire un risultato che può valere solo una di due alternative (successo|fallimento, rotto|integro, vero|falso, 1|0). Vogliamo identificare la soglia dei predittori che commuta il risultato\nUna regressione lineare non è adatta alla situazione: è evidente che i residui non sono normali e che la pendenza della regressione dipende molto da quanti punti sono raccolti nelle zone “sicure”\n\n\nDatiResidui modello lineare"
  },
  {
    "objectID": "slides/ADAS/3-regressione.html#regressione-logistica-1",
    "href": "slides/ADAS/3-regressione.html#regressione-logistica-1",
    "title": "Regressione",
    "section": "Regressione Logistica",
    "text": "Regressione Logistica\n\n\nLa funzione logistica regredita fornisce il parametro \\(x_0\\) che identifica il valore che separa una uguale quantità di falsi positivi e falsi negativi\nInoltre, è possibile individuare la soglia opportuna per ottenere una prefissata probabilità di falsi positivi (o falsi negativi)\nQuesto è il tipo più semplice di machine learning: un classificatore binomiale"
  },
  {
    "objectID": "slides/ADAS/3-regressione.html#confronto-grafico-di-serie",
    "href": "slides/ADAS/3-regressione.html#confronto-grafico-di-serie",
    "title": "Regressione",
    "section": "Confronto grafico di serie",
    "text": "Confronto grafico di serie\n\n\nSupponiamo di avere un processo il cui valore dipende da una variabile \\(x\\)\nSupponiamo che un parametro \\(S\\) di processo possa influire sul valore in uscita. Ad esempio:\n\nil valore è la durezza di un metallo, \\(x\\) è la temperatura, il parametro \\(S\\) è la quantità di un elemento in lega\nil valore è la produttività di un impianto, \\(x\\) è un parametro quantitativo di processo, il parametro \\(S\\) è il tipo di macchina utilizzato\n\n\n\n\n\n\n\n\n\n\n\n\nSupponiamo di ripetere 8 volte una misurazione del valore in uscita per le varie combinazioni di \\(x\\) e di \\(S\\), ottenendo i risultati in figura: quali differenze sono significative?"
  },
  {
    "objectID": "slides/ADAS/3-regressione.html#senza-un-modello-di-riferimento",
    "href": "slides/ADAS/3-regressione.html#senza-un-modello-di-riferimento",
    "title": "Regressione",
    "section": "Senza un modello di riferimento",
    "text": "Senza un modello di riferimento\n\n\nSenza un modello di riferimento che esprima \\(v=f(x, S)\\) non ha senso effettuare una regressione\nTuttavia posso riportare, per ogni trattamento\n\nil valor medio, unendo le serie con una spezzata al solo scopo di raggruppare visivamente i dati\ni limiti dell’intervallo di confidenza per ogni serie e per ogni trattamento\noppure, unire i limiti con una banda che rappresenta la confidenza sulla media\n\n\n\n\n\n\n\n\n\n\n\n\nZone in cui le bande sono sovrapposte sono statisticamente indistinguibili"
  },
  {
    "objectID": "slides/ADAS/3-regressione.html#con-un-modello",
    "href": "slides/ADAS/3-regressione.html#con-un-modello",
    "title": "Regressione",
    "section": "Con un modello",
    "text": "Con un modello\n\n\nSolo se ho un modello \\(v=f(x, S)\\) posso effettuare una regressione\nAnche in questo caso, la regressione va accompagnata con bande di confidenza\nDi nuovo, zone in cui le bande sono sovrapposte sono statisticamente indistinguibili"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#variabili-aleatorie-o-stocastiche",
    "href": "slides/ADAS/1-statistica.html#variabili-aleatorie-o-stocastiche",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Variabili aleatorie (o stocastiche)",
    "text": "Variabili aleatorie (o stocastiche)\nUna variabile stocastica è una variabile che assume valori casuali ad ogni osservazione, cioè tali per cui non è possibile prevedere il valore esatto della prossima osservazione, nemmeno conoscendo le osservazioni precedenti\n\nLa misurazione è il processo che porta alla valutazione oggettiva del misurando. Il risultato di una misurazione è chiamato misura\n\n\nLe variabili stocastiche sono di particolare interesse per l’ingegneria e per l’industria in genere, dato che ogni misurazione produce, come risultato, un valore che ha un contenuto casuale ed è quindi rappresentabile come una variabile stocastica\n\n\nA sua volta, il contributo casuale ad una misura è chiamato incertezza\n\n\nDato che ogni attività produttiva è indissolubilmente legata a delle misurazioni, è quindi evidente quanto sia fondamentale trattare in maniera coerente e robusta i contributi casuali alle misure"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#effetto-scala",
    "href": "slides/ADAS/1-statistica.html#effetto-scala",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Effetto scala",
    "text": "Effetto scala"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#effetto-scala-1",
    "href": "slides/ADAS/1-statistica.html#effetto-scala-1",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Effetto scala",
    "text": "Effetto scala"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#effetto-scala-2",
    "href": "slides/ADAS/1-statistica.html#effetto-scala-2",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Effetto scala",
    "text": "Effetto scala\nIn altre parole, la componente stocastica di una misura è affetta da un effetto scala: tanto più piccolo è il rapporto tra il valore medio misurato e la variabilità tipica dello strumento (cioè la sua precisione), tanto meno sarà apprezzabile l’effetto di casualità\nÈ quindi essenziale definire in maniera precisa ed efficace concetti intuitivi come variabilità e valore medio che abbiamo sopra espresso"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#popolazioni",
    "href": "slides/ADAS/1-statistica.html#popolazioni",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Popolazioni",
    "text": "Popolazioni\nIn statistica, una popolazione è un insieme di valori, oggetti o eventi di interesse per una qualche analisi o esperimento.\nPer studiare la statura dei residenti nella città di Trento, la popolazione di interesse è l’intero insieme degli abitanti di Trento.\nPer studiare il comportamento meccanico della lega d’Alluminio prodotta da un certo impianto, la popolazione di interesse può essere l’intera quantità di lega prodotta da un certo lotto di materia prima\nMa per studiare anche gli effetti di variabilità delle materie prime (tra un lotto e l’altro), delle condizioni ambientali, ecc., sarebbe più opportuno definire come popolazione un insieme più ampio\nQuindi spesso la definizione della popolazione di interesse è arbitraria"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#popolazioni-1",
    "href": "slides/ADAS/1-statistica.html#popolazioni-1",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Popolazioni",
    "text": "Popolazioni\nQuindi:\n\nla definizione della popolazione dipende dall’obiettivo dell’analisi\nla dimensione di una popolazione è generalmente molto ampia e potenzialmente non limitata\ndi conseguenza è spesso impraticabile considerare l’intera popolazione\nsi lavora quindi su sottoinsiemi estratti casualmente dalla popolazione, chiamati campioni\nessendo estratti casualmente, i campioni approssimano la popolazione, tanto meglio quanto più sono numerosi"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#popolazione",
    "href": "slides/ADAS/1-statistica.html#popolazione",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Popolazione",
    "text": "Popolazione\nOsservando una popolazione possiamo identificare un valore centrale e una variabilità"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#momenti-di-una-popolazione",
    "href": "slides/ADAS/1-statistica.html#momenti-di-una-popolazione",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Momenti di una popolazione",
    "text": "Momenti di una popolazione\nIl valore centrale è chiamato valore atteso e la variabilità è chiamata varianza\n\nValore atteso:\n\nper v.a. discrete: \\(\\mu = E(x) := \\sum_{i = 1}^N x_i p(x_i)\\)\nper v.a. continue: \\(\\mu = E(x) := \\int_{-\\infty}^{+\\infty} x f(x)~\\mathrm{d}x\\)\n\nVarianza:\n\nper v.a. discrete: \\(\\sigma^2 = V(x) := \\sum_{i = 1}^N (x_i -\\mu)^2 p(x_i)\\)\nper v.a. continue: \\(\\sigma^2 = V(x) := \\int_{-\\infty}^{+\\infty} (x - \\mu)^2 f(x)~\\mathrm{d}x\\)\n\n\ndove \\(E()\\) e \\(V()\\) sono gli operatori valore atteso e varianza, rispettivamente; \\(x_i\\) (e \\(x\\)) è la generica osservazione della v.a., e \\(p(x_i)\\) e \\(f(x)\\) sono la probabilità e la densità di probabilità di riscontrare un dato valore\nLe proprietà di una popolazione si indicano con lettere greche: \\(\\mu\\) e \\(\\sigma^2\\)\nNOTA: Risulta \\(\\sigma^2 = E\\left[(x-\\mu)^2\\right]\\)"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#probabilità-e-densità-di-probabilità",
    "href": "slides/ADAS/1-statistica.html#probabilità-e-densità-di-probabilità",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Probabilità e densità di probabilità",
    "text": "Probabilità e densità di probabilità\n\nProbabilità (o frequenza): per una v.a. discreta, corrisponde al rapporto tra il numero di osservazioni di un dato valore e il numero totale di osservazioni\nDensità di probabilità: per una v.a. continua, la probabilità di riscontrare esattamente un dato valore è nulla, quindi ci si riferisce ad una probabilità di riscontrare un valore all’interno di un dato intervallo. La densità di probabilità è la derivata di tale valore\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbabilità cumulata (Cumulated Distribution Function): \\(p(x_0) = P(x\\leq x_0)\\)\nDensità di probabilità (Probability Density Function): \\(f(x) = \\frac{d}{dx} p(x)\\)\nVale anche \\(p(x_0) = \\int_{-\\infty}^{x_0}f(x) \\mathrm d x\\)"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#probabilità-e-densità-di-probabilità-1",
    "href": "slides/ADAS/1-statistica.html#probabilità-e-densità-di-probabilità-1",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Probabilità e densità di probabilità",
    "text": "Probabilità e densità di probabilità\nInoltre, si noti che probabilità e frequenza devono sommare a 1: rispettivamente: \\[\n\\begin{array}{l} \\sum_i p(x_i) = 1 \\\\\n\\int_{-\\infty}^\\infty f(x)~\\mathrm{d}x = 1 \\end{array}\n\\]\nQuesto perché ovviamente la probabilità di riscontrare un qualsiasi valore deve essere certa, cioè 1"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#proprietà-degli-operatori",
    "href": "slides/ADAS/1-statistica.html#proprietà-degli-operatori",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Proprietà degli operatori",
    "text": "Proprietà degli operatori\nGli operatori valore atteso e varianza godono delle seguenti proprietà:\n\\[\n\\begin{array}{l}\nE(c)&=&c\\\\\nE(x)&=&\\mu\\\\\nE(cx)&=&cE(x)=c\\mu\\\\\nV(c)&=&0\\\\\nV(x)&=&\\sigma^2\\\\\nV(cx)&=&c^2V(x)=c^2\\sigma^2\\\\\nE(x+y)&=&E(x)+E(y)=\\mu_x+\\mu_y\\\\\n\\mathrm{Cov}(x,y)&=&E[(x-\\mu_x)(y-\\mu_y)]\\\\\nV(x+y)&=&V(x)+V(y)+2\\textrm{ Cov}(x,y)\\\\\nV(x-y)&=&V(x)+V(y)-2\\textrm{ Cov}(x,y)\n\\end{array}\n\\]\ndove \\(c\\) indica una costante"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#correlazione-e-covarianza",
    "href": "slides/ADAS/1-statistica.html#correlazione-e-covarianza",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Correlazione e covarianza",
    "text": "Correlazione e covarianza\nL’operatore covarianza è un indice di quanto due variabili stocastiche siano interdipendenti\nPiù utile della covarianza (che non è limitata) è la correlazione che ha il vantaggio di essere compresa nell’intervallo \\([-1,1]\\):\n\\[\n\\mathrm{Corr}(x, y) = \\frac{E[(x-\\mu_x)(y-\\mu_y)]}{\\sigma_x\\sigma_y} = \\frac{\\mathrm{Cov}(x,y)}{\\sigma_x\\sigma_y}\n\\]\n\nvicina a zero significa nessuna correlazione\nvicina a 1 significa forte correlazione positiva (se aumenta \\(x\\) aumenta anche \\(y\\))\nvicina a -1 significa forte correlazione negativa (se aumenta \\(x\\) diminuisce \\(y\\)).\n\nCovarianza e correlazione sono anche indicate come \\(\\sigma_{xy}^2\\) e \\(\\rho_{xy}\\), rispettivamente."
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#campioni",
    "href": "slides/ADAS/1-statistica.html#campioni",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Campioni",
    "text": "Campioni\nUna popolazioni può essere eccessivamente numerosa per essere analizzata direttamente\nQuindi si analizzano dei sottoinsiemi ottenuti per campionatura, cioè estrazione casuale\nL’estrazione casuale di un campione sufficientemente grande non altera le proprietà di distribuzione della popolazione\nDa una popolazione di \\(N\\) elementi è possibile estrarre un numero di campioni di dimensione \\(n\\) differenti descritto dal coefficiente binomiale: \\[\\binom{N}{n}=\\frac{N!}{(N-n)!n!},~N&gt;n\\]"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#stimatori",
    "href": "slides/ADAS/1-statistica.html#stimatori",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Stimatori",
    "text": "Stimatori\nPer ogni proprietà della popolazione è possibile definire uno stimatore, o statistica, costruito sul campione\nSi definiscono media e varianza campionarie\n\\[\n\\begin{eqnarray} \\bar x &=& \\frac{1}{n}\\sum_{i=1}^n x_i\\\\\nS^2 &=& \\frac{\\sum_{i=1}^n (x_i - \\bar x)^2}{n-1}\n\\end{eqnarray}\n\\]\nUn particolare valore assunto da uno stimatore è detto stima\nAl posto della varianza \\(S^2\\) si usa spesso la deviazione standard \\(S\\) (stesse unità di misura)"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#stime",
    "href": "slides/ADAS/1-statistica.html#stime",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Stime",
    "text": "Stime\nDato che ogni campione è estratto casualmente, ogni stima è una variabile aleatoria\nPiù grande è il campione, più la stima si avvicina alla proprietà corrispondente: si ha convergenza in distribuzione"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#valore-atteso-e-varianza-della-media-campionaria",
    "href": "slides/ADAS/1-statistica.html#valore-atteso-e-varianza-della-media-campionaria",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Valore atteso e varianza della media campionaria",
    "text": "Valore atteso e varianza della media campionaria\nValore atteso della media:\n\\[\n\\begin{eqnarray} \\mathrm E(\\bar x) &=& \\mathrm E(\\frac{x_1+x_2+\\dots+x_n}{n}) = \\frac{1}{n}\\left[\\mathrm E(x_1+x_2+\\dots+x_n) \\right]\\\\\n&=& \\frac{1}{n}\\left[\\mathrm E(x_1)+\\mathrm E(x_2)+\\dots+\\mathrm E(x_n) \\right] = \\frac{1}{n} n\\mathrm E(x) \\\\\n\\mathrm E(\\bar x)&=& \\mu\n\\end{eqnarray}\n\\]\nVarianza della media:\n\\[\n\\begin{eqnarray}\n\\mathrm V(\\bar x) &=& \\mathrm V(\\frac{x_1+x_2+\\dots+x_n}{n}) = \\frac{1}{n^2}\\left[\\mathrm V(x_1+x_2+\\dots+x_n) \\right]\\\\\n&=& \\frac{1}{n^2}\\left[\\mathrm V(x_1)+\\mathrm V(x_2)+\\dots+\\mathrm V(x_n) \\right] = \\frac{n\\mathrm V(x)}{n^2} = \\frac{\\mathrm V(x)}{n} \\\\\n\\mathrm V(\\bar x) &=& \\frac{\\sigma^2}{n}\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#gradi-di-libertà",
    "href": "slides/ADAS/1-statistica.html#gradi-di-libertà",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Gradi di libertà",
    "text": "Gradi di libertà\nI gradi di libertà di una statistica (GdF o DoF) sono il numero di elementi indipendenti che compaiono nella sua definizione. Dalla definizione della varianza risulta che\n\\[\n\\sigma^2=E\\left(\\frac{\\sum(x_i - \\bar x)^2}{n-1}\\right)=E\\left(\\frac{SS}{\\nu}\\right)\n\\]\nCioè la varianza è il valore atteso della somma quadratica (Sum of Squares) divisa per il suo numero di gradi di libertà \\(\\nu\\), cioè di elementi indipendenti.\nChe questi ultimi siano \\(n-1\\) è dimostrato dalla seguente relazione: \\[\n\\sum_{i=1}^n(x_i-\\bar x) = \\sum_{i=1}^n(x_i)-n\\bar x=:0\n\\] quindi non tutti gli \\(n\\) elementi nella definizione di \\(SS\\) possono essere indipendenti, dato che il valore di uno di essi è prevedibile dai restanti \\(n-1\\) grazie alla definizione di \\(\\bar x\\)"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#nota-la-media-di-un-campione-è-possibile-calcolare-la-varianza",
    "href": "slides/ADAS/1-statistica.html#nota-la-media-di-un-campione-è-possibile-calcolare-la-varianza",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Nota la media di un campione è possibile calcolare la varianza?",
    "text": "Nota la media di un campione è possibile calcolare la varianza?\n\nSì\nNo"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#covarianza-campionaria",
    "href": "slides/ADAS/1-statistica.html#covarianza-campionaria",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Covarianza campionaria",
    "text": "Covarianza campionaria\nDate due v. a. \\(x\\) e \\(y\\) di cui si considerano due campioni, sia \\(\\sigma_{xy}=-7.52\\). Allora:\n\n\\(x\\) e \\(y\\) sono fortemente correlate in senso positivo\n\\(x\\) e \\(y\\) sono fortemente correlate in senso negativo\n\\(x\\) e \\(y\\) sono debolmente correlate\n\\(x\\) e \\(y\\) sono di certo anti-correlate\nNon è possibile affermare quanto siano correlate"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#varianza-della-media-campionaria",
    "href": "slides/ADAS/1-statistica.html#varianza-della-media-campionaria",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Varianza della media campionaria",
    "text": "Varianza della media campionaria\nDa una stessa popolazione estraggo una serie di campioni di 16 elementi e una seconda serie di campioni di 64 elementi. Considerando la varianza delle due medie campionarie:\n\nÈ uguale per entrambe le serie\nQuella della seconda serie è circa un quarto di quella della prima\nQuella della prima serie è circa il doppio di quella della seconda\nQuella della seconda serie è un quarto di quella della prima\nQuella della prima serie è il doppio di quella della seconda"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#distribuzioni-1",
    "href": "slides/ADAS/1-statistica.html#distribuzioni-1",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Distribuzioni",
    "text": "Distribuzioni\n\n\nDistribuzioni discrete\n\n\n\n\n\n\n\n\n\n\nDistribuzioni continue"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#distribuzione-binomiale-o-di-bernoulli",
    "href": "slides/ADAS/1-statistica.html#distribuzione-binomiale-o-di-bernoulli",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Distribuzione binomiale o di Bernoulli",
    "text": "Distribuzione binomiale o di Bernoulli\n\n\n\nDescrizioneDefinizioniEsempi\n\n\nUn processo di Bernoulli è una serie di \\(n\\) eventi con risultati \\(z_1, z_2, \\dots, z_n\\) tali per cui:\n\ngli eventi \\(z_i\\) sono tutti indipendenti\nogni \\(z_i\\) è rappresentabile con 0 o con 1\nla probabilità di successo \\(p_s\\in[0,1]\\) di ciascun evento è costante\n\nLa distribuzione binomiale descrive la probabilità di ottenere \\(x\\in[0, n]\\) successi in \\(n\\) eventi\n\n\n\nSi dice che \\(x\\sim\\mathrm{Binom}(n,p_s)\\) o anche \\(x\\sim\\mathcal{B}(n,p_s)\\) quando la probabilità è: \\[\np(x)=\\binom{n}{x}p_s^x(1-p_s)^{n-x},~~~x\\in{0,1\\dots,n}\n\\]\nMomenti: \\[\n\\mu=np_s,~~~\\sigma^2=np_s(1-p_s)\n\\]\n\n\n\n\nprobabilità di ottenere 8 volte testa lanciando 10 volte una moneta"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#distribuzione-di-poisson",
    "href": "slides/ADAS/1-statistica.html#distribuzione-di-poisson",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Distribuzione di Poisson",
    "text": "Distribuzione di Poisson\n\n\n\nDescrizioneDefinizioniEsempi\n\n\n\nProbabilità di avere un numero \\(x\\in\\mathbb{N}^+\\) di eventi che si verificano successivamente ed indipendentemente in un dato intervallo di tempo (o spazio…)\nMediamente si verificano \\(\\lambda\\in\\mathbb{R}^+\\) eventi nello stesso intervallo\nÈ nota anche come legge degli eventi rari\n\n\n\n\nSi dice che \\(x\\sim\\mathrm{Poisson}(\\lambda)\\) oppure \\(x\\sim\\mathcal{P}(\\lambda)\\) quando la probabilità è: \\[\np(x)=\\frac{e^{-\\lambda}\\lambda^x}{x!},~~~\\forall x\\in\\mathbb{N}^+\n\\]\nMomenti: \\[\n\\mu=\\lambda,~~~\\sigma^2=\\lambda\n\\]\n\n\n\n\nprobabilità di riscontrare un difetto su 1 m di filo, quando ci sono mediamente 9 difetti ogni 100 m\nprobabilità di ricevere una telefonata nei prossimi 10’ in un centralino che ne riceve mediamente 250 al giorno"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#distribuzione-geometrica",
    "href": "slides/ADAS/1-statistica.html#distribuzione-geometrica",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Distribuzione geometrica",
    "text": "Distribuzione geometrica\n\n\n\nDescrizioneDefinizioniEsempi\n\n\nÈ la distribuzione di probabilità di ottenere in un processo di Bernoulli un successo dopo \\(x \\in \\mathbb{N}^+\\) fallimenti\n\n\n\nSi dice che \\(x\\sim\\mathrm{Geom}(p_s)\\) oppure \\(x\\sim\\mathcal{G}(p_s)\\) quando la probabilità è: \\[\np(x)=p_s(1-p_s)^{x-1},~~~x \\in\\mathbb{N}^+\n\\]\nMomenti: \\[\n\\mu=(1-p_s)/p_s,~~~\\sigma^2=(1-p_s)/p_s^2\n\\]\n\n\n\n\nLa probabilità di ottenere per la prima volta un dato numero dopo \\(x\\) lanci di un dado a sei facce (\\(p_s=1/6\\))"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#distribuzione-uniforme",
    "href": "slides/ADAS/1-statistica.html#distribuzione-uniforme",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Distribuzione Uniforme",
    "text": "Distribuzione Uniforme\n\n\n\nDescrizioneDefinizioniEsempi\n\n\n\ndistribuzione in cui tutti i valori della v.a. hanno la stessa probabilità\npuò essere sia discreta che continua\n\n\n\n\nSi dice \\(x\\sim\\mathcal{U}(a,b)\\) quando la PDF è: \\[\nf(x)=\\begin{cases} 1/(b-a),&x\\in[a, b] \\\\\n0,&\\textrm{altrimenti} \\end{cases}\n\\]\nMomenti: \\[\n\\mu=(b+a)/2,~~~\\sigma^2=\\frac{(b-a)^2}{12}\n\\]\n\n\n\n\nIl lancio di un dado a 6 facce (discreta)\nL’estrazione di un numero della tombola (discreta)\nL’angolo di arresto di una ruota in rotazione libera (continua)"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#distribuzione-normale-o-gaussiana",
    "href": "slides/ADAS/1-statistica.html#distribuzione-normale-o-gaussiana",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Distribuzione normale o gaussiana",
    "text": "Distribuzione normale o gaussiana\n\n\n\nDescrizioneDefinizioniNota\n\n\n\nRappresenta il caso in cui la probabilità di riscontrare valori via via più lontani dal valore atteso decresce asintoticamente a 0\nLa probabilità di un qualsiasi valore non è mai nulla\n\n\n\n\nSi dice che \\(x\\sim\\mathrm{Norm}(\\mu,\\sigma^2)\\) oppure \\(x\\sim\\mathcal{N}(\\mu,\\sigma^2)\\) quando la PDF è: \\[\nf(x) =\\frac{1}{\\sigma \\sqrt{2 \\pi}}e^{-\\frac{1}{2}\\left[\\frac{x-\\mu}{\\sigma}\\right]^2}\n\\]\nMomenti: coincidono con i due parametri \\(\\mu\\) e \\(\\sigma^2\\)\n\n\n\nSe \\(x\\sim\\mathcal{N}(\\mu, \\sigma^2)\\) allora \\[\n\\frac{x-\\mu}{\\sigma}\\sim\\mathcal{N}(0,1)\n\\] e la distribuzione \\(\\mathcal{N}(0,1)\\) è detta normale standard.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nÈ una distribuzione che gioca un ruolo centrale, in virtù del teorema del limite centrale"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#teorema-del-limite-centrale-enunciato",
    "href": "slides/ADAS/1-statistica.html#teorema-del-limite-centrale-enunciato",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Teorema del limite centrale (enunciato)",
    "text": "Teorema del limite centrale (enunciato)\n\n\nTeorema\n\nSe \\(x_1, x_2, \\dots,x_n\\) sono \\(n\\) variabili aleatorie indipendenti e identicamente distribuite (IID) con \\(E(x_i)=\\mu\\) e \\(V(x_i)=\\sigma^2~~\\forall i=1,2,\\dots,n\\) (entrambi finiti), e \\(y=x_1+x_2+\\dots+x_n\\), allora: \\[\nz_n=\\frac{y-n\\mu}{\\sqrt{n\\sigma^2}}\n\\] approssima una distribuzione \\(\\mathcal{N}(0,1)\\), nel senso che se \\(F_n(z)\\) è la funzione di distribuzione di \\(z_n\\) e \\(\\Phi(z)\\) è la funzione di distribuzione di \\(\\mathcal{N}(0,1)\\), allora: \\[\n\\lim_{n\\rightarrow+\\infty}\\frac{F_n(z)}{\\Phi(z)}=1\n\\]"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#teorema-del-limite-centrale-significato",
    "href": "slides/ADAS/1-statistica.html#teorema-del-limite-centrale-significato",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Teorema del limite centrale (significato)",
    "text": "Teorema del limite centrale (significato)\n\nFondamentale nel campo delle misure:\n\nuna misura è somma di una serie di eventi\nogni evento può avere una distribuzione ignota\nsommando molte distribuzioni il risultato converge alla normale\nquindi il risultato di una misura è spesso normale\n\nLa convergenza è spesso molto rapida (una decina di elementi)\nNota: sommando o moltiplicando una distribuzione per una costante cambiano i suoi momenti ma la distribuzione rimane la stessa. Invece, operazioni tra v.a. cambiano la distribuzione risultante!"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#distribuzione-chi-quadro",
    "href": "slides/ADAS/1-statistica.html#distribuzione-chi-quadro",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Distribuzione Chi-quadro",
    "text": "Distribuzione Chi-quadro\n\n\n\nDescrizioneDefinizioniNota\n\n\n\nÈ la distribuzione di una somma di distribuzioni normali standard\nCioè, sia \\(x = z_1^2+z_2^2+\\dots+z_k^2\\), con \\(z_i\\sim\\mathcal{N}(0,1)~~\\forall i=1, 2, \\dots,k\\), allora la distribuzione di \\(x\\) è una Chi-quadro\nIl numero di normali sommate \\(k\\) è il numero di gradi di libertà della distribuzione\n\n\n\n\nSi dice che \\(x\\sim\\chi^2_k\\) quando la PDF è: \\[\nf(x)=\\frac{1}{2^{k/2}\\Gamma(k/2)}x^{k/2-1}e^{-y/2}\n\\]\nMomenti: \\[\n\\mu=k,~~~\\sigma^2=2k\n\\]\n\n\n\nConsiderando la somma quadratica di un campione di \\(k\\) elementi \\(y_i\\), ciascuno proveniente da una distribuzione \\(\\mathcal{N}(\\mu, \\sigma^2)\\), risulta che: \\[\n\\frac{(y_i-\\bar y)}{\\sigma}\\sim \\mathcal{N}(0,1)~~\\forall i=1,2,\\dots,k\n\\] e quindi: \\[\n\\frac{\\mathit{SS}}{\\sigma^2}=\\frac{\\sum_{i=1}^k(y_i-\\bar y)^2}{\\sigma^2} \\sim \\mathcal{X}^2_{k-1}\n\\]"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#distribuzione-t-di-student",
    "href": "slides/ADAS/1-statistica.html#distribuzione-t-di-student",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Distribuzione T di Student",
    "text": "Distribuzione T di Student\n\n\n\nDescrizioneDefinizioniNota\n\n\nSiano due v.a. \\[\nz\\sim\\mathcal{N}(0,1),~x\\sim\\mathcal{X}^2_k\n\\] allora la loro combinazione \\[\nx=\\frac{z}{\\sqrt{x/k}}\n\\] è distribuita come una T di Student\n\n\n\nSi dice che \\(x\\sim\\mathrm{T}_k\\) oppure \\(x\\sim\\mathcal{T}_k\\) quando la PDF è: \\[\nf(x)=\\frac{\\Gamma\\left((k-1)/2\\right)}{\\sqrt{k\\pi}\\Gamma(k/2)}\\frac{1}{((x^2/k)+1)^{(k+1)/2}}\n\\]\nMomenti: \\[\n\\mu = 0,~~~ \\sigma^2=k/(k-2)\n\\]\n\n\n\nLa T di Student è un caso particolare della \\(\\mathcal{N}(0,1)\\): \\[\n\\lim_{k\\rightarrow+\\infty}t_k=\\mathcal{N}(0,1)\n\\] La convergenza è molto rapida: già per \\(k&gt;30\\) la differenza tra le due funzioni di distribuzione diventa trascurabile\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa figura confronta la T di Student con la normale standard (tratteggiata)"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#distribuzione-f-di-snedecor",
    "href": "slides/ADAS/1-statistica.html#distribuzione-f-di-snedecor",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Distribuzione F di Snedecor",
    "text": "Distribuzione F di Snedecor\n\n\n\nDescrizioneDefinizioni\n\n\nSiano due v.a. \\[\nx_u\\sim\\mathcal{X}^2_u,~~~x_v\\sim\\mathcal{X}^2_v\n\\] e si definisce \\(x\\) come: \\[x=\\frac{x_u/u}{x_v/v}\\] allora \\(x\\) è una v.a. distribuita come una F di Snedecor\n\n\n\nSi dice che \\(x\\sim\\mathrm{F}_{u,v}\\) oppure \\(x\\sim\\mathcal{F}_{u,v}\\) quando la PDF è: \\[\nf(x)=\\frac{\\Gamma\\left(\\frac{u+v}{2}\\right)\\left(\\frac{u}{v}\\right)^{u/2}x^{(u/2)-1}}{\\Gamma\\left( \\frac{u}{2} \\right)\\Gamma\\left( \\frac{v}{2} \\right) \\left(\\frac{u}{v}x+1\\right)^{(u+v)/2}}\n\\]\nMomenti: \\[\n\\mu = \\frac{v}{v-2},~~~\\sigma^2=\\frac{2v^2(u+v-2)}{u(v-2)^2(v-4)}\n\\]"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#funzioni-di-distribuzione",
    "href": "slides/ADAS/1-statistica.html#funzioni-di-distribuzione",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Funzioni di distribuzione",
    "text": "Funzioni di distribuzione\n\n\nUna distribuzione è descritta da tre funzioni, tra loro correlate:\n\nfunzione di densità di distribuzione, PDF\nfunzione di distribuzione cumulata, CDF, è l’integrale progressivo della PDF: \\[\n\\begin{array}{rcl}\n\\mathrm{CDF}^-(x) &=& \\int_{-\\infty}^x \\mathrm{PDF}(x)~\\mathrm{d}x \\\\\n\\mathrm{CDF}^+(x) &=& \\int^{+\\infty}_x \\mathrm{PDF}(x)~\\mathrm{d}x\n\\end{array}\n\\]\nfunzione quantile, è l’inversa della CDF; è definita solo in \\([0,1]\\)\n\n\n\nPDFCDFQuantile"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#gioco-del-lotto",
    "href": "slides/ADAS/1-statistica.html#gioco-del-lotto",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Gioco del lotto",
    "text": "Gioco del lotto\nNel gioco del lotto, la probabilità di vincere giocando sempre gli stessi numeri:\n\nAumenta ad ogni estrazione, come predetto dalla distribuzione \\(\\mathcal{B}\\)\nAumenta ad ogni estrazione, come predetto dalla distribuzione \\(\\mathcal{P}\\)\nAumenta ad ogni estrazione, come predetto dalla distribuzione \\(\\mathcal{G}\\)\nÈ costante ad ogni estrazione"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#pdf-e-cdf",
    "href": "slides/ADAS/1-statistica.html#pdf-e-cdf",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "PDF e CDF",
    "text": "PDF e CDF\nQuali di queste affermazioni sono corrette:\n\nQualsiasi CDF è monotona crescente\nEsistono distribuzioni con PDF definita solo per valori positivi\nPer ogni CDF possono esistere più di una PDF che la generano\n\\(\\mathrm{PDF}(-x) = -\\mathrm{PDF}(x)\\)\n\\(\\mathrm{CDF}^+(x) = \\mathrm{CDF}^-(-x)\\)\n\\(\\mathrm{CDF}^+(x) = 1- \\mathrm{CDF}^-(x)\\)\nUna funzione quantile può avere un dominio illimitato a destra o sinistra"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#ipotesi-statistiche",
    "href": "slides/ADAS/1-statistica.html#ipotesi-statistiche",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Ipotesi statistiche",
    "text": "Ipotesi statistiche\n\nSi è visto che media e varianza campionaria sono due stimatori e rappresentano delle variabili aleatorie\nQuindi prelevando due campioni da una popolazione le due stime di media e varianza saranno sempre differenti\nCome faccio a sapere se due campioni con media diversa provengono dalla stessa popolazione?\nPosso formulare una coppia di ipotesi alternative: \\[\n\\begin{eqnarray}\nH_0:~&\\mu_1 = \\mu_2 \\\\\nH_1:~&\\mu_1 \\neq \\mu_2 \\\\\n\\end{eqnarray}\n\\]\n\n\n\n\\(H_0\\) è detta ipotesi nulla, o debole; \\(H_1\\) è detta ipotesi alternativa, o forte"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#matrice-di-confusione",
    "href": "slides/ADAS/1-statistica.html#matrice-di-confusione",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Matrice di confusione",
    "text": "Matrice di confusione\n\n\nUn test di ipotesi può avere due tipi di errore:\n\nErrore di tipo I: falso positivo, o falso allarme\nErrore di tipo II: falso negativo, o mancato allarme\n\nLo scopo della statistica inferenziale è associare una probabilità a questi errori\n\n\n\n\n\n\nIpotesi nulla\nvera\nfalsa\n\n\n\n\naccettata\nOK\nErrore tipo II\n\n\nrifiutata\nErrore tipo I\nOK\n\n\n\n\n\n\nLa probabilità di un Errore di tipo I è \\(\\alpha\\), la probabilità di un Errore di tipo II è \\(\\beta\\).\nIl valore \\(1-\\beta\\) è la potenza \\(P\\) del test\n\n\nNota: \\(\\alpha \\neq 1-\\beta\\)"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#test-di-student",
    "href": "slides/ADAS/1-statistica.html#test-di-student",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Test di Student",
    "text": "Test di Student\n\n\n\n\n\nWilliam S. Gosset\n\n\n\n\nWilliam S. Gosset, detto Student, mastro birraio Guinness a Dublino, ~1900\nProblema: come capire se due medie diverse su campioni di processo indicano due diversi processi?\nSiano i due campioni normali e indipendenti \\(y_{1,i},~i=1, 2, \\dots, n_1\\) e \\(y_{2,i},~i=1, 2, \\dots, n_2\\), sarà ovviamente \\(\\bar{y_1}\\neq\\bar{y_2}\\), quindi come faccio a capire se vale \\(H_0\\) o \\(H_1\\)?\n\n\nOvviamente la risposta è probabilistica: posso solo associare una probabilità d’errore \\(\\alpha\\) al test di ipotesi \\[\n\\begin{eqnarray}\nH_0:~&\\mu_1 = \\mu_2 \\\\\nH_1:~&\\mu_1 \\neq \\mu_2 \\\\\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#test-di-student-1",
    "href": "slides/ADAS/1-statistica.html#test-di-student-1",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Test di Student",
    "text": "Test di Student\n\n\n\nI campioni C1 e C3 non avranno valori comuni: è molto probabile che vengano da popolazioni differenti\nI campioni C1 e C2 invece sono più difficili da distinguere\nIntuitivamente, \\(H_1\\) è tanto più probabile quanto più le medie sono distanti e le varianze strette\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNel grafico, i trattini verticali in basso rappresentano i valori di 10 campioni casuali estratti dalle rispettive popolazioni"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#test-di-student-2",
    "href": "slides/ADAS/1-statistica.html#test-di-student-2",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Test di Student",
    "text": "Test di Student\nPer i due campioni \\(y_{1,i}\\) e \\(y_{2,i}\\) è possibile definire la variabile: \\[\nt_0 = \\frac{\\bar{y_2} - \\bar{y_1}}{S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_s} }}\n\\] dove \\(S_p^2\\) è chiamata varianza comune (pooled variance) e vale: \\[\nS_p^2 = \\frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1+n_2-2}\n\\] La \\(t_0\\) è il rapporto tra una v.a. normale al numeratore e una v.a. \\(\\chi^2\\) al denominatore. Di conseguenza è essa stessa una v.a. ed è definita come una T di Student.\n\nIl numero di gradi di libertà è lo stesso della \\(\\chi^2\\) ed è \\(n_1+n_2-2\\)\n\\(t_0\\) è detta statistica di test e vale che \\(t_0\\sim t_{n_1+n_2-2}\\)"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#test-di-student-3",
    "href": "slides/ADAS/1-statistica.html#test-di-student-3",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Test di Student",
    "text": "Test di Student\n\nÈ evidente che casi come C1 vs. C3 nel grafico precedente avranno un valore di \\(|t_0|\\) più alto che casi come C1 vs. C2 (rapporto tra distanza e varianza)\nMa data una coppia di campioni provenienti dalla stessa popolazione la probabilità di valori elevati di \\(|t_0|\\) è molto bassa\nDato che conosciamo la distribuzione di \\(t_0\\) possiamo quindi calcolare la probabilità di riscontrare un dato valore assumendo che valga \\(H_1\\)\nIn altre parole, la probabilità di rifiutare \\(H_0\\) quando essa è vera è pari alla probabilità di riscontrare un valore pari o superiore a \\(t_0\\) nella distribuzione di Student\nIn realtà, il segno di \\(t_0\\) è arbitrario, quindi bisogna verificare la probabilità di riscontrare un valore esterno all’intervallo \\([-|t_0|, |t_0|]\\)"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#test-di-student-4",
    "href": "slides/ADAS/1-statistica.html#test-di-student-4",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Test di Student",
    "text": "Test di Student\nIn definitiva, la probabilità di un errore di tipo I nel test di Student risulta: \\[\np\\mathrm{-value} = 2\\mathrm{CDF}^+_t(|t_0|,n_1+n_2-2)\n\\] dove \\(\\mathrm{CDF}^{+}_{t}\\) è la distribuzione cumulata, coda alta, di una T di Student con \\(n_1+n_2-2\\) g.d.l, e dove il fattore 2 tiene in considerazione l’ultimo punto della pagina precedente\nLa probabilità d’errore di un qualunque test statistico è chiamata p-value\n\nPiù piccolo è il p-value, più siamo spinti a rifiutare \\(H_0\\)\nTipicamente, si tende a rifiutare \\(H_0\\) quando il p-value diventa minore del 5%"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#test-di-student-5",
    "href": "slides/ADAS/1-statistica.html#test-di-student-5",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Test di Student",
    "text": "Test di Student\nSpesso nei test statistici si accetta o rifiuta \\(H_0\\) sulla base di una soglia di probabilità d’errore di tipo I, indicata con \\(\\alpha\\)\n\nSi fissa un \\(\\alpha\\) a priori in funzione del rischio associato al test\nSi calcola il valore di \\(t_{0,\\mathrm{max}}\\) che corrisponde a \\(\\alpha\\)\nSe risulta \\(|t_0| \\geq t_{0,\\mathrm{max}}\\) si rifiuta \\(H_0\\) con una probabilità d’errore minore di \\(\\alpha\\)\n\nIl valore di soglia si calcola mediante la funzione quantile e si rifiuta \\(H_0\\) quando: \\[\n|t_0| \\geq t_{0,\\mathrm{max}} = t_{\\alpha/2, n_1+n_2-2}\n\\] dove \\(t_{\\alpha/2, n_1+n_2-2}\\) è appunto la funzione quantile, coda alta, della T di Student valutata per la probabilità \\(\\alpha/2\\) e per \\(n_1+n_2-2\\) g.d.l."
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#test-di-student-6",
    "href": "slides/ADAS/1-statistica.html#test-di-student-6",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Test di Student",
    "text": "Test di Student\n\n\n\n\n\n\n\n\n\n\n\n\nNote:\n\nL’ordinata della \\(\\mathrm{CDF}^+\\) in \\(t_0\\) è uguale all’area sottesa dalla \\(\\mathrm{PDF}\\) in \\([t_0, +\\infty)\\) (in grigio)\nIl segno del numeratore in \\(t_0\\) è arbitrario, quindi \\(\\mathrm{CDF}^+(t_0)\\) è la metà della probabilità d’errore di tipo I complessiva, cioè \\(\\alpha/2\\)"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#tabella-dei-quantili",
    "href": "slides/ADAS/1-statistica.html#tabella-dei-quantili",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Tabella dei quantili",
    "text": "Tabella dei quantili\nIn assenza dei calcolatori, il Test di Student veniva effettuato predeterminando \\(\\alpha\\) e decidendo se rifiutare \\(H_0\\) sulla base della tabella dei quantili:\n\n\n\n\n\n\n\ndof\n0.1\n0.05\n0.025\n0.01\n0.005\n0.0025\n0.001\n\n\n\n\n1\n3.078\n6.314\n12.706\n31.821\n63.657\n127.321\n318.309\n\n\n2\n1.886\n2.920\n4.303\n6.965\n9.925\n14.089\n22.327\n\n\n3\n1.638\n2.353\n3.182\n4.541\n5.841\n7.453\n10.215\n\n\n4\n1.533\n2.132\n2.776\n3.747\n4.604\n5.598\n7.173\n\n\n5\n1.476\n2.015\n2.571\n3.365\n4.032\n4.773\n5.893\n\n\n6\n1.440\n1.943\n2.447\n3.143\n3.707\n4.317\n5.208\n\n\n7\n1.415\n1.895\n2.365\n2.998\n3.499\n4.029\n4.785\n\n\n8\n1.397\n1.860\n2.306\n2.896\n3.355\n3.833\n4.501\n\n\n9\n1.383\n1.833\n2.262\n2.821\n3.250\n3.690\n4.297\n\n\n10\n1.372\n1.812\n2.228\n2.764\n3.169\n3.581\n4.144\n\n\n\n\n\n\n\n\n\n\n\ndof\n0.1\n0.05\n0.025\n0.01\n0.005\n0.0025\n0.001\n\n\n\n\n11\n1.363\n1.796\n2.201\n2.718\n3.106\n3.497\n4.025\n\n\n12\n1.356\n1.782\n2.179\n2.681\n3.055\n3.428\n3.930\n\n\n13\n1.350\n1.771\n2.160\n2.650\n3.012\n3.372\n3.852\n\n\n14\n1.345\n1.761\n2.145\n2.624\n2.977\n3.326\n3.787\n\n\n15\n1.341\n1.753\n2.131\n2.602\n2.947\n3.286\n3.733\n\n\n16\n1.337\n1.746\n2.120\n2.583\n2.921\n3.252\n3.686\n\n\n17\n1.333\n1.740\n2.110\n2.567\n2.898\n3.222\n3.646\n\n\n18\n1.330\n1.734\n2.101\n2.552\n2.878\n3.197\n3.610\n\n\n19\n1.328\n1.729\n2.093\n2.539\n2.861\n3.174\n3.579\n\n\n20\n1.325\n1.725\n2.086\n2.528\n2.845\n3.153\n3.552\n\n\n\n\n\n\nAd esempio, per un campione con 8 g.d.l. e \\(\\alpha=0.1\\) risulta un valore critico di \\(t_0=1.86\\): ogni valore di \\(t_0\\) calcolato superiore a tale valore (in modulo) comporta il rifiuto di \\(H_0\\) con una probabilità d’errore inferiore al 10%"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#varianti-del-test-di-student",
    "href": "slides/ADAS/1-statistica.html#varianti-del-test-di-student",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Varianti del test di Student",
    "text": "Varianti del test di Student\nIl test di Student sopra visto vale per il caso più generico. Esistono varianti del test per le seguenti condizioni, che possono combinarsi per dare luogo a 4 diversi test:\n\ntest a uno o due campioni\ntest a uno o due lati (in \\(H_1\\) la disuguaglianza è sostituita con un \\(&gt;\\) o un \\(&lt;\\))\n\nInoltre, nel caso di test a due campioni è possibile assumere che i campioni siano omoschedastici oppure no\n\n\nDue campioni sono detti omoschedastici quando provengono da popolazioni con identica varianza"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#varianti-del-test-di-student-1",
    "href": "slides/ADAS/1-statistica.html#varianti-del-test-di-student-1",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Varianti del test di Student",
    "text": "Varianti del test di Student\n\n\n\nTest a un campione:\n\n\\[\n\\begin{eqnarray}\nH_0:~& \\mu = \\mu_0 \\\\\nH_1:~& \\mu \\neq \\mu_0\n\\end{eqnarray}\n\\]\n\\[\nt_0 = \\frac{\\mu_0 - \\bar y}{S/\\sqrt{n}}\n\\]\n\n\nTest a un lato:\n\n\\[\n\\begin{eqnarray}\nH_0:~& \\mu_1 = \\mu_2 \\\\\nH_1:~& \\mu_1 \\gtrless \\mu_2\n\\end{eqnarray}\n\\]\n\\[\n\\begin{eqnarray}\nt_0 &\\gtrless& \\pm t_{\\alpha, k} \\\\\np\\textrm{-value} &=& CDF_t^{\\pm}(t_0, k)\n\\end{eqnarray}\n\\]\n\nNota: il test a un lato, a pari valore di \\(t_0\\), ha una soglia di rifiuto di \\(H_0\\) più bassa, ed è quindi più potente"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#omoschedasticità",
    "href": "slides/ADAS/1-statistica.html#omoschedasticità",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Omoschedasticità",
    "text": "Omoschedasticità\nIl test a due campioni sopra visto assume che i due campioni siano omoschedastici. Se non lo sono, è necessario rivedere la definizione di \\(t_0\\) come segue (Test di Welch): \\[\nt_0 = \\frac{\\bar{y_1} - \\bar{y_2}}{\\sqrt{S_1^2/n_1 + S_2^2/n_2}};~~~\\nu=\\frac{(S_1^2/n_1 + S_2^2/n_2)^2}{\\frac{(S_1^2/n_1)^2}{n_1-1}+\\frac{(S_2^2/n_2)^2}{n_2-1}}\n\\] L’ipotesi di omoschedasticità va preliminarmente verificata con un test della varianza: \\[\n\\begin{eqnarray}\nH_0 :~& \\sigma^2_1 = \\sigma^2_2 \\\\\nH_1 :~& \\sigma^2_1 \\neq \\sigma^2_2\n\\end{eqnarray},~~~F_0=\\frac{S^2_1}{S^2_2}\\sim\\mathcal{F}_{n_1-1, n_2-1}\n\\] Cioè se il p-value associato a \\(F_0\\) è piccolo, si assume che i campioni non siano omoschedastici e quindi si effettua il test di Welch; altrimenti vale il test di Student"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#il-test-di-student-accoppiato",
    "href": "slides/ADAS/1-statistica.html#il-test-di-student-accoppiato",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Il test di Student accoppiato",
    "text": "Il test di Student accoppiato\nNel caso di test di Student a due campioni, quando essi hanno la stessa dimensione e sono raccolti due a due in condizioni molto simili, è opportuno accoppiarli e effettuare il test di Student accoppiato\nOgni misura è esprimibile come: \\[\ny_{ij}=\\mu_i+\\beta_j+\\varepsilon_{ij};\\hspace{9pt} \\left\\{ \\begin{array}{l}i=1,2\\\\j=1,2,\\dots ,n\\end{array} \\right.\n\\] Se definisco \\(d_j=y_{1j} - y_{2j}\\), ricordando le proprietà dell’operatore \\(E(\\cdot)\\), risulta \\(\\mu_d = \\mu_1 - \\mu_2\\). Quindi posso riformulare una coppia di ipotesi equivalenti: \\[\n\\begin{eqnarray}\nH_0 :~& \\mu_d = 0 \\\\\nH_1 :~& \\mu_d \\neq 0\n\\end{eqnarray}\n\\] Quindi il test accoppiato è un test a un campione, con il vantaggio che gli effetti casuali tra coppie di osservazioni non influiscono sul risultato del test"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#lintervallo-di-confidenza",
    "href": "slides/ADAS/1-statistica.html#lintervallo-di-confidenza",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "L’intervallo di confidenza",
    "text": "L’intervallo di confidenza\nDato parametro ignoto \\(\\vartheta\\), vogliamo definire due statistiche \\(\\color{darkred}{L}\\) e \\(\\color{green}{U}\\), tali per cui la probabilità: \\(P(\\color{darkred}L\\color{darkblue}\\leq\\vartheta\\leq \\color{green}U\\color{darkblue})= 1-\\alpha\\). In questo caso l’intervallo \\([L,U]\\) è detto intervallo di confidenza per il parametro \\(\\vartheta\\).\nConsideriamo un T-test ad un campione. Sappiamo che: \\[\nt_0=\\frac{\\bar x - \\mu}{S/\\sqrt{n}}\\sim t_{n-1}\n\\] Se \\(c\\) è \\(t_{\\alpha/2, n-1}\\), allora, per definizione: \\(P(-c\\leq t_0\\leq c) = 1-\\alpha\\)\nSostituendo \\(t_0\\) otteniamo: \\[\nP(\\color{darkred}{\\bar x - cS/\\sqrt{n}}\\color{darkblue} \\leq \\mu \\leq \\color{green}\\bar x + cS/\\sqrt{n}\\color{darkblue}) = 1-\\alpha\n\\]"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#intervallo-di-confidenza",
    "href": "slides/ADAS/1-statistica.html#intervallo-di-confidenza",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Intervallo di confidenza",
    "text": "Intervallo di confidenza\nIn pratica, significa che il valore atteso della popolazione (che è ignoto!) ha la probabilità \\(1-\\alpha\\) di ricadere nell’intervallo \\([\\bar x - cS/\\sqrt{n}, \\bar x + cS/\\sqrt{n}]\\), detto di confidenza\nSe il \\(\\mu_0\\) del corrispondente test sta al di fuori dell’intervallo di confidenza, allora possiamo rifiutare \\(H_0\\) con un errore inferiore a \\(\\alpha\\).\nPer un test a due campioni risulta: \\[\nP\\left( -t_{\\alpha/2,n_1+n_2-2}\\leqslant\\frac{(\\bar y_1-\\bar y_2)-(\\mu_1-\\mu_2)}{S_p\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}}\\leqslant t_{\\alpha/2,n_1+n_2-2} \\right)=1-\\alpha\n\\] Quindi l’intervallo di confidenza \\([L,U]\\) è definito da: \\[\n\\left[(\\bar y_1-\\bar y_2)-t_{\\alpha/2,n_1+n_2-2}S_p\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}},~\n(\\bar y_1-\\bar y_2)+t_{\\alpha/2,n_1+n_2-2}S_p\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}~\\right]\n\\]"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#potenza-del-test-di-student",
    "href": "slides/ADAS/1-statistica.html#potenza-del-test-di-student",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Potenza del test di Student",
    "text": "Potenza del test di Student\nLa potenza di un test statistico è la probabilità di rifiutare l’ipotesi nulla quando essa è falsa, cioè \\(1-\\beta\\). È l’affidabilità del test. Essa dipende ovviamente dalla dimensione dei campioni. È possibile calcolare le curve di potenza per diverse dimensioni dei campioni e per diverse dimensioni di dello scarto relativo \\(\\delta =(\\mu_1 - \\mu_2) / \\sigma\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa potenza del test diminuisce con la differenza tra le medie\nLa potenza del test aumenta con la dimensione dei campioni\nLa potenza del test aumenta con \\(\\alpha\\)\nCon \\(\\alpha=0.05\\) e \\(n=10\\), la potenza del test è \\(\\geq0.9\\) per \\(\\delta\\geq1.5\\)"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#test-di-student-7",
    "href": "slides/ADAS/1-statistica.html#test-di-student-7",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Test di Student",
    "text": "Test di Student\nHo due campioni di dati, uno con \\(n_1=10\\), \\(\\mu_1 = 13.59213\\), \\(\\sigma_1=4.33921\\) e l’altro con \\(n_2=10\\), \\(\\mu_2 = 10.30381\\), \\(\\sigma_2=2.24028\\). Voglio confrontare le medie dei due campioni con un test di Student, assumendo \\(\\alpha=0.05\\):\n\n\\(\\mu_1\\neq\\mu_2\\), con un \\(p\\)-value pari a 0.0473\n\\(\\mu_1 = \\mu_2\\), con un \\(p\\)-value pari a 0.0522\n\\(\\mu_1\\neq \\mu_2\\), con un \\(p\\)-value pari a 0.0623\n\\(\\mu_1 = \\mu_2\\), con un \\(p\\)-value pari a 1.39E-4\n\\(\\mu_1\\neq \\mu_2\\), con un \\(p\\)-value pari a 0.0487"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#calcolatrice",
    "href": "slides/ADAS/1-statistica.html#calcolatrice",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Calcolatrice",
    "text": "Calcolatrice\nUsare il riquadro sottostante per risolvere l’esercizio precedente (provare anche con la tabella dei quantili)\nNota: la radice quadrata si ottiene con sqrt(x), l’elevamento a potenza con x^y."
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#anomalie",
    "href": "slides/ADAS/1-statistica.html#anomalie",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Anomalie",
    "text": "Anomalie\nLa raccolta dei dati relativi ad un campione può essere affetta da errori:\n\nnell’operazione di misura\nnell’operazione di trascrittura del dato\n\nQuesti errori ovviamente hanno un’elevata probabilità di non concordare con il resto dei dati, cioè di essere troppo lontani dalla media in rapporto alla varianza tipica del processo.\nQuesti errori sono chiamati anomalie ed è buona norma identificarli ed eliminarli immediatamente dopo aver concluso la raccolta dei dati\n\n\nIn Inglese le anomalie sono note come outlier"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#anomalie-metodo-grafico",
    "href": "slides/ADAS/1-statistica.html#anomalie-metodo-grafico",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Anomalie: metodo grafico",
    "text": "Anomalie: metodo grafico\nIl più comune metodo grafico per individuare le anomalie è il box-plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nil box va dal primo al terzo quartile (metà dei dati)\nla linea nel box è la mediana\ni baffi vanno dal minimo al punto più lontano ma non oltre 1.5 volte l’interquartile\ni punti oltre i baffi sono possibili anomalie"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#anomalie-test-statistici",
    "href": "slides/ADAS/1-statistica.html#anomalie-test-statistici",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Anomalie: test statistici",
    "text": "Anomalie: test statistici\n\nCriterio di ChauvenetTest di Grubb\n\n\nNon è un vero test ma un criterio in base al quale eliminare il punto più lontano dalla media (uno solo!)\nDati \\(\\left&lt;x_1, x_2,\\dots,x_n\\right&gt;\\) con distribuzione normale, si calcola la massima differenza assoluta \\(s_0=\\underset{i=1,\\dots,n}{\\max}\\left(\\frac{|x_i - \\bar x|}{S_x}\\right)\\)\nPer via della normalità degli \\(x_i\\) è evidente che \\(|x_i-\\bar x|/S_x\\sim\\mathcal{N}(0,1)\\). Possiamo quindi calcolare la probabilità di un valore maggiore o uguale a \\(s_0\\) dalla funzione di ripartizione, coda superiore: \\[\nP_s=\\mathrm{CDF}_{\\mathcal{N}}^+(s_0)\n\\] Su \\(n\\) osservazioni normali ci aspettiamo \\(nP_s\\) valori più grandi di \\(s_0\\). Quindi, se risulta \\(nP_s &lt; 0.5\\), mentre noi abbiamo un punto sospetto, allora possiamo scartare l’anomalia\n\n\nIl test di Grubb è un vero e proprio test statistico, basato su una coppia di ipotesi (\\(H_1\\) porta allo scarto della sospetta anomalia), su una statistica di test e sul calcolo di un p-value.\nIn breve: \\[\n\\begin{eqnarray}\nG_0 &=& \\frac{\\underset{i=1,\\dots,n}{\\max}|y_i-\\bar y|}{S_x}\\\\\nG_0 &&gt;& \\frac{n-1}{n}\\sqrt{\\frac{t^2_{\\alpha/(2n),n-2}}{n-2+t^2_{\\alpha/(2n),n-2}}} \\Rightarrow \\neg H_0\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#analisi-di-normalità",
    "href": "slides/ADAS/1-statistica.html#analisi-di-normalità",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Analisi di normalità",
    "text": "Analisi di normalità\nI test visti fin qui assumono sempre che i campioni su cui si opera derivino da una distribuzione normale, per quanto a parametri incogniti\nPrima di effettuare un qualsiasi test, quindi, è necessario verificare l’ipotesi di normalità\nCome per l’analisi delle anomalie, anche in questo caso è possibile utilizzare sia metodi grafici che test statistici\nIn generale, i test statistici sono sempre preferibili, perché\n\nsono meno soggetti a interpretazione personale\nconsentono l’automazione della scelta all’interno di algoritmi"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#analisi-di-normalità-istogramma",
    "href": "slides/ADAS/1-statistica.html#analisi-di-normalità-istogramma",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Analisi di normalità: istogramma",
    "text": "Analisi di normalità: istogramma\n\n\n\n\n\n\n\n\n\n\n\n\n\nPer il numero di canne, o bin, si usa la formula di Sturges: \\(K = \\lceil\\log_2 n \\rceil + 1\\) o la formula di Scott: \\(h=3.49 s / \\sqrt[3]{n}\\) (\\(h\\) è l’ampiezza della canna)"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#sturges-e-scott",
    "href": "slides/ADAS/1-statistica.html#sturges-e-scott",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Sturges e Scott",
    "text": "Sturges e Scott"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#analisi-di-normalità-grafico-quantile-quantile",
    "href": "slides/ADAS/1-statistica.html#analisi-di-normalità-grafico-quantile-quantile",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Analisi di normalità: grafico quantile-quantile",
    "text": "Analisi di normalità: grafico quantile-quantile\n\n\nTabella campionaria:\n\n\n\n\n\n\ni\nx\nf\nq\n\n\n\n\n1\n-1.540\n0.061\n-1.547\n\n\n2\n-0.929\n0.159\n-1.000\n\n\n3\n-0.326\n0.256\n-0.655\n\n\n4\n-0.295\n0.354\n-0.375\n\n\n5\n-0.006\n0.451\n-0.123\n\n\n6\n0.415\n0.549\n0.123\n\n\n7\n1.263\n0.646\n0.375\n\n\n8\n1.272\n0.744\n0.655\n\n\n9\n1.330\n0.841\n1.000\n\n\n10\n2.405\n0.939\n1.547\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa colonna x sono le osservazioni ordinate; la ripartizione dei punti, colonna f, è corretta con la formula di Bloom: \\(f=(1-3/8)/(n+1-3/4)\\); la colonna q rappresenta i quantili normali standard di f.\nLa diagonale sul grafico passa per i due punti del primo e terzo quartile"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#analisi-di-normalità-grafico-quantile-quantile-1",
    "href": "slides/ADAS/1-statistica.html#analisi-di-normalità-grafico-quantile-quantile-1",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Analisi di normalità: grafico quantile-quantile",
    "text": "Analisi di normalità: grafico quantile-quantile"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#analisi-di-normalità-test-del-chi-quadro",
    "href": "slides/ADAS/1-statistica.html#analisi-di-normalità-test-del-chi-quadro",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Analisi di normalità: test del Chi-Quadro",
    "text": "Analisi di normalità: test del Chi-Quadro\nÈ un test statistico per verificare l’ipotesi nulla che un dato campione provenga da una data (a scelta!) distribuzione\n\nRaggruppiamo \\(\\left&lt;x_1, x_2, \\dots, x_n\\right&gt;\\) in \\(k\\) classi, t.c ogni classe abbia almeno 4–5 elementi. La largezza di ciascun intervallo può essere diversa\nSia \\(O_i,~i=1,2,\\dots,k\\) il numero di osservazioni in ogni classe, e \\(E_i\\) il numero di osservazioni attese (expected) in ogni classe per la distribuzione ipotizzata\nLe differenze tra \\(O_i\\) e \\(E_i\\) saranno tanti più grandi quanto più varrà \\(H_1\\). Quindi si definisce la statistica di test:\n\n\\[\nX_0^2 = \\sum_{i=1}^k \\frac{(O_i-E_i)^2}{E_i} \\sim \\chi_{k-p-1}^2\n\\] dove \\(p\\) è il numero di parametri della distribuzione ipotizzata (2 per la normale)"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#analisi-di-normalità-test-di-shapiro-wilk",
    "href": "slides/ADAS/1-statistica.html#analisi-di-normalità-test-di-shapiro-wilk",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Analisi di normalità: test di Shapiro-Wilk",
    "text": "Analisi di normalità: test di Shapiro-Wilk\nTra i test di normalità è quello con la potenza maggiore per un dato livello di significatività\nLa statistica di test è basata su una distribuzione anonima e nota solo numericamente\nPer campioni sufficientemente grandi, il test è così potente da evidenziare anche piccole deviazioni dalla normalità, dovute ad esempio a anomalie. In questi casi è opportuno accompagnarlo con un grafico Quantile-Quantile per discriminare questi effetti"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#verifica-di-correlazione",
    "href": "slides/ADAS/1-statistica.html#verifica-di-correlazione",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Verifica di correlazione",
    "text": "Verifica di correlazione\n\n\n\nIl T-test vale per campioni normali e indipendentemente distribuiti (IID)\nÈ necessario verificare questa ipotesi\nSe riporto in un grafico i valori dei due campioni nell’ordine in cui sono stati acquisiti posso osservare correlazione (s1 vs. s3) o mancanza di correlazione (s1 vs. s2)\nÈ evidente che \\(\\rho_{s_1s_2}\\) sarà vicina a 0, mentre \\(\\rho_{s_1s_3}\\) sarà vicina a 1\n\n\n\n\n\n\n\n\n\n\n\n\nEsiste un test di correlazione di Pearson che fornisce un p-value associato all’ipotesi alternativa che i due campioni siano correlati."
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#fattori",
    "href": "slides/ADAS/1-statistica.html#fattori",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Fattori",
    "text": "Fattori\n\nUn fattore è un parametro che determina il risultato di un processo\nÈ possibile avere fattori quantitativi o qualitativi\n\nFattori quantitativi assumono livelli rappresentabili da numeri reali\nFattori qualitativi assumono livelli non ordinati e non misurabili\n\nI livelli di un fattore sono chiamati trattamenti\nIl valore misurato come uscita del processo è chiamato resa (yield)"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#esperimenti-a-un-fattore-e-più-livelli",
    "href": "slides/ADAS/1-statistica.html#esperimenti-a-un-fattore-e-più-livelli",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Esperimenti a un fattore e più livelli",
    "text": "Esperimenti a un fattore e più livelli\nEsempio: resistenza a trazione di un filato in funzione della percentuale di fibre di cotone (fattore quantitativo)\n\nOgni trattamento è ripetuto 5 volte\nIl boxplot è utile come orientamento ma per campioni così piccoli l’indicazione delle anomalie non è affidabile\n\n\n\n\n\n\n\n\n% Cotone\n#1\n#2\n#3\n#4\n#5\n\n\n\n\n15\n7\n7\n15\n11\n9\n\n\n20\n12\n17\n12\n18\n18\n\n\n25\n14\n18\n18\n19\n19\n\n\n30\n19\n25\n22\n19\n23\n\n\n35\n7\n10\n11\n15\n11"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#esperimenti-a-un-fattore-e-più-livelli-1",
    "href": "slides/ADAS/1-statistica.html#esperimenti-a-un-fattore-e-più-livelli-1",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Esperimenti a un fattore e più livelli",
    "text": "Esperimenti a un fattore e più livelli\nLa domanda è: i trattamenti producono variazioni di resa statisticamente significative?\nOgni singolo valore di resa può essere espresso come (modello delle medie): \\[\ny_{ij}=\\mu_i+\\varepsilon_{ij},~~\\left\\{\\begin{array}{rcl}\ni &=& 1, 2, \\dots, a \\\\\nj &=& 1, 2, \\dots, n\n\\end{array}\\right.\n\\] dove \\(i\\) sono i trattamenti, \\(j\\) sono le ripetizioni, \\(\\varepsilon_{ij}\\) sono i residui, cioè la componente puramente stocastica e a media nulla della variabile aleatoria \\(y_{ij}\\), mentre \\(\\mu_i\\) è la componente deterministica\nPossiamo separare \\(\\mu_i = \\mu + \\tau_i\\), dove \\(\\tau_i\\) è il contributo del trattamento alla media complessiva \\(\\mu\\) (modello degli effetti): \\[\ny_{ij}=\\mu + \\tau_i+\\varepsilon_{ij},~~\\left\\{\\begin{array}{rcl}\ni &=& 1, 2, \\dots, a \\\\\nj &=& 1, 2, \\dots, n\n\\end{array}\\right.\n\\]"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#esperimenti-a-un-fattore-e-più-livelli-2",
    "href": "slides/ADAS/1-statistica.html#esperimenti-a-un-fattore-e-più-livelli-2",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Esperimenti a un fattore e più livelli",
    "text": "Esperimenti a un fattore e più livelli\n\nPossiamo sfruttare i modelli precedenti per studiare la significatività del fattore\nUsando il modello delle medie abbiamo la coppia di ipotesi:\n\n\\[\n\\begin{eqnarray}\nH_0&:&~\\mu_1=\\mu_2=\\dots =\\mu_a \\\\\nH_1&:&~\\mu_i\\neq\\mu_j~~~\\textrm{per almeno una coppia }(i, j)\n\\end{eqnarray}\n\\]\n\nEquivalentemente, usando il modello degli effetti:\n\n\\[\n\\begin{eqnarray}\nH_0&:&~\\tau_1=\\tau_2=\\dots =\\tau_a = 0 \\\\\nH_1&:&~\\tau_i\\neq0~~~\\textrm{per almeno un }i\n\\end{eqnarray}\n\\]\n\nPer sciogliere l’ipotesi dobbiamo formulare una statistica di test appropriata"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#decomposizione-della-varianza",
    "href": "slides/ADAS/1-statistica.html#decomposizione-della-varianza",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Decomposizione della varianza",
    "text": "Decomposizione della varianza\nDefinizioni:\n\nNotazione abbreviata per le somme:\n\n\\[\n\\begin{eqnarray}\ny_{i.}&=&\\sum_{j=1}^n y_{ij},~\\bar y_{i.}=y_{i.}/n,~i=1, 2, \\dots, a \\\\\ny_{..}&=&\\sum_{i=1}^a\\sum_{j=1}^n y_{ij},~\\bar y_{..}=y_{..}/N,~N=na\n\\end{eqnarray}\n\\]\n\nIntroduciamo la somma quadratica totale corretta: \\[\n\\begin{equation}\nSS_T=\\sum_{i=1}^a\\sum_{j=1}^n (y_{ij}-\\bar y_{..})^2 = (N-1)V(y_{ij})\n\\end{equation}\n\\]"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#decomposizione-della-varianza-1",
    "href": "slides/ADAS/1-statistica.html#decomposizione-della-varianza-1",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Decomposizione della varianza",
    "text": "Decomposizione della varianza\nConsiderando che: \\[\nSS_T=\\sum_{i=1}^a\\sum_{j=1}^n (y_{ij}-\\bar y_{..})^2 = \\sum_{i=1}^a\\sum_{j=1}^n [(\\bar y_{i.}-\\bar y_{..})+(y_{ij}-\\bar y_{i.})]^2\n\\]\novvero:\n\\[\nSS_T= n\\sum_{i=1}^a(\\bar y_{i.}-\\bar y_{..})^2 + \\sum_{i=1}^a\\sum_{j=1}^n (y_{ij}-\\bar y_{i.})^2 + 2\\sum_{i=1}^a\\sum_{j=1}^n(\\bar y_{i.}-\\bar y_{..})(y_{ij}-\\bar y_{i.})\n\\]\ne siccome \\(\\sum_{j=1}^n (y_{ij}-\\bar y_{i.})=y_{i.}-n\\bar y_{i.}=0\\), ne consegue che la \\(SS_T\\) può essere decomposta come:\n\\[\nSS_T=\\sum_{i=1}^a\\sum_{j=1}^n (y_{ij}-\\bar y_{..})^2=n\\sum_{i=1}^a(\\bar y_{i.}-\\bar y_{..})^2 + \\sum_{i=1}^a\\sum_{j=1}^n (y_{ij}-\\bar y_{i.})^2=SS_{tr}+SS_E\n\\]"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#decomposizione-della-varianza-2",
    "href": "slides/ADAS/1-statistica.html#decomposizione-della-varianza-2",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Decomposizione della varianza",
    "text": "Decomposizione della varianza\nAbbiamo decomposto la \\(SS_T\\) (che è parente stretta della varianza) nelle somme quadratiche medie:\n\n\\(SS_{tr}\\), che misura la variabilità tra un trattamento e l’altro\n\\(SS_E\\), che misura la variabilità all’interno dei trattamenti\n\nRicordando quanto detto nella definizione della distribuzione \\(\\mathcal{X}^2\\):\n\\[\n\\frac{\\mathit{SS}}{\\sigma^2}=\\frac{\\sum_{i=1}^n(y_i-\\bar y)^2}{\\sigma^2} \\sim \\mathcal{X}^2_{n-1}\n\\]\nrisulta che:\n\\[\n\\sum_{i=1}^{a}\\sum_{j=1}^{n}\\frac{(y_{ij}-\\bar y_{..})^2}{\\sigma^2} = \\frac{SS_T}{\\sigma^2} \\sim \\chi^2_{N-1},~~~\\frac{SS_E}{\\sigma^2} \\sim \\chi^2_{N-a},~~~\\frac{SS_{tr}}{\\sigma^2} \\sim \\chi^2_{a-1}\n\\]"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#decomposizione-della-varianza-3",
    "href": "slides/ADAS/1-statistica.html#decomposizione-della-varianza-3",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Decomposizione della varianza",
    "text": "Decomposizione della varianza\n\nIl rapporto tra una somma quadratica e il relativo numero di g.d.l. è uguale alla varianza\nQuindi, se vale \\(H_0\\) deve essere \\(E(SS_{tr}/(a-1)) = E(SS_E/(N-a))\\), dato che si tratta del valore atteso di diversi campioni tratti dallo stesso gruppo di osservazioni, tutte provenienti dalla stessa popolazione (\\(H_0\\) significa che i trattamenti non sono significativi)\nMa allora possiamo scrivere:\n\n\\[\nF_0 = \\frac{SS_{tr}/(a-1)}{SS_E/(N-a)}=\\frac{MS_{tr}}{MS_E} \\sim F_{a-1,N-a}\n\\] Dove \\(MS_\\cdot\\) sono dette somme quadratiche medie\nSulla base dell’ultima equazione possiamo calcolare il p-value associato all’ipotesi \\(H_1\\): per valori piccoli possiamo affermare che almeno un trattamento ha effetto statisticamente significativo"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#teorema-di-cochran",
    "href": "slides/ADAS/1-statistica.html#teorema-di-cochran",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Teorema di Cochran",
    "text": "Teorema di Cochran\nLa definizione di \\(F_0\\) presuppone che \\(MS_{tr}\\) e \\(MS_E\\) siano indipendenti. Dato che sono il risultato della decomposizione di \\(SS_T\\) in \\(SS_{tr}+SS_E\\), ciò non è scontato. Vale però il teorema:\n\n\nTeorema\n\nSiano \\(Z_i\\sim \\mathcal{N}(0,1),~i=1,2,\\dots,\\nu\\) campioni indipendenti, con \\(\\sum_{i=1}^\\nu Z_i^2=Q_1+Q_2+\\dots+Q_s\\) dove \\(s\\leqslant \\nu\\) e \\(Q_i\\) ha \\(\\nu_i\\) gradi di libertà (\\(i=1, 2,\\dots,s\\)).\nAllora, \\(Q_1,Q_2,\\dots,Q_s\\) sono variabili casuali indipendenti distribuite come \\(\\mathcal{X}^2\\) con \\(\\nu_1,\\nu_2,\\dots,\\nu_s\\) gradi di libertà, se e soltanto se \\[\n\\nu=\\nu_1+\\nu_2+\\dots+\\nu_s\n\\]\nSiccome \\((N-a)+(a-1)=(N-1)\\), consegue che \\(SS_{tr}/\\sigma^2\\) e \\(SS_E/\\sigma^2\\) sono variabili casuali indipendenti distribuite come \\(\\mathcal{X}^2_{a-1}\\) e \\(\\mathcal{X}^2_{N-a}\\), rispettivamente."
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#anova-analysis-of-variance",
    "href": "slides/ADAS/1-statistica.html#anova-analysis-of-variance",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "ANOVA (ANalysis Of VAriance)",
    "text": "ANOVA (ANalysis Of VAriance)\nTornando all’esempio della resistenza dei filati misti, possiamo applicare la decomposizione della varianza ottenendo:\n\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nCotton\n4\n475.76\n118.94\n14.75682\n9.1e-06\n\n\nResiduals\n20\n161.20\n8.06\nNA\nNA\n\n\n\n\n\nLa tabella soprastante è detta tabella ANOVA\nIl p-value molto basso ci consente di rifiutare l’ipotesi nulla e affermare che almeno un trattamento ha effetti statisticamente significativi\nNon sappiamo comunque quanti e quali trattamenti siano significativi. Per studiare questo aspetto si utilizza il Test di Tukey"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#test-di-tukey",
    "href": "slides/ADAS/1-statistica.html#test-di-tukey",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Test di Tukey",
    "text": "Test di Tukey\nIl test valuta contemporaneamente tutte le seguenti coppie di ipotesi: \\[\n\\left.\n\\begin{eqnarray}\nH_0&:&~\\mu_i=\\mu_j \\\\\nH_1&:&~\\mu_i\\neq \\mu_j\n\\end{eqnarray}\n\\right\\} ~~\\forall i\\neq j\n\\] Per ogni coppia \\((i,j),~i\\neq j\\) si ha la statistica di test: \\[\nq_{0,ij}=\\frac{|\\bar y_{i.}-\\bar y_{j.}|}{S_{p,ij}\\sqrt{2/n}}\\sim\\mathcal{Q}_{a,k}\n\\] dove \\(n\\) è la dimensione dei campioni, uguale per tutti, \\(a\\) è il numero di trattamenti e \\(k\\) è il numero di gradi di libertà di \\(MS_E\\), cioè \\(N-a=an-a\\).\nPer ogni coppia si calcola quindi il p-value dalla CDF della distribuzione dell’intervallo studentizzato \\(\\mathcal{Q}\\) e si calcola l’intervallo di confidenza: \\[\n\\bar y_{i.}-\\bar y_{j.}-q_{\\alpha,a,N-a}\\frac{S_{p,ij}}{\\sqrt{n}} \\leqslant \\mu_i-\\mu_j\n\\leqslant \\bar y_{i.}-\\bar y_{j.}+q_{\\alpha,a,N-a}\\frac{S_{p,ij}}{\\sqrt{n}}\n\\]"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#test-di-tukey-1",
    "href": "slides/ADAS/1-statistica.html#test-di-tukey-1",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Test di Tukey",
    "text": "Test di Tukey\n\n\nGeneralmente, il risultato viene riportato in un grafico come a fianco\nLe coppie per cui l’intervallo di confidenza è a cavallo di 0 non hanno differenze significative, e viceversa\nNota: compiere altrettanti test di Student separati aumenterebbe la probabilità d’errore complessivo, che risulterebbe dalla combinazione delle probabilità d’errore dei singoli test\n\n\n\n\n\n\n\n\n\n\n\n\n\nCosa succede agli intervalli di confidenza passando da un intervallo al 95% ad un intervallo al 99%?"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#test-di-tukey-2",
    "href": "slides/ADAS/1-statistica.html#test-di-tukey-2",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Test di Tukey",
    "text": "Test di Tukey\nSe riduco \\(\\alpha\\) da 0.05 a 0.01, cosa succede agli intervalli di confidenza del test di Tukey?\n\nSi allargano tutti della stessa quantità\nSi allargano tutti, ma in misura diversa\nSi restringono tutti della stessa quantità\nSi restringono tutti, ma in misura diversa\nRimangono invariati"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#anova",
    "href": "slides/ADAS/1-statistica.html#anova",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "ANOVA",
    "text": "ANOVA\n\n\nNell’esperimento sopra riportato, una tabella ANOVA come la seguente è corretta oppure no?\n\n\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nCotton\n1\n33.62\n33.62000\n1.281632\n0.269261\n\n\nResiduals\n23\n603.34\n26.23217\nNA\nNA\n\n\n\n\n\n\n\nSì\nNo, la colonna Df è sospetta\nNo, la colonna Sum Sq è sospetta\nNo, la colonna Mean Sq è sospetta\nNo, la colonna F value è sospetta\nNo, la colonna Pr(&gt;F) è sospetta"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#il-significato-del-p-value",
    "href": "slides/ADAS/1-statistica.html#il-significato-del-p-value",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "Il significato del \\(p\\)-value",
    "text": "Il significato del \\(p\\)-value\n\n\nIl significato del \\(p\\)-value è illustrato con un esempio: si consideri un esperimento in cui si effettuano 1000 test di Student su altrettanti campioni di \\(n=15\\) elementi, ciascuno con un livello di significatività \\(\\alpha\\) del 5%\nSupponiamo che si tratti di test ad un campione, con ipotesi nulla \\(\\mu = \\mu_0\\) e ipotesi alternativa \\(\\mu \\neq \\mu_0\\), e supponiamo che tutti i campioni siano prelevati dalla stessa popolazione con valore atteso \\(\\mu_0\\)\nAllora il valore atteso delle predizioni errate (cioè quelle in cui vale \\(H_1\\)) è 5%\n\n\n\n\n\n\n\n\n\n\n\n\n\nNota: Lo stesso approccio frequentista è applicabile a qualsiasi altro test statistico di cui si calcoli il \\(p\\)-value"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#e-se-cambiamo-n",
    "href": "slides/ADAS/1-statistica.html#e-se-cambiamo-n",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "E se cambiamo \\(n\\)?",
    "text": "E se cambiamo \\(n\\)?\n\n\nSe aumentiamo la dimensione dei campioni da 15 a 50, la probabilità di errore di tipo I rimane sempre la stessa, a pari condizioni\nCom’è possibile?\nAumentando la dimensione del campione aumenta la potenza del test, cioè la sua capacità di discriminare anche piccoli differenze. Ma per un certo test, per una certa differenza, il \\(p\\)-value non dipende dalla dimensione del campione, perché è proprio calcolato compensando l’effetto della dimensione del campione"
  },
  {
    "objectID": "slides/ADAS/1-statistica.html#e-per-la-potenza",
    "href": "slides/ADAS/1-statistica.html#e-per-la-potenza",
    "title": "Statistica Descrittiva e Inferenziale",
    "section": "E per la potenza?",
    "text": "E per la potenza?\n\n\nConsideriamo i test in cui l’ipotesi nulla sia\n\\[\n\\begin{eqnarray}\nH_0:~&\\mu = \\mu_0 + \\sigma \\\\\nH_1:~&\\mu \\neq \\mu_0 + \\sigma\n\\end{eqnarray}\n\\]\ncioè una differenza di un \\(\\sigma\\) dal valore atteso della popolazione\nAllora in gran parte dei casi avremo un \\(p\\)-value piccolo, che ci spinge a rigettare \\(H_0\\) (Caso A)\nSe però passiamo da campioni da 15 elementi a campioni da 30 elementi (Caso B) la potenza del test aumenta sensibilmente, a pari condizioni\n\n\nCaso ACaso B"
  },
  {
    "objectID": "rcpp.html",
    "href": "rcpp.html",
    "title": "Rcpp examples",
    "section": "",
    "text": "Thanks to the Rcpp library it is particularly easy to interface R with C/C++ code. This is called wrapping, i.e. making a C++ function available to R as if it were an R function. This is particularly useful when performance is an issue, as C++ code is generally faster than R code.\nThe Rcpp library is designed to make it easy to interface with C++ functions and classes. Being a valid subset of C++, C can be used as well, although if you use external libraries compiled in C, you ought to remember to declare all functions as extern \"C\" in the C headers.\n\n\nI am linking here useful documentation and resources for Rcpp:\n\nRcpp for everyone\nRcpp in Hadley Wickham’s Advanced R\nRcpp website\nRcpp Gallery\nRcpp cheat sheet\nRcppEigen intro"
  },
  {
    "objectID": "rcpp.html#resources",
    "href": "rcpp.html#resources",
    "title": "Rcpp examples",
    "section": "",
    "text": "I am linking here useful documentation and resources for Rcpp:\n\nRcpp for everyone\nRcpp in Hadley Wickham’s Advanced R\nRcpp website\nRcpp Gallery\nRcpp cheat sheet\nRcppEigen intro"
  },
  {
    "objectID": "rcpp.html#hello-world",
    "href": "rcpp.html#hello-world",
    "title": "Rcpp examples",
    "section": "Hello, World!",
    "text": "Hello, World!\nC++ code can be embedded in R code as strings or read from source files:\n\ncode &lt;- r\"(\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\nvoid hello_world() {\n  Rcout &lt;&lt; \"Hello, World!\" &lt;&lt; std::endl;\n}\n)\"\n\nsourceCpp(code=code)\nhello_world()\n\nHello, World!\n\n\nNote the special comment // [[Rcpp::export]] that tells the sourceCpp function to make the function available to R. Without that comment, the function is compiled but not exposed to R."
  },
  {
    "objectID": "rcpp.html#deal-with-vectors",
    "href": "rcpp.html#deal-with-vectors",
    "title": "Rcpp examples",
    "section": "Deal with vectors",
    "text": "Deal with vectors\nVectors, lists and matrices can be passed to and returned from C++ functions:\n\ncode &lt;- r\"(\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\nNumericVector add_one(NumericVector x) {\n  return x + 1;\n}\n\n// [[Rcpp::export]]\nNumericVector scale(NumericVector x, double factor) {\n  return x * factor;\n}\n\n// [[Rcpp::export]]\nNumericVector add(NumericVector x, NumericVector y) {\n  return x + y;\n}\n\n// [[Rcpp::export]]\nNumericVector operate(NumericVector x, NumericVector y) {\n  if (x.size() != y.size()) {\n    stop(\"Vectors must have the same length\");\n  }\n  NumericVector res(x.size());\n  for (int i = 0, j = x.size()-1; i &lt; x.size(); i++, j--) {\n    res[i] = x[i] + y[j];\n  }\n  return res;\n}\n\n// [[Rcpp::export]]\nNumericMatrix outer_product(NumericVector x, NumericVector y) {\n  NumericMatrix res(x.size(), y.size());\n  for (int i = 0; i &lt; x.size(); i++) {\n    for (int j = 0; j &lt; y.size(); j++) {\n      res(i, j) = x[i] * y[j];\n    }\n  }\n  return res;\n}\n\n// [[Rcpp::export]]\nDataFrame data_frame(NumericVector x, NumericVector y) {\n  if (x.size() != y.size()) {\n    stop(\"Vectors must have the same length\");\n  }\n  return DataFrame::create(\n    _[\"x\"] = x, \n    _[\"y\"] = y\n  );\n}\n\n)\"\n\nsourceCpp(code=code)\nadd_one(1:10)\n\n [1]  2  3  4  5  6  7  8  9 10 11\n\nscale(1:10, 2)\n\n [1]  2  4  6  8 10 12 14 16 18 20\n\nadd(1:10, 11:20)\n\n [1] 12 14 16 18 20 22 24 26 28 30\n\noperate(1:10, 11:20)\n\n [1] 21 21 21 21 21 21 21 21 21 21\n\nouter_product(1:3, 1:4)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    2    4    6    8\n[3,]    3    6    9   12\n\ndata_frame(1:3, (1:3)^2)\n\n  x y\n1 1 1\n2 2 4\n3 3 9\n\n\nLook in Rcpp for everyone for the documentation of the Rcpp data classes and their methods."
  },
  {
    "objectID": "rcpp.html#functionals",
    "href": "rcpp.html#functionals",
    "title": "Rcpp examples",
    "section": "Functionals",
    "text": "Functionals\nR functions can be passed to and executed by C++ code, enabling functional programming:\n\ncode &lt;- r\"(\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\nNumericVector apply(NumericVector x, Function f) {\n  NumericVector res(x.size());\n  for (int i = 0; i &lt; x.size(); i++) {\n    res[i] = as&lt;double&gt;(f(x[i]));\n  }\n  return res;\n}\n)\"\n\nsourceCpp(code=code)\napply(1:10, \\(x){ x + 1 })\n\n [1]  2  3  4  5  6  7  8  9 10 11\n\n\nIf the passed function takes named arguments, it can be called in C++ as f(_[\"x\"] = 10, _[\"y\"] = 20). Look in Hadley Wickham’s Advanced R for more information."
  },
  {
    "objectID": "rcpp.html#rcppeigen",
    "href": "rcpp.html#rcppeigen",
    "title": "Rcpp examples",
    "section": "RcppEigen",
    "text": "RcppEigen\nEigen (or Armadillo) can be used to perform linear algebra operations:\n\ncode &lt;- r\"(\n#include &lt;RcppEigen.h&gt;\nusing namespace Rcpp;\nusing namespace Eigen;\n\n// [[Rcpp::depends(RcppEigen)]]\n// [[Rcpp::export]]\nNumericVector eigen_example(NumericMatrix x) {\n  Map&lt;MatrixXd&gt; m(as&lt;Map&lt;MatrixXd&gt; &gt;(x));\n  SelfAdjointEigenSolver&lt;MatrixXd&gt; es(m);\n  return wrap(es.eigenvalues());\n}\n)\"\n\nsourceCpp(code=code)\neigen_example(matrix(1:4, 2, 2))\n\n[1] 0 5"
  },
  {
    "objectID": "rcpp.html#wrapping-classes",
    "href": "rcpp.html#wrapping-classes",
    "title": "Rcpp examples",
    "section": "Wrapping Classes",
    "text": "Wrapping Classes\nC++ classes can be wrapped as modules and used in R as S4 objects. In this case, there is no need to mark the functions with // [[Rcpp::export]], you rather define a class mapping with the RCPP_MODULE macro:\n\ncode &lt;- r\"(\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\nclass Counter {\npublic:\n  Counter() : count(0) {}\n  void increment() { count++; }\n  int get() { return count; }\nprivate:\n  int count;\n};\n\nRCPP_MODULE(counter_module) {\n  class_&lt;Counter&gt;(\"Counter\")\n  .constructor()\n  .method(\"increment\", &Counter::increment)\n  .method(\"get\", &Counter::get);\n}\n\n)\"\n\nsourceCpp(code=code)\ncounter &lt;- new(Counter)\ncounter$increment()\ncounter$increment()\ncounter$get()\n\n[1] 2"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Example Data",
    "section": "",
    "text": "During the courses, example data files are used to illustrate different concepts. These files are available here for download or for direct usage."
  },
  {
    "objectID": "data.html#loading-the-data-files",
    "href": "data.html#loading-the-data-files",
    "title": "Example Data",
    "section": "Loading the data files",
    "text": "Loading the data files\nEach data file can be separately loaded from the URLs reported down below. Nonetheless, you can also exploit the ability of R read* functions to open a file directly from its URL. To do so, it can be useful to define a utility function like the following:\n\nexample_url &lt;- function(example) {\n  url = paste0(\"https://paolobosetti.quarto.pub/data/\", example)\n  return(url)\n}\n\nLoad this function in your script or put it into a utility library. Then, just pipe it with the proper file-reading function, e.g.:\n\nexample_url(\"timeseries.csv\") %&gt;% read_csv()\n\nor\n\nexample_url(\"3dprint.dat\") %&gt;% read_table(comment=\"#\")"
  },
  {
    "objectID": "data.html#list-of-csv-files",
    "href": "data.html#list-of-csv-files",
    "title": "Example Data",
    "section": "List of CSV files",
    "text": "List of CSV files\n\n\n\nduplicate.csv\nkfold.csv\nregression.csv\nsurface_scan.csv\ntaratura.csv\ntemperature-anomaly.csv\ntimeseries.csv\ntrain.csv"
  },
  {
    "objectID": "data.html#list-of-dat-files",
    "href": "data.html#list-of-dat-files",
    "title": "Example Data",
    "section": "List of DAT files",
    "text": "List of DAT files\n\n\n\n3dprint.dat\nanova.dat\nbattery.dat\nbearing.dat\nbesterfield_1.dat\nbesterfield_2.dat\nbesterfield_3.dat\nbesterfield_4.dat\nbesterfield_5.dat\ncotton.dat\ncutting.dat\ndiet.dat\ndrill.dat\nhardness.dat\nhardness2.dat\nmortar.dat\np_data.dat\nrf_generator.dat\nsoap_bottles.dat\ntwosample.dat"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "These pages present supporting material for lessons given by Paolo Bosetti, Associate Professor at Department of Industrial Engineering, University of Trento.\n\nContact: paolo [dot] bosetti [at] unitn [dot] it\nGitHub: https://github.com/pbosetti"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Teaching Stuff",
    "section": "",
    "text": "At the Department of Industrial Engineering of the University of Trento, I am teaching master courses (in English) and undergraduate courses (in Italian). Most of the teaching material used in these courses is available here."
  },
  {
    "objectID": "index.html#design-of-precision-systems-dps",
    "href": "index.html#design-of-precision-systems-dps",
    "title": "Teaching Stuff",
    "section": "Design of Precision Systems (DPS) 🇬🇧",
    "text": "Design of Precision Systems (DPS) 🇬🇧\nCourse for the Master Degree in Mechatronics Engineering, given in English\n\nLesson on K-Fold Cross Validation (also as a document)"
  },
  {
    "objectID": "index.html#design-of-experiments-and-statistical-analysis-of-experimental-data-desaned",
    "href": "index.html#design-of-experiments-and-statistical-analysis-of-experimental-data-desaned",
    "title": "Teaching Stuff",
    "section": "Design of Experiments and Statistical Analysis of Experimental Data (DESAnED) 🇬🇧",
    "text": "Design of Experiments and Statistical Analysis of Experimental Data (DESAnED) 🇬🇧\nPhD Seminar, given in English\n\nTable of contents with all slides"
  },
  {
    "objectID": "index.html#analisi-dei-dati-e-statistica-adas",
    "href": "index.html#analisi-dei-dati-e-statistica-adas",
    "title": "Teaching Stuff",
    "section": "Analisi dei Dati e Statistica (ADAS) 🇮🇹",
    "text": "Analisi dei Dati e Statistica (ADAS) 🇮🇹\nCorso per la Laurea triennale in Ingegneria Industriale, tenuto in italiano\n\nIndice con tutte le slide del corso\nEsperimento virtuale: taratura di una bilancia a due piatti\n\n\n\n\n\n\n\nContinuous updates!\n\n\n\nPlease note that the material is constantly updated. If you are interested in a specific topic, please check the last update (i.e., refresh your browser!)"
  },
  {
    "objectID": "kfold.html",
    "href": "kfold.html",
    "title": "K-Fold Cross Validation",
    "section": "",
    "text": "Loading the data\nWe load the data from a web address and make a quick plot:\n\ndata &lt;- read_csv(\"https://paolobosetti.quarto.pub/data/kfold.csv\", \n                 show_col_types = FALSE)\ndata %&gt;% ggplot(aes(x=x, y=y)) + geom_point()\n\n\n\n\n\n\n\n\n\n\nLinear model regression\nThe regression of a linear model is performed with the lm() function. It takes two arguments:\n\na formula, i.e. a description of the regression model\na data table, containing the data set to use for regression. The columns of the data set must have the same names used for the predictors\n\nThe formula is expressed in the formula notation, which is a map from an analytical regression model, as \\(y_i = a + bx_i + cx_i^2 + \\varepsilon_i\\) to a formula object as y~x + I(x^2)\nTo build a formula from a model you typically:\n\ndrop the grand average \\(a\\) and the residuals \\(\\varepsilon_i\\)\nwhen you need the power of a term (or any mathematical function applied to a term like a logarithm), you have to protect it with the identity function I()\nif you have more than one predictor, you can combine them as y~x1 + x2, which corresponds to \\(y_i = a + bx_{1,i} + cx_{2,1} + \\varepsilon_i\\) or as y~x1 + x2 + x1:x2, which corresponds to \\(y_i = a + bx_{1,i} + cx_{2,1} + dx_{1,i}x_{2,i} +\\varepsilon_i\\)\nthe notation y~x1 + x2 + x1:x2 can be abbreviated as y~x1*x2\nto remove from the model the grand average (called intercept), subtract 1: \\(y_i = bx_i + cx_i^2 + \\varepsilon_i\\) becomes y~x + I(x^2) - 1\n\nSo let’s build a linear model of degree 2:\n\ndata.lm &lt;-lm(y~x - 1, data=data)\n\ndata %&gt;% \n  add_predictions(data.lm) %&gt;%\n  ggplot(aes(x=x)) +\n  geom_point(aes(y=y)) +\n  geom_line(aes(y=pred))\n\n\n\n\n\n\n\ndata %&gt;% \n  add_residuals(data.lm) %&gt;% \n  ggplot(aes(x=x, y=resid)) +\n  geom_point()\n\n\n\n\n\n\n\n\nWe note that the residuals show a rather strong pattern, meaning that the linear relationship is underfitting the data, and thus we need to increase the degree of the fitting polynomial. But how much so?\n\n\nMultiple regressions\nThe degree of the fitting polynomial is a hyper-parameter. In fact, regression parameters are the coeffiients of the polynomial, to be calculated typically by minimizing the root mean square of the residuals. But the degree of the polynomial is a parameter that defines the number of regression parameters, and that is why it is named a hyper-parameter. Identifying the best hyper-parameter(s) is the aim of validation and cross-validation strategies.\nIn our case we want to compare polynomial fits up to degree 12. We use modelr::fit_with() to automate the building of many models together. The function needs two arguments:\n\nthe modeling function, typically lm()\nthe results of a call to formulas(), which in turn takes a list of formulas to be used. This list needs to have only right hand side formulas, being the first the response variable, the others are the model part combining the predictors\n\nWe first build a list of arguments:\n\ndeg &lt;- 2:12\nargs &lt;- list(\n  ~y, ~x,\n  map(deg, ~as.formula(paste0(\"~poly(x,\", ., \")\")))\n) %&gt;% \n  unlist()\n\nNote the usage of unlist() at the end: the previous command returns a nested list (a list of lists), and unlist() flattens it into a simple plain list of formulas.\nNow the formulas() function wants n parameters, each being a formula, while we have a list of formulas. We can solve this problem by using do.call() function, which calls a given function passing each element of a list as a separate argument:\n\nfits &lt;- data %&gt;% fit_with(\n  lm, \n  .formulas=do.call(formulas, args)\n)\n\nQuality of a regression can be verified with different metrics:\n\n\\(R^2=1-\\frac{\\sum (x_i - \\hat x_i)^2}{\\sum (x_i - \\bar x_i)^2}\\)\n\\(\\mathrm{MSE}=\\frac{\\sum(x_i - \\hat x_i)^2}{N}\\)\n\\(\\mathrm{RMSE}=\\sqrt{\\frac{\\sum(x_i - \\hat x_i)^2}{N}}\\)\n\\(\\mathrm{MAE}=\\frac{\\sum |x_i - \\hat x_i|}{N}\\)\n\\(\\mathrm{MAPE}=\\frac{1}{N}\\sum \\left|\\frac{x_i - \\hat x_i}{x_i}\\right|\\)\n\nTypically, the root means square of error (RMSE) and the mean absolute error (MAE) are the most commonly used metrics.\nLet’s see how the RMSE and the \\(R^2\\) metrics change when the polynomial degree increases. To do that we build a table with three columns:\n\nthe degree of the polynomial\nthe \\(R^2\\) value\nthe RMSE value\n\nWe extract these data from the list of linear models above created, fits. For each fitted linear model (an entry in fits), the \\(R^2\\) and RMSE can be extracted with the functions rsquare() and rmse(), respectively.\nWe use map_dbl() to map these functions over the list of polynomial degrees. The resulting table is then used to make a plot:\n\ntibble(\n  degree=c(1,deg), # deg starts from 2!\n  rsquare=fits %&gt;% map_dbl(~rsquare(., data)),\n  rmse=fits %&gt;% map_dbl(~rmse(., data))\n) %&gt;% \n  ggplot(aes(x=degree)) +\n  geom_line(aes(y=rmse), color=\"blue\") +\n  geom_line(aes(y=rsquare*4), color=\"red\") +\n  scale_y_continuous(sec.axis = sec_axis(\n    \\(x) scales::rescale(x, from=c(0,4), to=c(0,1)),\n    breaks=seq(0, 1, 0.1),\n    name=\"R squared\"))\n\n\n\n\n\n\n\n\nThe \\(R^2\\) increases pretty quickly and saturates after degree 3. The RMSE decreases sharply and monothonically. It’s hard to figure out the point where overfitting starts.\n\n\nK-fold cross-validation\nTo solve the problem we use K-fold cross validation. It is a regression strategy where we split the dataset into \\(k\\) subsets, or folds, with roughly the same amount of observations. Then:\n\nwe train the model over all the folds together except the first fold, and then we validate the model on the first model, i.e. we calculate one or more metrics on the validation data\nwe repeat the previous step setting aside each fold, one at a time, and using it for validation, while the remaining folds are used for training\neach fold is used exactly once for validation, exactly \\(k-1\\) times for training\nwe calculate the overall metrics, by calculating the average of the \\(k\\) metrics evaluated for each validation step, or — equivalently — by appliying the above reported equations to the whole set of validation values\n\nIn R, we use the caret library to simplify this process. The caret::train() function performs the folding for a given model: it takes as arguments the model formula, the regression function (in our case lm()), the dataset, and a list of parameters that can be created with the supporting trainControl() function.\nThe trainControl() function is used to define the details on the cross validation strategy to use. In our case we use the repeated K-fold cross validation, named \"repeatedcv\", which repeates a K-fold a given number of times.\nIn fact, the folds are defined by randomly sampling the initial dataset, so that the resulting RMSE (or any other metric) is also a random variable. Repeating the K-fold 100 times makes the whole process more robust:\n\nctrl &lt;- trainControl(method = \"repeatedcv\", number=5, repeats=100)\nmodel &lt;- train(y~poly(x,8), data=data, method=\"lm\", trControl=ctrl)\n\nmodel\n\nLinear Regression \n\n25 samples\n 1 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold, repeated 100 times) \nSummary of sample sizes: 20, 20, 21, 20, 19, 20, ... \nResampling results:\n\n  RMSE     Rsquared   MAE    \n  10.3213  0.9241026  6.25129\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\nThe model object contains a field named model$results that is a table with all the available performance metrics:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nintercept\nRMSE\nRsquared\nMAE\nRMSESD\nRsquaredSD\nMAESD\n\n\n\n\nTRUE\n10.3213\n0.9241026\n6.25129\n23.37541\n0.1183862\n12.14192\n\n\n\n\n\nNow we want to repeat the K-fold validation over the list of formulas corresponding to the set of polynomials with degrees from 1 to 12. We use again the map() function:\n\nfit_quality &lt;- tibble(\n  degree=c(1,deg),\n  results=map(degree, function(n) {\n    fm &lt;- paste0(\"y~poly(x,\", n, \")\") %&gt;% as.formula()\n    train(fm, data=data, method=\"lm\", trControl=ctrl)$results\n    })\n) %&gt;% \n  unnest(cols=results)\n\nNote the unnest() function at the end: the model field $results is actualy a table, so without that function in fit_quality we would get a column results that contains a list of tables. The unnest() function flattens this list of tables in place.\nNow we can finally make a plot of the metrics as a function of the polynomial degree:\n\nfit_quality %&gt;% \n  select(-intercept, -starts_with(\"Rsquared\")) %&gt;% \n  pivot_longer(-degree, names_to = \"metric\") %&gt;% \n  ggplot(aes(x=degree, y=value, group=metric, color=metric)) + \n  geom_line() +\n  geom_point() +\n  scale_y_log10() +\n  scale_x_continuous(breaks=c(1,deg))\n\n\n\n\n\n\n\n\nWe observe that the minima of any metric happens at degree 3. This means that below that value we have underfitting, above we have overfitting (i.e. the model is loosing generality).\n\n\nRegression\nSo we can finally accept the model \\(y_i=a + bx_i + cx_i^2 + dx_i^3 + \\varepsilon_i\\) (a degree 3 polynomial in \\(x_i\\)):\n\ndata.lm &lt;- lm(y~poly(x, 3), data=data)\n\ndata %&gt;% \n  add_predictions(data.lm) %&gt;% \n  ggplot(aes(x=x, y=y)) +\n  geom_point() +\n  geom_line(aes(y=pred))\n\n\n\n\n\n\n\ndata %&gt;% \n  add_residuals(data.lm) %&gt;% \n  ggplot(aes(x=x, y=resid)) +\n  geom_point()\n\n\n\n\n\n\n\n\nWe confirm that the residuals are free from patterns. We can also plot a confidence interval of the same regression by using the geom_smooth() layer in a ggplot:\n\ndata %&gt;% \n  ggplot(aes(x=x, y=y)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", formula=y~poly(x, 3))"
  },
  {
    "objectID": "slides/ADAS/0-intro.html#analisi-statistica-dei-dati",
    "href": "slides/ADAS/0-intro.html#analisi-statistica-dei-dati",
    "title": "Introduzione al Corso",
    "section": "Analisi statistica dei dati",
    "text": "Analisi statistica dei dati\nQuesto corso fornisce le competenze essenziali perché un ingegnere sia in grado di analizzare e visualizzare correttamente i dati risultanti da esperimenti scientifici (cioè condotti in laboratorio) o industriali (cioè condotti sul campo, ottenendo dati direttamente da un sistema di produzione)"
  },
  {
    "objectID": "slides/ADAS/0-intro.html#analisi-statistica-dei-dati-1",
    "href": "slides/ADAS/0-intro.html#analisi-statistica-dei-dati-1",
    "title": "Introduzione al Corso",
    "section": "Analisi statistica dei dati",
    "text": "Analisi statistica dei dati\nOgni esperimento è basato su un’operazione di misura, la quale produce dati quantitativi che contengono un contributo deterministico e un contributo aleatorio, cioè casuale\n\nIl primo è proprio del fenomeno in analisi\nil secondo deriva dal contesto: condizioni operative, sistema di misura, materie prime, condizioni ambientali, operatore, ecc.\n\nUn bravo sperimentatore ha le competenze che gli consentono di isolare il primo contributo dal secondo, ottenendo quindi informazioni più affidabili e ripetibili\nInoltre, un bravo sperimentatore è in grado di comunicare i risultati in modo chiaro e comprensibile e presentandoli in maniera efficace (grafici!)"
  },
  {
    "objectID": "slides/ADAS/0-intro.html#analisi-statistica-dei-dati-2",
    "href": "slides/ADAS/0-intro.html#analisi-statistica-dei-dati-2",
    "title": "Introduzione al Corso",
    "section": "Analisi statistica dei dati",
    "text": "Analisi statistica dei dati\nDato che l’analisi dei dati richiede capacità di calcolo, il corso legherà strettamente sia gli aspetti teorici che quelli implementativi, integrando cioè la teoria con la pratica di eseguire al calcolatore gli stessi concetti\nGli esercizi sono quindi parte integrante del corso\nImparare l’uso di un linguaggio e software di analisi dati (R + RStudio) è parte integrante del corso"
  },
  {
    "objectID": "slides/ADAS/0-intro.html#modalità-desame",
    "href": "slides/ADAS/0-intro.html#modalità-desame",
    "title": "Introduzione al Corso",
    "section": "Modalità d’esame",
    "text": "Modalità d’esame\nL’esame è in due parti, tipicamente nello stesso giorno\n\nprova pratica (mattina): sulla piattaforma Moodle esamionline vengono proposti una serie di esercizi in R\nOgni esercizio ha un punteggio prefissato\nGli esercizi vengono corretti automaticamente con revisione manuale per quelli che risultano errati. Il voto massimo è 26/30\nprova orale (pomeriggio): per chi desidera migliorare il voto (fino al massimo di 30/30L). L’orale può anche peggiorare il voto della prova pratica\n\n\nNon è possibile sostenere l’orale in una sessione differente dalla prova pratica"
  },
  {
    "objectID": "slides/ADAS/0-intro.html#modalità-desame-a.a.-202425",
    "href": "slides/ADAS/0-intro.html#modalità-desame-a.a.-202425",
    "title": "Introduzione al Corso",
    "section": "Modalità d’esame (A.A. 2024–25)",
    "text": "Modalità d’esame (A.A. 2024–25)\nATTENZIONE: questo corso è il secondo (o il primo) di due moduli: l’esame è quindi solo una prova parziale e non viene verbalizzato\nIl voto finale verrà verbalizzato solo dopo aver superato entrambi i moduli, inclusa cioè:\n\nstudenti del secondo anno: la prova parziale del primo modulo (prof. Bertolazzi)\nstudenti del terzo anno: la prova parziale del secondo modulo (prof. De Cecco).\n\n\nRicordarsi di iscriversi anche all’appello di registrazione!"
  },
  {
    "objectID": "slides/ADAS/0-intro.html#argomenti-del-corso",
    "href": "slides/ADAS/0-intro.html#argomenti-del-corso",
    "title": "Introduzione al Corso",
    "section": "Argomenti del corso",
    "text": "Argomenti del corso\n\n\nStatistica descrittiva e inferenziale: descrivere il comportamento di variabili aleatorie e effettuare previsioni probabilistiche\nIntroduzione a R e RStudio\nRegressione: adattare un modello analitico ad una serie di dati sperimentali\nTecniche di Bootstrap: analisi dell’incertezza\nTaratura di sistemi di misura e stima dell’incertezza\nDesign of Experiments: progettazione efficiente delle campagne sperimentali\nSerie temporali: analisi e previsione di variabili aleatorie tempo-dipendenti"
  },
  {
    "objectID": "slides/ADAS/0-intro.html#materiale-didattico",
    "href": "slides/ADAS/0-intro.html#materiale-didattico",
    "title": "Introduzione al Corso",
    "section": "Materiale didattico",
    "text": "Materiale didattico\n\nMoodle\nTesto di riferimento:\nCodice sviluppato in classe: Il codice R sarà gradualmente pubblicato e aggiornato su GitHub: https://github.com/pbosetti/ADAS-24"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#link-utili",
    "href": "slides/ADAS/2-R-intro.html#link-utili",
    "title": "Introduzione al Linguaggio R",
    "section": "Link utili",
    "text": "Link utili\n\nGNU-R: https://cran.mirror.garr.it/CRAN/\nRStudio: https://posit.co/downloads/\nCheat sheet: https://posit.co/resources/cheatsheets/\nTidyverse: https://tidyverse.org\nMateriale corso: https://github.com/pbosetti/ADAS-24"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#ambiente-rstudio",
    "href": "slides/ADAS/2-R-intro.html#ambiente-rstudio",
    "title": "Introduzione al Linguaggio R",
    "section": "Ambiente RStudio",
    "text": "Ambiente RStudio\n\nInstallazione: prima R, poi RStudio\nSolo su windows, installare anche Rtools\nRStudio lavora su cartelle o (meglio) progetti (.Rproj)\nUn progetto contiene anche impostazioni specifiche e comuni ai file nella cartella\nUna sessione di RStudio può operare su un unico progetto\nSi possono aprire più sessioni contemporaneamente\nRStudio è un ambiente molto potente e complesso, adatto anche alla compilazione di report tecnici, articoli, libri e presentazioni (come questa)\n\n\n\nNota: Queste slide contengono una versione ridotta e integrata di R, utile per provare in diretta il codice qui illustrato: cliccare sull’icona del prompt in basso a sinistra, o premere il tasto §."
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#il-linguaggio-r",
    "href": "slides/ADAS/2-R-intro.html#il-linguaggio-r",
    "title": "Introduzione al Linguaggio R",
    "section": "Il linguaggio R",
    "text": "Il linguaggio R\n\nR è un linguaggio ad alto livello, declarativo, interpretato, a sintassi C-like\nR è sia un linguaggio, sia un interprete\nR è un dynamically typed language\nR è utilizzato sia in modalità script che in modalità interattiva\nR è nato come versione GNU open source di S, un linguaggio proprietario per analisi statistiche\nRStudio è una IDE proprietaria (ma free) per R"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#assegnazioni",
    "href": "slides/ADAS/2-R-intro.html#assegnazioni",
    "title": "Introduzione al Linguaggio R",
    "section": "Assegnazioni",
    "text": "Assegnazioni\nOgni linguaggio usa delle variabili per memorizzare valori ed oggetti mediante un’operazione di assegnazione:\n\na &lt;- 1\n# ma anche\nb = 2\n# tuttavia si preferisce la notazione a freccia, \n# perché funziona anche così:\n3 -&gt; c\n# per visualizzare il valore di una variabile:\nc\n\n[1] 3\n\n# in un colpo solo, assegnazione e visualizzazione:\n(d &lt;- \"stringa\")\n\n[1] \"stringa\"\n\n\nL’esecuzione di un comando fornisce direttamente un risultato:\n\n12*12\n\n[1] 144\n\n\nSi noti il testo [1] all’inizio della riga di output: sarà chiaro più avanti"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#tipi-o-classi-native",
    "href": "slides/ADAS/2-R-intro.html#tipi-o-classi-native",
    "title": "Introduzione al Linguaggio R",
    "section": "Tipi, o classi native",
    "text": "Tipi, o classi native\n\nR ha 6(+1) tipi o classi native\n\ncharacter: \"a\", \"string\", 'my text'\nnumeric: 1, 3.1416\ninteger: 1L\nlogical: TRUE, FALSE (oppure T e F)\ncomplex: 1+4i\nfunction: una funzione\n(raw: sequenza di bit)\n\nOgni istanza è intrinsecamente un vettore\nUno scalare è semplicemente un vettore di lunghezza 1"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#valori-speciali",
    "href": "slides/ADAS/2-R-intro.html#valori-speciali",
    "title": "Introduzione al Linguaggio R",
    "section": "Valori speciali",
    "text": "Valori speciali\n\nSono definiti i seguenti valori speciali:\n\nNA: valore mancante\nNULL: niente\nInf: Infinito\nNaN: Not a Number (esempio 0/0)"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#coercizione",
    "href": "slides/ADAS/2-R-intro.html#coercizione",
    "title": "Introduzione al Linguaggio R",
    "section": "Coercizione",
    "text": "Coercizione\n\nQuando si mescolano tipi differenti, ad es. in un vettore, R li trasforma in un tipo comune:\n\n\nc(1L, 7, \"2\")\n\n[1] \"1\" \"7\" \"2\"\n\nc(T, 0)\n\n[1] 1 0\n\nas.numeric(c(\"a\", \"1\"))\n\n[1] NA  1\n\nas.character(c(1, 1.7))\n\n[1] \"1\"   \"1.7\""
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#vettori",
    "href": "slides/ADAS/2-R-intro.html#vettori",
    "title": "Introduzione al Linguaggio R",
    "section": "Vettori",
    "text": "Vettori\n# Si costruiscono con l'operatore/funzione c():\nv1 &lt;- c(10, 2, 7.5, 3)\n# oppure con una sequenza:\nv2 &lt;- 1:10\n# anche con passo specificato:\nv3 &lt;- seq(1, 10, 0.5)\n# Le funzioni si chiamano con le parentesi tonde, \n# separando argomenti con ,\nSi noti in questo caso l’output per v3:\n\n\n [1]  1.0  1.5  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0\n[12]  6.5  7.0  7.5  8.0  8.5  9.0  9.5 10.0\n\n\nIl primo elemento della prima riga è l’elemento [1] del vettore, mentre il primo elemento della seconda riga è l’elemento [12]. In tutto, il vettore v3 ha 19 elementi"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#vettori-1",
    "href": "slides/ADAS/2-R-intro.html#vettori-1",
    "title": "Introduzione al Linguaggio R",
    "section": "Vettori",
    "text": "Vettori\nLe variabili sono nativamente dei vettori. Gli scalari sono solo vettori di dimensione 1:\n\na &lt;- 10\nlength(a)\n\n[1] 1\n\nlength(v3)\n\n[1] 19\n\n\nLe funzioni e gli operatori agiscono quindi sempre su vettori (sono vettorializzati):\n\na * 2\n\n[1] 20\n\nv3 + 2\n\n [1]  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5  7.0  7.5  8.0\n[12]  8.5  9.0  9.5 10.0 10.5 11.0 11.5 12.0"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#introspezione",
    "href": "slides/ADAS/2-R-intro.html#introspezione",
    "title": "Introduzione al Linguaggio R",
    "section": "Introspezione",
    "text": "Introspezione\n\nFunzioni utili per ispezionare gli oggetti:\n\nmode(): storage mode\nclass(): classe (alto livello, uguale a mode() per tipi base)\ntypeof(): tipo (basso livello)\nlength(): lunghezza vettore\nattributes(): metadati\nstr(): struttura di un oggetto\nsummary(): riassunto statistico"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#matrici",
    "href": "slides/ADAS/2-R-intro.html#matrici",
    "title": "Introduzione al Linguaggio R",
    "section": "Matrici",
    "text": "Matrici\n\nSi costruiscono con la funzione matrix()\n\n(m1 &lt;- matrix(1:10, 2, 5))\n\nla funzione array() costruisce matrici n-dimensionali\nUna matrice è un vettore con attributo dim:\n\nattr(m1, \"dim\")\nv &lt;- 1:4\nattr(v, \"dim\") &lt;- c(2,2) # equivale a dim(m) &lt;- c(2,2)\nv"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#fattori",
    "href": "slides/ADAS/2-R-intro.html#fattori",
    "title": "Introduzione al Linguaggio R",
    "section": "Fattori",
    "text": "Fattori\n\nUna classe aggiuntiva (non base) ma molto comune è factor\nRappresenta variabili categoriche (ordinate o non)\n\n\n(vf &lt;- factor(LETTERS[1:5], levels=LETTERS[c(2, 1, 3, 5, 4)], ordered=T))\n\n[1] A B C D E\nLevels: B &lt; A &lt; C &lt; E &lt; D\n\nclass(vf)\n\n[1] \"ordered\" \"factor\" \n\ntypeof(vf)\n\n[1] \"integer\"\n\nvf[1] &lt; vf[3]\n\n[1] TRUE"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#stringhe",
    "href": "slides/ADAS/2-R-intro.html#stringhe",
    "title": "Introduzione al Linguaggio R",
    "section": "Stringhe",
    "text": "Stringhe\nUna stringa può essere pensata come un vettore di caratteri di lunghezza maggiore di 1.\nLe funzioni di manipolazione di stringhe più comuni sono cat(), paste() e paste0(). La prima serve a stampare la stringa tale e quale:\n\ncat(\"Ciao!\")\n\nCiao!\n\n\nLe due funzioni paste() e paste0() servono a unire due o più stringhe, la prima inserendo uno spazio in mezzo, la seconda senza spazio:\n\npaste(\"Ciao,\", \"Mondo!\")\n\n[1] \"Ciao, Mondo!\"\n\npaste0(\"Ciao,\", \"Mondo!\")\n\n[1] \"Ciao,Mondo!\""
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#indicizzazione",
    "href": "slides/ADAS/2-R-intro.html#indicizzazione",
    "title": "Introduzione al Linguaggio R",
    "section": "Indicizzazione",
    "text": "Indicizzazione\n\nLa sintassi di indicizzazione di R è molto flessibile e potente\nsi usano sempre le parentesi quadre [r,c], la base è 1\nse un indice manca, significa “tutte le righe|colonne”\n\n\nv3[3]\n\n[1] 2\n\nm1[1,1]\n\n[1] 1\n\nm1[2,]\n\n[1]  2  4  6  8 10\n\nm1[,]\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    3    5    7    9\n[2,]    2    4    6    8   10"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#indicizzazione-1",
    "href": "slides/ADAS/2-R-intro.html#indicizzazione-1",
    "title": "Introduzione al Linguaggio R",
    "section": "Indicizzazione",
    "text": "Indicizzazione\n\nUn indice può essere anche un vettore di posizioni o un vettore di valori booleani\n\n\nv1[c(2,4,1)] # estrae solo gli elementi 2, 4, e 1\n\n[1]  2  3 10\n\nv2[v2 %% 2 == 0] # estrae gli elementi divisibili per 2\n\n[1]  2  4  6  8 10\n\n\nIl secondo caso funziona grazie all’operatore modulo:\n\nv2 %% 2 == 0 # operatore modulo (resto)\n\n [1] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE\n[10]  TRUE\n\n\n\nNOTA: TRUE e FALSE possono essere abbreviati in T e F"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#funzioni",
    "href": "slides/ADAS/2-R-intro.html#funzioni",
    "title": "Introduzione al Linguaggio R",
    "section": "Funzioni",
    "text": "Funzioni\n\nLe funzioni sono first class objects, cioè sono variabili come altre\npossono essere assegnate a variabili e passate a funzioni\n\n\nmy_fun &lt;- function(x) x^2\nmy_fun(1:5)\n\n[1]  1  4  9 16 25\n\nyour_fun &lt;- my_fun\nyour_fun(6)\n\n[1] 36\n\nmy_apply &lt;- function(x, f) f(x)\nmy_apply(10, my_fun)\n\n[1] 100\n\n\n\nSe la definizione richiede più righe, si usa un blocco tra {}\nOgni funzione ritorna sempre l’ultima espressione valutata\nOppure esplicitamente mediante return()"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#funzioni-freccia-replacement-functions",
    "href": "slides/ADAS/2-R-intro.html#funzioni-freccia-replacement-functions",
    "title": "Introduzione al Linguaggio R",
    "section": "Funzioni freccia (replacement functions)",
    "text": "Funzioni freccia (replacement functions)\n\nAbbiamo visto cose come dim(v) &lt;- c(2,3): come si dichiarano?\n\n\n`pwr&lt;-` &lt;- function(obj, value) obj ** value\na &lt;- 2\npwr(a) &lt;- 10\na\n\n[1] 1024\n\n\n\nL’ultimo argomento deve chiamarsi value e rappresenta il lato destro dell’assegnazione!"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#controllo-di-flusso",
    "href": "slides/ADAS/2-R-intro.html#controllo-di-flusso",
    "title": "Introduzione al Linguaggio R",
    "section": "Controllo di flusso",
    "text": "Controllo di flusso\nR supporta le tipiche istruzioni di controllo di flusso\n\nper istruzioni condizionali:\n\nif(cond) expr\nif(cond) true.expr  else  false.expr\nifelse(cond, true.expr, false.expr)\n\ne per i cicli:\n\nfor(var in seq) expr\nwhile(cond) expr\nrepeat expr\nbreak\nnext"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#esercizio",
    "href": "slides/ADAS/2-R-intro.html#esercizio",
    "title": "Introduzione al Linguaggio R",
    "section": "Esercizio",
    "text": "Esercizio\nScrivere una funzione sum_all che sommi tutti gli elementi con indice dispari di un vettore numerico. La funzione deve avere un argomento x e restituire la somma di tutti gli elementi di x, usando un ciclo for.\n\n\n\n\n\n\n\n\n\n\nL’uso del ciclo for è sconsigliato in R (perché è più lento di alcune alternative), ma è utile per esercitarsi con il controllo di flusso."
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#argomenti-delle-funzioni",
    "href": "slides/ADAS/2-R-intro.html#argomenti-delle-funzioni",
    "title": "Introduzione al Linguaggio R",
    "section": "Argomenti delle funzioni",
    "text": "Argomenti delle funzioni\n\nGli argomenti possono essere indicati per posizione o per nome\nGli argomenti nominati possono comparire in qualsiasi ordine\nGli argomenti possono avere un default, in tal caso sono opzionali\n\n\nf &lt;- function(x, y, n=10, test=F) { \n  ifelse(test, 0, x^y + n)\n}\nf(2, 10)\n\n[1] 1034\n\nf(test=F, y=10, x=2)\n\n[1] 1034\n\nf(test=T)\n\n[1] 0"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#differenza-tra---e",
    "href": "slides/ADAS/2-R-intro.html#differenza-tra---e",
    "title": "Introduzione al Linguaggio R",
    "section": "Differenza tra <- e =",
    "text": "Differenza tra &lt;- e =\n\nL’operatore = come assegnazione è valido solo al top-level\nL’operatore &lt;- è valido ovunque, anche come argomento di funzione:\n\n\nsystem.time(m &lt;- mean(1:1E6))\n\n   user  system elapsed \n  0.001   0.000   0.002 \n\nm\n\n[1] 500000.5"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#dataframe",
    "href": "slides/ADAS/2-R-intro.html#dataframe",
    "title": "Introduzione al Linguaggio R",
    "section": "Dataframe",
    "text": "Dataframe\n\nIn R più che matrici si usano dataframe\nSi tratta di tabelle organizzate per colonne, internamente omogenee ma potenzialmente di tipi differenti\n\n\ndf &lt;- data.frame(A=1:10, B=letters[1:10])\nhead(df)\n\n  A B\n1 1 a\n2 2 b\n3 3 c\n4 4 d\n5 5 e\n6 6 f"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#dataframe-1",
    "href": "slides/ADAS/2-R-intro.html#dataframe-1",
    "title": "Introduzione al Linguaggio R",
    "section": "Dataframe",
    "text": "Dataframe\n\nUn dataframe può essere indicizzato come una matrice (due indici)\nOppure selezionando una colonna con la notazione $\n\n\ndf[2,2]\n\n[1] \"b\"\n\ndf$B[2]\n\n[1] \"b\"\n\n\nAnche in assegnazione:\n\ndf$C &lt;- LETTERS[1:10]\nhead(df, 3)\n\n  A B C\n1 1 a A\n2 2 b B\n3 3 c C"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#esercizio-1",
    "href": "slides/ADAS/2-R-intro.html#esercizio-1",
    "title": "Introduzione al Linguaggio R",
    "section": "Esercizio",
    "text": "Esercizio\nDal data frame mtcars, estrarre le righe corrispondenti ai veicoli con cilindrata (disp) maggiore di 200 e peso minore di 3.5 tonnellate (wt)"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#liste",
    "href": "slides/ADAS/2-R-intro.html#liste",
    "title": "Introduzione al Linguaggio R",
    "section": "Liste",
    "text": "Liste\nUna lista è una sequenza di coppie chiave-valore, cioè una sequenza di valori identificati da un nome, o chiave.\nA differenza dei vettori (che sono sempre omogenei) possono contenere valori eterogenei.\n\n(l &lt;- list(A=\"uno\", B=\"due\", C=1:4))\n\n$A\n[1] \"uno\"\n\n$B\n[1] \"due\"\n\n$C\n[1] 1 2 3 4\n\n\nUna lista può essere indicizzata in tre modi:\n\ncon l’operatore $: si estrae un unico elemento per nome\ncon l’operatore []: si estraggono elementi per posizione e si ottiene una lista\ncon l’operatore [[]]: si estrae un unico elemento per posizione"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#algoritmi-di-uso-comune",
    "href": "slides/ADAS/2-R-intro.html#algoritmi-di-uso-comune",
    "title": "Introduzione al Linguaggio R",
    "section": "Algoritmi di uso comune",
    "text": "Algoritmi di uso comune\n\nOrdinamento: sort, rev, order\nCampionamento: sample, expand.grid\nAggregazione: by, aggregate\nTabelle di contingenza: table"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#ordinamento-di-vettori",
    "href": "slides/ADAS/2-R-intro.html#ordinamento-di-vettori",
    "title": "Introduzione al Linguaggio R",
    "section": "Ordinamento di vettori",
    "text": "Ordinamento di vettori\nPer ordinare un vettore si usa la funzione sort:\n\nv &lt;- runif(5, 1, 10)\nsort(v)\n\n[1] 3.389578 4.349115 6.155680 9.070275 9.173870\n\nrev(sort(v))\n\n[1] 9.173870 9.070275 6.155680 4.349115 3.389578\n\nsort(v, decreasing = T)\n\n[1] 9.173870 9.070275 6.155680 4.349115 3.389578"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#ordinamento-di-dataframe",
    "href": "slides/ADAS/2-R-intro.html#ordinamento-di-dataframe",
    "title": "Introduzione al Linguaggio R",
    "section": "Ordinamento di dataframe",
    "text": "Ordinamento di dataframe\nPer riordinare un data frame si estraggono gli indici ordinati:\n\ndf &lt;- data.frame(A=1:5, B=runif(5))\ndf[order(df$B),]\n\n  A         B\n1 1 0.2016819\n5 5 0.6291140\n4 4 0.6607978\n2 2 0.8983897\n3 3 0.9446753\n\n\nLa funzione order ritorna appunto gli indici di un vettore ordinati secondo i valori:\n\norder(df$B)\n\n[1] 1 5 4 2 3\n\n\ndove il primo è l’indice del valore più piccolo di df$B e l’ultimo l’indice del più grande"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#campionamento",
    "href": "slides/ADAS/2-R-intro.html#campionamento",
    "title": "Introduzione al Linguaggio R",
    "section": "Campionamento",
    "text": "Campionamento\nCampionare un insiame di dati (un vettore) significa estrarre un sottoinsieme (detto campione) di valori in maniera casuale. Si esegue con la funzione sample:\n\nsample(1:10) # senza reinserimento\n\n [1]  2  3  1  5  7 10  6  4  9  8\n\nsample(1:10, replace = T) # con reinserimento\n\n [1]  9  5  5  9  9  5  5  2 10  9\n\n\nLa dimensione del campione può essere uguale (caso sopra) o più piccola dell’insieme iniziale:\n\nsample(1:10, size = 5)\n\n[1] 1 4 3 6 2\n\nsample(10) # generazione interi casuali senza ripetizione\n\n [1] 10  6  7  4  8  9  2  1  3  5"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#griglie",
    "href": "slides/ADAS/2-R-intro.html#griglie",
    "title": "Introduzione al Linguaggio R",
    "section": "Griglie",
    "text": "Griglie\nUna griglia è una matrice che contiene tutte le combinazioni (ordinate) tra \\(n\\) vettori di dimensioni possibilmente diverse. In R viene rappresentata come un data frame e costruita con la funzione expand.grid:\n\n(df &lt;- expand.grid(A=1:2, B=c(\"-\", \"+\"), D=c(\"a\", \"b\", \"c\")))\n\n   A B D\n1  1 - a\n2  2 - a\n3  1 + a\n4  2 + a\n5  1 - b\n6  2 - b\n7  1 + b\n8  2 + b\n9  1 - c\n10 2 - c\n11 1 + c\n12 2 + c"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#esercizio-2",
    "href": "slides/ADAS/2-R-intro.html#esercizio-2",
    "title": "Introduzione al Linguaggio R",
    "section": "Esercizio",
    "text": "Esercizio\nRiordinare il dataframe df in maniera casuale:"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#aggregazione",
    "href": "slides/ADAS/2-R-intro.html#aggregazione",
    "title": "Introduzione al Linguaggio R",
    "section": "Aggregazione",
    "text": "Aggregazione\nPer aggregazione si intende raggruppare righe aventi elementi comuni in un data frame e applicare ad ogni gruppo una data funzione. È utile ad esempio per il calcolo di sub-totali.\nIn R può essere eseguita mediante la funzione by o la funzione aggregate (cambia il tipo di output):\n\nby(df$A, INDICES = df$B, FUN=sum)\n\ndf$B: -\n[1] 9\n--------------------------------------------- \ndf$B: +\n[1] 9\n\naggregate(A~B, data = df, FUN = sum)\n\n  B A\n1 - 9\n2 + 9"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#tabelle-di-contingenza",
    "href": "slides/ADAS/2-R-intro.html#tabelle-di-contingenza",
    "title": "Introduzione al Linguaggio R",
    "section": "Tabelle di contingenza",
    "text": "Tabelle di contingenza\nUna tabella di contingenza conta le occorrenze tra una coppia di colonne in un data frame:\n\nhead(airquality, n = 3)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n\nwith(airquality, table(OzHi = Ozone &gt; 80, Month, \n                       useNA = \"ifany\"))\n\n       Month\nOzHi     5  6  7  8  9\n  FALSE 25  9 20 19 27\n  TRUE   1  0  6  7  2\n  &lt;NA&gt;   5 21  5  5  1\n\n\nNOTA: with() serve per risparmiarsi di scrivere airquality$Ozone e airquality$Month"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#tabelle-di-contingenza-1",
    "href": "slides/ADAS/2-R-intro.html#tabelle-di-contingenza-1",
    "title": "Introduzione al Linguaggio R",
    "section": "Tabelle di contingenza",
    "text": "Tabelle di contingenza\n\nÈ anche utile tapply(), che opera su una tabella analogamente alle funzioni di aggregazione:\n\n\nround(with(airquality, \n           tapply(Ozone, Month, mean, na.rm=T)), 1)\n\n   5    6    7    8    9 \n23.6 29.4 59.1 60.0 31.4 \n\n\nCon aggregate() si farebbe:\n\naggregate(Ozone~Month, data=airquality, FUN=mean, ra.rm=T)\n\n  Month    Ozone\n1     5 23.61538\n2     6 29.44444\n3     7 59.11538\n4     8 59.96154\n5     9 31.44828"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#inputoutput-su-file",
    "href": "slides/ADAS/2-R-intro.html#inputoutput-su-file",
    "title": "Introduzione al Linguaggio R",
    "section": "Input/output su file",
    "text": "Input/output su file\nDato che la statistica si occupa generalmente di grandi quantità di dati è fondamentale poter importare ed esportare dati in formati generici.\nGeneralmente i dati sono presentati in forma tabulare (per righe e colonne)\nI formati più semplici e comuni sono:\n\nFlat File: un file di testo ASCII contenente valori in riga e colonna; le colonne possono essere separate\n\na lunghezza fissa\nmediante caratteri separatori\n\nCSV (Comma-Separated Values): una versione speciale di FF in cui i campi colonna sono separati da virgole"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#input-da-file",
    "href": "slides/ADAS/2-R-intro.html#input-da-file",
    "title": "Introduzione al Linguaggio R",
    "section": "Input da file",
    "text": "Input da file\nUn Flat File con campi separati da spazi può avere questo aspetto:\n# Dati raccolti il 10/8/2023\nx y z\n1.2 3.7 2.7\n2.1 2.5 3.9\n3.8 2.2 6.8\nUn simile file può essere importato come data frame in questo modo:\ndf &lt;- read.table(\"data_file.txt\", header=T, sep=\" \", comment.char=\"#\")\nLa funzione read.table() dispone di numerose opzioni che consentono di gestire tutti i possibili casi in cui file contenga campi separati da caratteri specifici (spazi od altro)"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#input-da-file-1",
    "href": "slides/ADAS/2-R-intro.html#input-da-file-1",
    "title": "Introduzione al Linguaggio R",
    "section": "Input da file",
    "text": "Input da file\nUn Flat File con campi a larghezza fissa può avere questo aspetto:\n# Dati raccolti il 10/8/2023\nx     y     z\n1.2   3.7   2.7\n2.1   2.5   3.9\n3.8   2.2   6.8\nUn simile file può essere importato come data frame in questo modo:\ndf &lt;- read.fwf(\"data_file.txt\", widths=5, header=T, skip=1)\nIl parametro skip=1 richiede di saltare la prima linea (commento)"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#input-da-file-csv",
    "href": "slides/ADAS/2-R-intro.html#input-da-file-csv",
    "title": "Introduzione al Linguaggio R",
    "section": "Input da file CSV",
    "text": "Input da file CSV\nI file CSV sono FF speciali in cui il separatore di campo è la virgola. In questi casi si usa la funzione read.csv() che opera analogamente a read.table() ma non richiede di specificare il separatore.\nUn CSV ha questo aspetto:\n# Dati raccolti il 10/8/2023\nx,y,z\n1.2,3.7,2.7\n2.1,2.5,3.9\n3.8,2.2,6.8\nSoftware che usano lingue latine (Italiano, Spagnolo, Portoghese e Francese) adottano la virgola come separatore decimale. Di conseguenza quando questi software (ad. es. MS Excel) generano dei CSV usano il punto e virgola come separatore di campo.\nIn questo caso da R è necessario utilizzare la funzione read.csv2(), che assume la virgola come separatore decimale e il punto e virgola come separatore di campo."
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#output-su-file",
    "href": "slides/ADAS/2-R-intro.html#output-su-file",
    "title": "Introduzione al Linguaggio R",
    "section": "Output su file",
    "text": "Output su file\nL’operazione opposta all’importazione di un file in un data frame è l’esportazione di un data frame su un file.\nQuesta operazione viene eseguita con le funzioni opposte alle precedenti:\n\nwrite.table()\nwrite.fwf()\nwrite.csv() e write.csv2()\n\nTutte queste funzioni hanno due argomenti obbligatori: il data frame da salvare e il file di destinazione:\nwrite.csv(df, \"data.csv\")\nAltri argomenti opzionali servono per personalizzare il risultato."
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#tidyverse",
    "href": "slides/ADAS/2-R-intro.html#tidyverse",
    "title": "Introduzione al Linguaggio R",
    "section": "Tidyverse",
    "text": "Tidyverse\nAssieme a RStudio è emersa una new wave di librerie R che modificano radicalmente l’approccio. Vanno sotto il nome collettivo di tidyverse\n\n\n\nggplot2: grafici\npurrr: programmazione funzionale\ndplyr: manipolazione dati\nstringr: manipolazione stringhe\n\n\n\ntibble: data frame migliorati\nreadr: importazione dati\ntidyr: preparazione dati\nlubridate: manipolazione date"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#tidyverse-1",
    "href": "slides/ADAS/2-R-intro.html#tidyverse-1",
    "title": "Introduzione al Linguaggio R",
    "section": "Tidyverse",
    "text": "Tidyverse\nL’approccio tidyverse ha alcune caratteristiche comuni:\n\ndati in formato tidy (un’osservazione per riga; una variabile, o osservando, per colonna)\ncomposizione di funzioni grafiche con + (ggplot(...) + geom_line()), ogni funzione è un layer\nnotazione prefissa con %&gt;% (a %&gt;% str() invece di str(a))\n\nÈ utile consultare i cheat sheet: https://posit.co/resources/cheatsheets/"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#notazione-infissa",
    "href": "slides/ADAS/2-R-intro.html#notazione-infissa",
    "title": "Introduzione al Linguaggio R",
    "section": "Notazione infissa",
    "text": "Notazione infissa\n# creo l'istogramma di un campione di 10 elementi da 100 numeri casuali\n# infissa:\nhist(sample(rnorm(100), 10))\nPoco leggibile, il primo passo dell’algoritmo è quello più interno"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#notazione-infissa-sequenziata",
    "href": "slides/ADAS/2-R-intro.html#notazione-infissa-sequenziata",
    "title": "Introduzione al Linguaggio R",
    "section": "Notazione infissa, sequenziata",
    "text": "Notazione infissa, sequenziata\n# creo l'istogramma di un campione di 10 elementi da 100 numeri casuali\n# infissa:\nhist(sample(rnorm(100), 10))\n\n# infissa sequenziata:\ns &lt;- rnorm(100)\nc &lt;- sample(s, 10)\nhist(c)\nPiù leggibile, l’algoritmo è più evidente, ma richiede la creazione di variabili intermedie"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#notazione-prefissa",
    "href": "slides/ADAS/2-R-intro.html#notazione-prefissa",
    "title": "Introduzione al Linguaggio R",
    "section": "Notazione prefissa",
    "text": "Notazione prefissa\n# creo l'istogramma di un campione di 10 elementi da 100 numeri casuali\n# infissa:\nhist(sample(rnorm(100), 10))\n\n# infissa sequenziata:\ns &lt;- rnorm(100)\nc &lt;- sample(s, 10)\nhist(c)\n\n# prefissa con pipe:\nrnorm(100) %&gt;% sample(10) %&gt;% hist()\nMolto più leggibile, l’algoritmo sequenziale è evidente, non sono necessarie variabili intermedie"
  },
  {
    "objectID": "slides/ADAS/2-R-intro.html#notazione-prefissa-1",
    "href": "slides/ADAS/2-R-intro.html#notazione-prefissa-1",
    "title": "Introduzione al Linguaggio R",
    "section": "Notazione prefissa",
    "text": "Notazione prefissa\n# creo l'istogramma di un campione di 10 elementi da 100 numeri casuali\n# infissa:\nhist(sample(rnorm(100), 10))\n\n# infissa sequenziata:\ns &lt;- rnorm(100)\nc &lt;- sample(s, 10)\nhist(c)\n\n# prefissa con pipe:\nrnorm(100) %&gt;% sample(10) %&gt;% hist()\n\n# anche su più righe:\nrnorm(100) %&gt;%\n1  sample(10) %&gt;%\n2  hist\n\n1\n\nle righe successive alla prima vanno indentate\n\n2\n\nsolo quando si usa pipe, se non ci sono argomenti le parentesi sono opzionali"
  },
  {
    "objectID": "slides/ADAS/4-bootstrap.html#tecniche-di-bootstrap",
    "href": "slides/ADAS/4-bootstrap.html#tecniche-di-bootstrap",
    "title": "Tecniche di Bootstrap",
    "section": "Tecniche di Bootstrap",
    "text": "Tecniche di Bootstrap\nScopo: analizzare una particolare statistica \\(\\hat{\\vartheta}\\) simulando nuovi campioni a partire da un campione originario per effettuare dell’inferenza sulla statistica in questione\nSi punta a inferire da \\(\\hat\\vartheta\\) il corrispondente parametro \\(\\vartheta\\) della distribuzione.\nLa simulazione può essere fatta in due modi:\n\nBootstrap non-parametrico: i campioni bootstrap vengono generati dal campione originario mediante campionamento con reinserimento\nBootstrap parametrico i campioni bootstrap vengono generati da distribuzioni aventi una forma nota (e assunta corretta) e parametri stimati dal campione originario (tipicamente media e varianza).\n\n\n\nUn parametro in statistica, è una costante da cui dipende la forma di una distribuzione: ad esempio, la distribuzione \\(\\mathcal{N}(\\mu, \\sigma^2)\\) è una distribuzione a due parametri: il valore atteso \\(\\mu\\) e la varianza \\(\\sigma^2\\)"
  },
  {
    "objectID": "slides/ADAS/4-bootstrap.html#tecniche-di-bootstrap-1",
    "href": "slides/ADAS/4-bootstrap.html#tecniche-di-bootstrap-1",
    "title": "Tecniche di Bootstrap",
    "section": "Tecniche di Bootstrap",
    "text": "Tecniche di Bootstrap\nAd esempio, supponiamo di voler stimare un determinato parametro \\(\\theta\\) con lo stimatore \\(\\hat\\theta=s(x)\\), dove \\(s(\\cdot)\\) è una qualche funzione dei dati campionari\n\\(\\theta\\) potrebbe essere il valore atteso e \\(\\hat\\theta\\) la media campionaria\nSi noti che \\(\\hat\\theta\\) è a sua volta una variabile casuale. In quanto tale, \\(\\hat\\theta\\) avrà una sua distribuzione, ed indicheremo con \\(G(\\theta)=P(\\hat\\theta&lt;\\theta)\\) la sua funzione di ripartizione (\\(\\mathrm{CDF}^-\\)).\nLa distribuzione \\(G\\) è chiamata distribuzione campionaria della statistica \\(\\hat\\theta\\)."
  },
  {
    "objectID": "slides/ADAS/4-bootstrap.html#tecniche-di-bootstrap-2",
    "href": "slides/ADAS/4-bootstrap.html#tecniche-di-bootstrap-2",
    "title": "Tecniche di Bootstrap",
    "section": "Tecniche di Bootstrap",
    "text": "Tecniche di Bootstrap\nLa forma di \\(G\\) dipende da:\n\nla distribuzione originaria \\(F\\)\nla funzione \\(s(\\cdot)\\) utilizzata per calcolare la statistica \\(\\hat\\theta\\)\nla dimensione del campione \\(n\\)\n\nIn alcuni casi—cioè per alcune (poche) combinazioni di \\(F\\), \\(s(\\cdot)\\) e \\(n\\)—la distribuzione campionaria è nota in maniera esatta. Tuttavia, nella maggior parte dei casi essa è nota solo asintoticamente, cioè quando \\(n\\rightarrow +\\infty\\). In alcuni casi, addirittura, \\(G\\) può non essere nota nemmeno asintoticamente."
  },
  {
    "objectID": "slides/ADAS/4-bootstrap.html#esempio-media-campionaria-distribuzione-normale",
    "href": "slides/ADAS/4-bootstrap.html#esempio-media-campionaria-distribuzione-normale",
    "title": "Tecniche di Bootstrap",
    "section": "Esempio: media campionaria, distribuzione normale",
    "text": "Esempio: media campionaria, distribuzione normale\n\n\nReplichiamo 10 000 volte la media di un campione casuale normale di \\(n\\) elementi\nValutiamo la distribuzione delle 10 000 stime con un istogramma\nRipetiamo il processo per dimensioni del campione \\(n \\in \\left&lt;5, 10, 25, 50\\right&gt;\\)\nL’istogramma è pressoché perfettamente sovrapposto alla PDF di \\(\\mathcal{N}(\\theta, \\sigma^2/n)\\), quale che sia il valore di \\(n\\)"
  },
  {
    "objectID": "slides/ADAS/4-bootstrap.html#esempio-media-campionaria-distribuzione-uniforme",
    "href": "slides/ADAS/4-bootstrap.html#esempio-media-campionaria-distribuzione-uniforme",
    "title": "Tecniche di Bootstrap",
    "section": "Esempio: media campionaria, distribuzione uniforme",
    "text": "Esempio: media campionaria, distribuzione uniforme\n\n\nSe il campione proviene da una distribuzione non-normale (ad esempio uniforme) allora, grazie al teorema del limite centrale, la distribuzione della statistica campionaria è asintoticamente normale, cioè \\(G(\\theta)\\rightarrow\\mathcal{N}(\\theta, \\sigma^2/n)\\) quando \\(n\\rightarrow +\\infty\\).\nRipetiamo l’analisi sopra fatta per il campione normale\nQuesta volta gli istogrammi, come previsto dal teorema del limite centrale diventano gradualmente più normali all’aumentare del numero di elementi di ciascun campione"
  },
  {
    "objectID": "slides/ADAS/4-bootstrap.html#esempio-mediana-campionaria",
    "href": "slides/ADAS/4-bootstrap.html#esempio-mediana-campionaria",
    "title": "Tecniche di Bootstrap",
    "section": "Esempio: mediana campionaria",
    "text": "Esempio: mediana campionaria\n\n\nConsideriamo la mediana \\(\\tilde x\\) come statistica, a partire da campioni uniformi\nSi può dimostrare analiticamente come in questo caso la distribuzione campionaria di \\(\\hat\\theta=\\tilde x\\) tende ad una distribuzione normale \\(\\mathcal{N}(\\theta, 1/(4nf(\\theta)^2))\\), dove \\(f(\\cdot)\\) è la PDF della normale standard, quando \\(n\\rightarrow +\\infty\\)\nIn questo caso, la convergenza è ancora più lenta della convergenza della media su un campione uniforme\nLa convergenza è comunque garantita dal teorema del limite centrale"
  },
  {
    "objectID": "slides/ADAS/4-bootstrap.html#e-quindi",
    "href": "slides/ADAS/4-bootstrap.html#e-quindi",
    "title": "Tecniche di Bootstrap",
    "section": "E Quindi?",
    "text": "E Quindi?\n\nEffettuare inferenza su una statistica non sempre è analiticamente possibile\nÈ possibile generare molti campioni in modo parametrico, cioè assumendo una distribuzione nota sulla base della quale generare campioni casuali\nTuttavia cambiare la distribuzione di partenza può modificare sensibilmente il risultato\nQuindi: sia i metodi analitici che un bootstrap parametrico presentano alcuni limiti"
  },
  {
    "objectID": "slides/ADAS/4-bootstrap.html#boostrap-non-parametrico",
    "href": "slides/ADAS/4-bootstrap.html#boostrap-non-parametrico",
    "title": "Tecniche di Bootstrap",
    "section": "Boostrap Non-Parametrico",
    "text": "Boostrap Non-Parametrico\nSi assume che la distribuzione del campione sia ignota\nSi generano \\(R\\) campioni a partire dal campione originario mediante campionamento con reinserimento\nDato che tutti gli elementi hanno la stessa probabilità di essere estratti dal campione originario, ogni campione di boostrap mantiene la stessa distribuzione (incognita)\nSe \\(R\\) è grande (\\(R &gt; 10000\\)) è possibile studiare la distribuzione del parametro in studio sui campioni di bootstrap per inferirne le proprietà\nIn particolare sarà possibile calcolarne:\n\nil valore atteso\nla varianza\nl’intervallo di confidenza"
  },
  {
    "objectID": "slides/ADAS/4-bootstrap.html#procedura-generale",
    "href": "slides/ADAS/4-bootstrap.html#procedura-generale",
    "title": "Tecniche di Bootstrap",
    "section": "Procedura Generale",
    "text": "Procedura Generale\n\n\nProcedura\n\nSe \\(x\\) è un campione di \\(n\\) elementi provenienti da una distribuzione ignota:\n\nsi ricampiona \\(x\\) con reinserimento, ottenendo \\(x_i^*=\\left&lt;x^*_{i,1}, x^*_{i,2},\\dots,x^*_{1,n}\\right&gt;\\)\nsi calcola la statistica \\(\\hat\\theta^*_1 = s(x_1^*)\\)\nsi ripetono i primi due passi \\(R\\) volte, ottenendo un campione di \\(\\hat \\theta^*_i,~i=1, 2, \\dots, R\\)\n\nLa distribuzione di bootstrap consiste quindi di \\(R\\) stime di \\(\\theta\\) più la stima del campione originale \\(\\hat \\theta\\), cioè si ha il campione di \\(R+1\\) stime \\(\\left&lt;\\hat\\theta,\\hat\\theta^*_1,\\hat\\theta^*_2,\\dots,,\\hat\\theta^*_R\\right&gt;\\).\nSi possono ora stimare le proprietà del parametro \\(\\theta\\) sulla base del campione di bootstrap:\n\nil valore atteso di \\(\\theta\\) è stimata con la media del campione di bootstrap\nla varianza di \\(\\theta\\) è stimata con la varianza del campione di bootstrap\nl’intervallo di confidenza di \\(\\theta\\) è stimato dai quantili del campione di boostrap"
  },
  {
    "objectID": "slides/ADAS/4-bootstrap.html#esempio",
    "href": "slides/ADAS/4-bootstrap.html#esempio",
    "title": "Tecniche di Bootstrap",
    "section": "Esempio",
    "text": "Esempio\n\n\nConsideriamo un campione di \\(n=100\\) elementi provenienti da una popolazione con distribuzione ignota\nDal grafico è evidente che la distribuzione non è normale\nVogliamo calcolare il valore atteso della media campionaria e il suo intervallo di confidenza\n\n\n\n\n\n\n\n\n\n\n\nNOTA: se valesse l’ipotesi di normalità, l’intervallo di confidenza potrebbe essere calcolato dal T-test a un campione"
  },
  {
    "objectID": "slides/ADAS/4-bootstrap.html#esempio-1",
    "href": "slides/ADAS/4-bootstrap.html#esempio-1",
    "title": "Tecniche di Bootstrap",
    "section": "Esempio",
    "text": "Esempio\n\n\nCostruiamo un campione di boostrap ricampionando \\(R=5\\times 10^{4}\\) volte il campione originario, calcolando \\(R+1\\) volte la media campionaria\nRiportiamo in istogramma le \\(R+1\\) stime della media\nIl valore atteso della media è \\(E(\\bar x)=\\bar{\\hat \\theta} = 10.0888\\)\nL’intervallo di confidenza per la media al 95% è calcolato dai quantili empirici di \\(\\hat \\theta\\): \\[\nL = Q^-(\\hat \\theta, (1-0.95)/2),~~~U = Q^+(\\hat \\theta, (1-0.95)/2)\n\\]\n\n\n\n\n\n\n\n\n\n\n\nNOTA: confrontando con l’intervallo calcolato dal T-test si nota che il limite inferiore è diverso: ciò è dovuto all’assimmetria della distribuzione del campione di base"
  },
  {
    "objectID": "slides/ADAS/4-bootstrap.html#vantaggi-della-tecnica-bootstrap",
    "href": "slides/ADAS/4-bootstrap.html#vantaggi-della-tecnica-bootstrap",
    "title": "Tecniche di Bootstrap",
    "section": "Vantaggi della Tecnica Bootstrap",
    "text": "Vantaggi della Tecnica Bootstrap\nIl primo vantaggio della tecnica bootstrap rispetto alle tecniche analitiche lo abbiamo visto nell’esempio precedente:\n\nconsente di effettuare inferenza su una statistica senza alcuna assunzione sulla distribuzione dei dati\n\nÈ anche evidente, tuttavia, che per come è costruita\n\nla tecnica boostrap può essere applicata a qualsiasi statistica, incluse statistiche costruite per via numerica o comunque non esprimibili analiticamente\n\n\n\nL’unico svantaggio delle tecniche di bootstrap è che sono tecniche esclusivamente computazionali e sono quindi più “costose”"
  },
  {
    "objectID": "slides/ADAS/4-bootstrap.html#esempio-2",
    "href": "slides/ADAS/4-bootstrap.html#esempio-2",
    "title": "Tecniche di Bootstrap",
    "section": "Esempio",
    "text": "Esempio\n\n\nSupponiamo di voler calcolare gli intervalli di confidenza su una statistica calcolata per via numerica: la regressione non-lineare mediante il metodo dei minimi quadrati\nIl caso di interesse è la misura dell’istante in cui avviene il contatto tra due corpi, identificato mediante una misura di forza (rumorosa)\nIl modello da regredire è: \\[\nf =\n\\begin{cases}\nf_0 & t &lt; t_0 \\\\\na t^2 + b t + c & t \\geq t_0\n\\end{cases}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nDove \\(f\\) è la forza, \\(f_0\\) è il livello della forza prima del contatto, \\(t_0\\) è l’istante del contatto, e \\(b, ~c\\) parametri di forma della curva di contatto"
  },
  {
    "objectID": "slides/ADAS/4-bootstrap.html#esempio-3",
    "href": "slides/ADAS/4-bootstrap.html#esempio-3",
    "title": "Tecniche di Bootstrap",
    "section": "Esempio",
    "text": "Esempio\n\n\nSiccome \\(f(\\cdot)\\) deve essere continua con derivata continua, risulta \\(2at_0+b=0\\) e \\(at_0^2+bt_0+c=f_0\\) e quindi si hanno tre parametri: \\[\nf =\n\\begin{cases}\nf_0 & t &lt; t_0 \\\\\na t^2 - (2at_0)t + at_0^2+f_0 & t \\geq t_0\n\\end{cases}\n\\] Mediante regressione ai minimi quadrati e successivo bootstrap è possibile identificare l’istante di contatto e il relativo intervallo di confidenza per un assegnato livello di confidenza (in questo caso 95%)\n\n\n\n\n\n\n\n\n\n\n\n\n\nRicordare che un livello di confidenza del 95% corrisponde a un limite sulla probabilità di errore di tipo I del test associato pari a \\(100\\% - 95\\% = 5\\%\\)"
  },
  {
    "objectID": "slides/ADAS/4-bootstrap.html#esempio-4",
    "href": "slides/ADAS/4-bootstrap.html#esempio-4",
    "title": "Tecniche di Bootstrap",
    "section": "Esempio",
    "text": "Esempio\n\n\nÈ anche possibile calcolare, per ogni valore del predittore \\(t\\), gli estremi di \\(f\\) considerando tutte le possibili combinazioni dei tre parametri \\(f_0,~t_0,~a\\)\nIn questo modo è possibile identificare una banda di confidenza sulla funzione interpolante (per un assegnato livello di confidenza)"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#esperimento-industriale-o-esperimento-scientifico",
    "href": "slides/ADAS/6-DoE.html#esperimento-industriale-o-esperimento-scientifico",
    "title": "Design of Experiments",
    "section": "Esperimento industriale o esperimento scientifico?",
    "text": "Esperimento industriale o esperimento scientifico?\n\n\nUn esperimento scientifico viene generalmente condotto allo scopo di supportare o confutare una teoria\n\nè sempre basato su un modello teorico da verificare\nspesso il modello si focalizza sull’effetto di un numero limitato di fattori\n\n\nIn campo industriale spesso ciò non è possibile:\n\nspesso un modello teorico per l’oggetto dell’esperimento non è disponibile per motivi scientifici, tecnici o pratici\nl’interazione tra più fattori è spesso ciò che più interessa\n\n\n\nAi fini pratici, la progettazione dell’esperimento è tanto più importante quanto più è elevata la complessità (cioè il numero di fattori coinvolti)"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#obiettivi-di-un-esperimento",
    "href": "slides/ADAS/6-DoE.html#obiettivi-di-un-esperimento",
    "title": "Design of Experiments",
    "section": "Obiettivi di un esperimento",
    "text": "Obiettivi di un esperimento\nIn generale, un esperimento serve a:\n\nconfermare un’ipotesi teorica (modello): si vuole verificare la forma \\(y=f(\\cdot)\\) del modello teorico; la regressione del modello è affiancata allo studio degli intervalli di confidenza (analitici o bootstrap)\ncalibrare i parametri di un modello: la forma è nota e si vogliono ricavare i valori dei parametri; generalmente si effettua una regressione, raccogliendo i dati in condizioni operative realistiche\nidentificare i fattori che influiscono su un processo: il modello può essere ignoto e si vuole determinare la lista di fattori che compaiono nella \\(y=f(\\cdot)\\); l’obiettivo è costruire un modello empirico approssimato, eventualmente utilizzabile come punto di partenza per la formulazione di un modello teorico"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#dimensionalità-di-un-esperimento",
    "href": "slides/ADAS/6-DoE.html#dimensionalità-di-un-esperimento",
    "title": "Design of Experiments",
    "section": "Dimensionalità di un esperimento",
    "text": "Dimensionalità di un esperimento\nSe il modello di interesse è semplice (un regressore), l’esperimento consiste nell’analisi dell’uscita in corrispondenza di una sequenza di livelli per l’ingresso. Il numero di livelli è correlato con il grado atteso della risposta: per una regressione di grado \\(l\\) servono almeno \\(l+1\\) livelli\nMa se il modello è ha più di un regressore, cioè l’uscita dipende da \\(n\\) fattori, e ogni fattore viene indagato su \\(l\\) livelli, allora il numero di condizioni di test è \\(l^n\\)\nSe ogni condizione di test viene poi ripetuta \\(r\\) volte (per mediare i risultati), il numero di singoli esprimenti è \\(rl^n\\)\nQuesto numero può diventare grande e economicamente insostenibile molto in fretta\n\n\nSupponiamo di avere 4 parametri e 4 livelli, ripetendo ogni test 3 volte: il numero totale di prove è 768; se i parametri diventano 5, il numero cresce a 3072, cioè 4 volte tanto, rapporto che vale anche per il costo"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#piano-fattoriale",
    "href": "slides/ADAS/6-DoE.html#piano-fattoriale",
    "title": "Design of Experiments",
    "section": "Piano fattoriale",
    "text": "Piano fattoriale\n\n\n\nDue fattori, \\(A\\) e \\(B\\)\nIndaghiamo 2 livelli per ogni fattore indicato come \\(X-\\) e \\(X+\\)\nCambiamo un livello alla volta\nValutiamo ogni trattamento 1 sola volta\nValutiamo gli effetti di \\(A\\) e \\(B\\): \\[\n\\begin{align}\nA &= 50 - 20 = 30\\\\\nB &= 30 - 20 = 10\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#notazione-di-yates",
    "href": "slides/ADAS/6-DoE.html#notazione-di-yates",
    "title": "Design of Experiments",
    "section": "Notazione di Yates",
    "text": "Notazione di Yates\nQuando i livelli di tutti i fattori sono solo due, si può usare l’ordine di Yates per indicare le combinazioni di livelli:\n\nFattori ed effetti dei fattori si indicano con lettere maiuscole\nI trattamenti si indicano con combinazioni di lettere minuscole\n\nlettera presente significa fattore a livello alto\nlettera assente significa fattore a livello basso\nse tutte le lettere sono assente si scrive \\((1)\\)\n\n\n\n\nNell’esempio alla slide precedente, i trattamenti sono \\((1), a, b\\)"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#piano-fattoriale-1",
    "href": "slides/ADAS/6-DoE.html#piano-fattoriale-1",
    "title": "Design of Experiments",
    "section": "Piano fattoriale",
    "text": "Piano fattoriale\n\n\nModificando un fattore alla volta non si individuano le interazioni\nSi ha interazione quando l’effetto di un fattore dipende dal livello di un altro fattore\nIn questo secondo esempio misuriamo la risposta dei trattamenti \\((1), a, b, ab\\)\nPossiamo stimare sia gli effetti di \\(A\\) e \\(B\\) che l’interazione \\(AB\\):\n\\[\n\\begin{align}\nA &= \\frac{a+ab}{2} - \\frac{(1) + b}{2} = 6\\\\\nB &= \\frac{b+ab}{2} - \\frac{(1) + a}{2} = -14 \\\\\nAB &= \\frac{a+b}{2} - \\frac{(1)+ab}{2} = -24\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#grafico-di-interazione",
    "href": "slides/ADAS/6-DoE.html#grafico-di-interazione",
    "title": "Design of Experiments",
    "section": "Grafico di interazione",
    "text": "Grafico di interazione\n\n\nIl concetto di interazione è ben illustrato dai grafici di interazione\n\nSe i due segmenti sono paralleli non c’è interazione\nSe i due segmenti sono incrociati o convergenti c’è interazione\nÈ indifferente quale fattore è in ascissa e quale in serie\n\n\n\nResa vs. AResa vs. B"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#superficie-di-risposta",
    "href": "slides/ADAS/6-DoE.html#superficie-di-risposta",
    "title": "Design of Experiments",
    "section": "Superficie di risposta",
    "text": "Superficie di risposta\n\n\nI grafici di interazione non sono altro che proiezioni sull’asse di uno dei due fattori della superficie di risposta\nSi usano generalmente unità codificate, riscalando l’intervallo di ciascun fattore sull’intervallo \\([-1,1]\\)\nIn questo modo si ha la stessa sensibilità indipendentemente dall’intervallo della scala originale\n\n\n\n\n\n\n\n\n\n\n\nNOTA: né la superficie di risposta né i grafici di interazione dànno alcuna informazione sulla significatività statistica degli effetti"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#piani-fattoriali-k2",
    "href": "slides/ADAS/6-DoE.html#piani-fattoriali-k2",
    "title": "Design of Experiments",
    "section": "Piani fattoriali \\(k^2\\)",
    "text": "Piani fattoriali \\(k^2\\)\nIn generale, un esperimento in cui si abbiano 2 fattori ciascuno testato su \\(k\\) livelli è un piano fattoriale \\(k^2\\), perché il numero totale di trattamenti è \\(N=rk^2\\), dove \\(r\\) è il numero di ripetizioni per ciascun trattamento\nIl modello statistico e il modello di regressione associati all’esperimento sono:\n\\[\ny_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk};\\quad \\hat y = \\mu + \\alpha x_1 + \\beta x_2 + (\\alpha\\beta)x_1 x_2\n\\] con \\(x_1\\) e \\(x_2\\) i valori dei due fattori in unità codificate\nCome tale, l’esperimento può essere studiato con un’ANOVA a due fattori (o ANOVA a due vie):\n\n\n\\[\n\\textrm{A)}~\\left\\{\n\\begin{align}\nH_0&: \\alpha_1 = \\alpha_2 = \\dots =\\alpha_a = 0 \\\\\nH_1&: \\alpha_i \\ne 0\\quad\\textrm{per almeno un}~i\n\\end{align}\n\\right.\n\\]\n\\[\n\\textrm{B)}~\\left\\{\n\\begin{align}\nH_0&: \\beta_1 = \\beta_2 = \\dots =\\beta_b = 0 \\\\\nH_1&: \\beta_j \\ne 0\\quad\\textrm{per almeno un}~j\n\\end{align}\n\\right.\n\\]\n\n\\[\n\\textrm{AB)}~\\left\\{\n\\begin{align}\nH_0&: (\\alpha\\beta)_{ij} =   0\\quad \\forall~(i,j) \\\\\nH_1&: (\\alpha\\beta)_{ij} \\ne 0\\quad \\textrm{per almeno una}~(i,j)\n\\end{align}\n\\right.\n\\]"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#piani-fattoriali-k2-1",
    "href": "slides/ADAS/6-DoE.html#piani-fattoriali-k2-1",
    "title": "Design of Experiments",
    "section": "Piani fattoriali \\(k^2\\)",
    "text": "Piani fattoriali \\(k^2\\)\nA queste coppie di ipotesi corrisponde una decomposizione della somma quadratica totale corretta \\(SS_\\mathrm{tot}=SS_A + SS_B + SS_{AB} + SS_E\\)\nLa tabella ANOVA corrispondente è:\n\n\n\n\n\n\n\n\n\n\n\nEffetto\n\\(\\nu\\) (GdL)\n\\(SS\\)\n\\(MS\\)\n\\(F_0\\)\np-value\n\n\n\n\nA\n\\(a-1\\)\n\\(SS_A\\)\n\\(SS_A/\\nu_A\\)\n\\(MS_A/MS_E\\)\n\\(\\mathrm{CDF}^+(F_{0,A}, \\nu_A, \\nu_E)\\)\n\n\nB\n\\(b-1\\)\n\\(SS_B\\)\n\\(SS_B/\\nu_B\\)\n\\(MS_B/MS_E\\)\n\\(\\mathrm{CDF}^+(F_{0,B}, \\nu_B, \\nu_E)\\)\n\n\nAB\n\\((a-1)(b-1)\\))\n\\(SS_{AB}\\)\n\\(SS_{AB}/\\nu_{AB}\\)\n\\(MS_{AB}/MS_E\\)\n\\(\\mathrm{CDF}^+(F_{0,AB}, \\nu_{AB}, \\nu_E)\\)\n\n\nErrore\n\\(ab(-1)\\)\n\\(SS_E\\)\n\\(SS_E/\\nu_E\\)\n—\n—\n\n\nTotale\n\\(abn-1\\)\n\\(SS_\\mathrm{tot}\\)\n\\(SS_\\mathrm{tot}/\\nu_\\mathrm{tot}\\)\n—\n—"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#esempio",
    "href": "slides/ADAS/6-DoE.html#esempio",
    "title": "Design of Experiments",
    "section": "Esempio",
    "text": "Esempio\nVogliamo studiare l’effetto della velocità di taglio (fattore \\(A\\)) e dell’angolo di spoglia (fattore \\(B\\)) di un utensile di tornitura sulla vita utensile\nI due fattori sono entrambi quantitativi; decidiamo di indagare tre livelli per ciascun fattore, ripetendo 2 volte ogni trattamento: piano fattoriale \\(2\\cdot 3^2\\)\n\n\n\npreparazione della griglia di test\ncasualizzazione della sequenza operativa\nconduzione degli esperimenti e raccolta dati\nformulazione e verifica del modello statistico\nanalisi della varianza (ANOVA)\ncreazione della superficie di risposta\n\nI punti 4 e 5 sono possibilmente iterati\n\n\n\n\n\n\nStdOrder\nAngolo\nA\nVelocità\nB\nRipetizione\nVita\n\n\n\n\n1\n15\n-1\n125\n-1\n1\nNA\n\n\n2\n20\n0\n125\n-1\n1\nNA\n\n\n3\n25\n1\n125\n-1\n1\nNA\n\n\n4\n15\n-1\n150\n0\n1\nNA\n\n\n5\n20\n0\n150\n0\n1\nNA\n\n\n6\n25\n1\n150\n0\n1\nNA\n\n\n7\n15\n-1\n175\n1\n1\nNA\n\n\n8\n20\n0\n175\n1\n1\nNA\n\n\n9\n25\n1\n175\n1\n1\nNA\n\n\n\n\n\n\n\n\nPer motivi di spazio la tabella riporta solo la prima metà delle righe, corrispondenti con la prima ripetizione"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#esempio-passo-2.",
    "href": "slides/ADAS/6-DoE.html#esempio-passo-2.",
    "title": "Design of Experiments",
    "section": "Esempio — passo 2.",
    "text": "Esempio — passo 2.\n\n\nNella griglia si genera una nuova colonna di interi \\(\\left&lt;1\\dots N\\right&gt;,~N=rl^n\\) ordinati casualmente\nSi riordina la griglia secondo la nuova colonna, generalmente chiamata Run Order\n\n\n\n\n\n\nStdOrder\nRunOrder\nAngolo\nA\nVelocità\nB\nRipetizione\nVita\n\n\n\n\n4\n1\n15\n-1\n150\n0\n1\nNA\n\n\n5\n2\n20\n0\n150\n0\n1\nNA\n\n\n10\n3\n15\n-1\n125\n-1\n2\nNA\n\n\n2\n4\n20\n0\n125\n-1\n1\nNA\n\n\n12\n5\n25\n1\n125\n-1\n2\nNA\n\n\n15\n6\n25\n1\n150\n0\n2\nNA\n\n\n3\n7\n25\n1\n125\n-1\n1\nNA\n\n\n18\n8\n25\n1\n175\n1\n2\nNA\n\n\n17\n9\n20\n0\n175\n1\n2\nNA\n\n\n14\n10\n20\n0\n150\n0\n2\nNA\n\n\n8\n11\n20\n0\n175\n1\n1\nNA\n\n\n13\n12\n15\n-1\n150\n0\n2\nNA\n\n\n6\n13\n25\n1\n150\n0\n1\nNA\n\n\n1\n14\n15\n-1\n125\n-1\n1\nNA\n\n\n11\n15\n20\n0\n125\n-1\n2\nNA\n\n\n7\n16\n15\n-1\n175\n1\n1\nNA\n\n\n16\n17\n15\n-1\n175\n1\n2\nNA\n\n\n9\n18\n25\n1\n175\n1\n1\nNA"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#esempio-passo-3.",
    "href": "slides/ADAS/6-DoE.html#esempio-passo-3.",
    "title": "Design of Experiments",
    "section": "Esempio — passo 3.",
    "text": "Esempio — passo 3.\n\n\nSi effettuano gli esperimenti secondo il run order\nEseguendo gli esperimenti secondo il run order eventuali effetti ignoti e incontrollati si distribuiscono casualmente su tutti i trattamenti:\n\nla varianza globale aumenta (\\(SS_\\mathrm{tot}\\))\nl’effetto relativo dei fattori non viene alterato (\\(SS_X\\))\n\nNota: una ripetizione è una replica dell’intero esperimento, non della sola operazione di misura\n\n\n\n\n\n\nStdOrder\nRunOrder\nAngolo\nA\nVelocità\nB\nRipetizione\nVita\n\n\n\n\n4\n1\n15\n-1\n150\n0\n1\n-3\n\n\n5\n2\n20\n0\n150\n0\n1\n1\n\n\n10\n3\n15\n-1\n125\n-1\n2\n-1\n\n\n2\n4\n20\n0\n125\n-1\n1\n0\n\n\n12\n5\n25\n1\n125\n-1\n2\n0\n\n\n15\n6\n25\n1\n150\n0\n2\n6\n\n\n3\n7\n25\n1\n125\n-1\n1\n-1\n\n\n18\n8\n25\n1\n175\n1\n2\n-1\n\n\n17\n9\n20\n0\n175\n1\n2\n6\n\n\n14\n10\n20\n0\n150\n0\n2\n3\n\n\n8\n11\n20\n0\n175\n1\n1\n4\n\n\n13\n12\n15\n-1\n150\n0\n2\n0\n\n\n6\n13\n25\n1\n150\n0\n1\n5\n\n\n1\n14\n15\n-1\n125\n-1\n1\n-2\n\n\n11\n15\n20\n0\n125\n-1\n2\n2\n\n\n7\n16\n15\n-1\n175\n1\n1\n2\n\n\n16\n17\n15\n-1\n175\n1\n2\n3\n\n\n9\n18\n25\n1\n175\n1\n1\n0"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#esempio-passo-4.",
    "href": "slides/ADAS/6-DoE.html#esempio-passo-4.",
    "title": "Design of Experiments",
    "section": "Esempio — passo 4.",
    "text": "Esempio — passo 4.\n\n\nFormuliamo il modello di regressione completo di secondo grado:\n\\[\\begin{align}\n\\hat y = & \\mu + \\alpha_1 x_1 + \\beta_1 x_2 + (\\alpha\\beta)_1x_1x_2 + \\\\\n         & + \\alpha_2x_1^2 + \\beta_2x_2^2 + (\\alpha\\beta)_{2,1}x_1^2x_2 + \\\\\n         & + (\\alpha\\beta)_{1,2}x_1x_2^2 + (\\alpha\\beta)_{2,2}x_1^2x_2^2\n      \n\\end{align}\\]\ndove, ricordiamo \\((\\alpha\\beta)\\) non è un prodotto, ma rappresenta il coefficiente di un prodotto di fattori \\(A\\) e \\(B\\)\nÈ necessario valutare normalità e assenza di pattern nei residui\n\n\n\\(\\varepsilon(A)\\)\\(\\varepsilon(B)\\)\\(\\varepsilon(r)\\)Quantile-Quantile"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#esempio-passo-5.",
    "href": "slides/ADAS/6-DoE.html#esempio-passo-5.",
    "title": "Design of Experiments",
    "section": "Esempio — passo 5.",
    "text": "Esempio — passo 5.\n\n\nScegliendo il 5% come soglia sul p-value, osserviamo che risultano statisticamente non significativi gli effetti:\n\n\\(B^2\\), corrispondente al termine \\(\\beta_2\\) nella regressione\n\\(A^2B\\), corrispondente al termine \\((\\alpha\\beta)_{2,1}\\) nella regressione\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neffetto\n\\(\\nu\\)\n\\(SS\\)\n\\(MS\\)\n\\(F_0\\)\n\\(p\\mathrm{-value}\\)\n\n\n\n\n\\(A\\)\n1\n8.333333\n8.333333\n5.769231\n0.0397723\n\n\n\\(B\\)\n1\n21.333333\n21.333333\n14.769231\n0.0039479\n\n\n\\(A^2\\)\n1\n16.000000\n16.000000\n11.076923\n0.0088243\n\n\n\\(B^2\\)\n1\n4.000000\n4.000000\n2.769231\n0.1304507\n\n\n\\(AB\\)\n1\n8.000000\n8.000000\n5.538462\n0.0430650\n\n\n\\(A^2B\\)\n1\n2.666667\n2.666667\n1.846154\n0.2073056\n\n\n\\(AB^2\\)\n1\n42.666667\n42.666667\n29.538462\n0.0004137\n\n\n\\(A^2B^2\\)\n1\n8.000000\n8.000000\n5.538462\n0.0430650\n\n\n\\(\\varepsilon\\)\n9\n13.000000\n1.444444\nNA\nNA\n\n\n\n\n\n\nQuindi l’equazione di regressione diventa: \\[\\begin{align}\n\\hat y = & \\mu + \\alpha_1 x_1 + \\beta_1 x_2 + (\\alpha\\beta)_1x_1x_2 + \\\\\n         &  + \\alpha_2x_1^2 + (\\alpha\\beta)_{1,2}x_1x_2^2 + \\\\\n         &  + (\\alpha\\beta)_{2,2}x_1^2x_2^2\n      \n\\end{align}\\]"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#esempio-passo-6.",
    "href": "slides/ADAS/6-DoE.html#esempio-passo-6.",
    "title": "Design of Experiments",
    "section": "Esempio — passo 6.",
    "text": "Esempio — passo 6.\n\n\nLa superficie di risposta consente di identificare punti e direzioni notevoli:\n\nIl punto S è un punto di sella, in cui il gradiente è nullo in qualsiasi direzione: punto stabile\nNel punto P, le direzioni tangenti all’isoipsa sono direzioni a resa costante\nIl punto M è un massimo della resa\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn generale, una superfifice di risposta è una iper-superficie in uno spazio \\(n+1\\)-dimensionale, dove \\(n\\) è il numero di fattori"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#analisi-di-adeguatezza-del-modello",
    "href": "slides/ADAS/6-DoE.html#analisi-di-adeguatezza-del-modello",
    "title": "Design of Experiments",
    "section": "Analisi di adeguatezza del modello",
    "text": "Analisi di adeguatezza del modello\nDopo aver eventualmente escluso alcuni effetti (ad esempio \\(B^2\\) e \\(A^2B\\)) è necessario:\n\nriformulare il modello\nanalizzare i residui del nuovo modello\nconfermare con una nuova ANOVA\n\nIn particolare, l’analisi dei residui è detta verifica di adeguatezza del modello (Model Adequacy Check, MAC) e consiste dipicamente in:\n\nverifica assenza di pattern nei residui in funzione dei fattori\nverifica assenza di pattern nei residui in funzione dell’ordine di test\nverifica di normalità dei residui (grafico Q-Q e test di Shapiro-Wilk)"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#piano-fattoriale-2n",
    "href": "slides/ADAS/6-DoE.html#piano-fattoriale-2n",
    "title": "Design of Experiments",
    "section": "Piano fattoriale \\(2^n\\)",
    "text": "Piano fattoriale \\(2^n\\)\n\n\nI piani fattoriali in cui tutti i fattori hanno due livelli (basso e alto, -1 e +1 in unità codificate) sono di particolare interesse\n\nconsentono di regredire modelli soltanto del primo grado\nrichiedono il minimo delle prove\nconsentono comunque di definire la sensibilità del processo ai vari fattori, escludendo i fattori non-significativi\nsono il punto di partenza di qualsiasi analisi di processi complessi"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#piano-fattoriale-2n-1",
    "href": "slides/ADAS/6-DoE.html#piano-fattoriale-2n-1",
    "title": "Design of Experiments",
    "section": "Piano fattoriale \\(2^n\\)",
    "text": "Piano fattoriale \\(2^n\\)\n\n\n\nOgni fattore ha due livelli: basso e alto. In unità codificate valgono -1 e +1, indicati in breve come - e +\nI trattamenti, cioè combinazioni di livelli per gli \\(n\\) fattori, sono indicati con la notazione di Yates\nla matrice di progetto si ottiene permutando tutti i fattori tra - e + con frequenze via via dimezzate\nla matrice di progetto è ripetuta per le \\(r\\) ripetizioni e riporta anche la colonna dell’ordine casuale di esecuzione\nla matrice degli effetti si ottiene aggiungendo le colonne per le interazioni, calcolate come prodotto dei segni relativi\n\n\n\nMatrice di progettoMatrice degli effetti\n\n\n\n\n\n\n\ntrattamento\nripetizione\nA\nB\nC\nordine\n\n\n\n\n(1)\n1\n-\n-\n-\n1\n\n\na\n1\n+\n-\n-\n7\n\n\nb\n1\n-\n+\n-\n5\n\n\nab\n1\n+\n+\n-\n6\n\n\nc\n1\n-\n-\n+\n8\n\n\nac\n1\n+\n-\n+\n4\n\n\nbc\n1\n-\n+\n+\n2\n\n\nabc\n1\n+\n+\n+\n3\n\n\n\n\n\n\n\n\n\n\n\n\ntrattamento\nI\nA\nB\nAB\nC\nAC\nBC\nABC\n\n\n\n\n(1)\n+\n-\n-\n+\n-\n+\n+\n-\n\n\na\n+\n+\n-\n-\n-\n-\n+\n+\n\n\nb\n+\n-\n+\n-\n-\n+\n-\n+\n\n\nab\n+\n+\n+\n+\n-\n-\n-\n-\n\n\nc\n+\n-\n-\n+\n+\n-\n-\n+\n\n\nac\n+\n+\n-\n-\n+\n+\n-\n-\n\n\nbc\n+\n-\n+\n-\n+\n-\n+\n-\n\n\nabc\n+\n+\n+\n+\n+\n+\n+\n+"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#piano-fattoriale-2n-2",
    "href": "slides/ADAS/6-DoE.html#piano-fattoriale-2n-2",
    "title": "Design of Experiments",
    "section": "Piano fattoriale \\(2^n\\)",
    "text": "Piano fattoriale \\(2^n\\)\nLa matrice degli effetti contiene le informazioni per calcolare gli effetti e le somme quadratiche \\(SS\\) che, a loro volta, servono per completare la tabella ANOVA\n\n\nPer gli effetti: \\[\n\\begin{align}\nA &= \\frac{-(1)+a-b+ab}{2r} \\\\\nB &= \\frac{-(1)-a+b+ab}{2r} \\\\\nAB &= \\frac{+(1)-a-b+ab}{2r}\n\\end{align}\n\\]\n\nPer le somme quadratiche: \\[\n\\begin{align}\nSS_A &=& \\frac{(-(1)+a-b+ab)^2}{4r} \\\\\nSS_B &=& \\frac{(-(1)-a+b+ab)^2}{4r} \\\\\nSS_{AB} &=& \\frac{(+(1)-a-b+ab)^2}{4r}\n\\end{align}\n\\]\n\nIn generale: \\(\\mathrm{Ef}(X) = \\frac{2}{2^nr}\\mathrm{Contrast}(X)\\) e \\(\\mathit{SS}(X) = \\frac{1}{2^nr}\\mathrm{Contrast}(X)^2\\) dove il contrasto del fattore \\(\\mathrm{Contrast}(X)\\) è calcolato usando i segni della relativa colonna \\(X\\) per i trattamenti nell’ordine di Yates (es. \\(\\mathrm{Contrast}(AB)=+(1)-a-b+ab\\))"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#piano-fattoriale-2n-modello-statistico",
    "href": "slides/ADAS/6-DoE.html#piano-fattoriale-2n-modello-statistico",
    "title": "Design of Experiments",
    "section": "Piano fattoriale \\(2^n\\): modello statistico",
    "text": "Piano fattoriale \\(2^n\\): modello statistico\nIl modello statistico di un piano fattoriale \\(2^n\\) è ovviamente un modello lineare di primo grado in tutti i fattori. Per \\(n=2\\), ad esempio:\n\\[\ny_{ijk} = \\mu + \\alpha_{i} + \\beta_{j} + (\\alpha\\beta)_{ij}+\\varepsilon_{ijk}\n\\] Per \\(n=3\\): \\[\ny_{ijkl} = \\mu + \\alpha_{i} + \\beta_{j} + (\\alpha\\beta)_{ij}+ \\gamma_k + (\\alpha\\gamma)_{ik} + (\\beta\\gamma)_{jk} + (\\alpha\\beta\\gamma)_{ijk} + \\varepsilon_{ijkl}\n\\] e così via.\nIn R vedremo che questi modelli possono essere rispettivamente abbreviati come Y~A*B e Y~A*B*C e utilizzati per calcolare la tabella ANOVA\n\n\nEsercizio: ricavare i modelli di regressione corrispondenti ai due modelli statistici sopra riportati"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#piani-fattoriali-non-replicati",
    "href": "slides/ADAS/6-DoE.html#piani-fattoriali-non-replicati",
    "title": "Design of Experiments",
    "section": "Piani fattoriali non replicati",
    "text": "Piani fattoriali non replicati\nAl crescere del numero di fattori, il numero di singoli test può diventare insostenibile\nIl modo più semplice per ridurre il numero di test è evitare le ripetizioni per i vari trattamenti\nSe un trattamento non è ripetuto, però, non è possibile calcolare la \\(SS_E\\) e quindi non si può completare la tabella ANOVA con le \\(F_0\\) e i p-value\nLa soluzione è stata proposta da C. Daniel e si basa sull’ipotesi che almeno uno dei fattori o delle interazioni non sia significativo\nQuest’ipotesi è di solito ragionevole per i processi più complessi con \\(n\\) elevato, proprio i casi in cui è particolarmente importante ridurre il numero di test\nL’idea è che gli effetti non-significativi siano statistiche calcolate su diversi sotto-campioni dello stesso campione omogeneo, e quindi siano normalmente distribuiti. Solo gli effetti significativi si dipartono dalla distribuzione normale degli altri"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#metodo-di-daniel",
    "href": "slides/ADAS/6-DoE.html#metodo-di-daniel",
    "title": "Design of Experiments",
    "section": "Metodo di Daniel",
    "text": "Metodo di Daniel\n\n\nQuali effetti siano probabilmente significativi è quindi possibile determinarlo con un grafico quantile-quantile degli stessi\nIl grafico è un primo screening che deve essere conservativo: serve solo per rimuovere effetti sicuramente non significativi (cioè molto allineati alla diagonale) e consentire l’esecuzione dell’ANOVA\nLa tabella ANOVA va comunque calcolata su un modello statistico lineare ridotto, in modo da confermare il risultato del metodo grafico o da rimuovere ulteriori effetti che risultassero effettivamente non-significativi"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#metodo-di-daniel-1",
    "href": "slides/ADAS/6-DoE.html#metodo-di-daniel-1",
    "title": "Design of Experiments",
    "section": "Metodo di Daniel",
    "text": "Metodo di Daniel\n\n\nIn questo caso risultano non-normali solo gli effetti \\(A, C, D\\) e le interazioni \\(AC\\) e \\(AD\\)\nIl modello statistico lineare può quindi essere rivisto come\n\\[\n\\begin{multline}\ny_{ijkl} = \\mu + \\alpha_{i} + \\\\\n+ \\gamma_{j} + (\\alpha\\gamma)_{ij}+ \\delta_k + (\\alpha\\delta)_{ik} + \\varepsilon_{ijkl}\n\\end{multline}\n\\] Cioè possiamo già escludere che \\(B\\) sia di fatto un fattore. In questo modo, anziché un piano fattoriale \\(2^4\\) non replicato abbiamo a che fare con un \\(2^3\\) replicato due volte (cioè \\(2\\cdot 2^3\\)), per il quale possiamo eseguire una normale analisi ANOVA"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#esempio-1",
    "href": "slides/ADAS/6-DoE.html#esempio-1",
    "title": "Design of Experiments",
    "section": "Esempio",
    "text": "Esempio\n\n\nConsideriamo i dati rappresentati in questo grafico\nNon ha importanza l’origine dei dati: limitiamoci a costruire un modello lineare dei dati\n\\[ y_{ij} = \\mu + x_i + \\varepsilon_{ij}\\]\nSi noti come i residui non sembrano normali e che ci sono degli evidenti pattern\n\n\nDatiResidui vs. xResidui vs. \\(\\hat y\\)Q-Q"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#esempio-2",
    "href": "slides/ADAS/6-DoE.html#esempio-2",
    "title": "Design of Experiments",
    "section": "Esempio",
    "text": "Esempio\n\n\nSe osserviamo dal boxplot che la resa tende ad aumentare più che linearmente con \\(x\\), possiamo pensare di riformulare il modello trasformandolo mediante un’elevazione al quadrato:\n\\[ y_{ij} = (\\mu + x_i + \\varepsilon_{ij})^2\\] Il modello non sembra più lineare, ma considerando che può essere riscritto come\n\\[ \\sqrt{y_{ij}} = \\mu + x_i + \\varepsilon_{ij}\\]\nè evidente che si tratta ancora di un modello lineare nei coefficienti.\n\n\nResidui vs. xResidui vs. \\(\\hat y\\)Q-Q\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn seguito a questa trasformazione è quindi evidente che il nuovo modello è più adeguato, con residui privi di pattern e più compatibili con una distribuzione normale"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#trasformazioni-box-cox",
    "href": "slides/ADAS/6-DoE.html#trasformazioni-box-cox",
    "title": "Design of Experiments",
    "section": "Trasformazioni Box-Cox",
    "text": "Trasformazioni Box-Cox\nBox e Cox hanno proposto un metodo per identificare la miglior trasformazione nella famiglia delle trasformazioni di potenza \\(y^* = y^\\lambda\\), con \\(y^*=\\ln(y)\\) quando \\(\\lambda=0\\)\nSi calcola un grafico della verosimiglianza logaritmica \\(\\mathcal{L}\\) (log-likelyhood) della seguente \\(y^{(\\lambda)}\\):\n\\[\ny_i^{(\\lambda)} =\n\\begin{cases}\n\\frac{y_i^\\lambda-1}{\\lambda\\dot y^{\\lambda-1}} & \\lambda\\neq 0 \\\\\n\\dot y \\ln y_i & \\lambda = 0\n\\end{cases}, ~~~ \\dot y = \\exp\\left[(1/n)\\sum \\ln y_i\\right],~i=1, 2,\\dots,n\n\\] La verosimiglianza \\(\\ln\\mathcal{L}(\\lambda|y)\\) non è altro che la probabilità di estrarre un campione \\(y\\) dato un certo parametro \\(\\lambda\\). Il suo massimo coincide col valore di \\(\\lambda\\) che rende il campione \\(y\\) più normale."
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#diagramma-box-cox",
    "href": "slides/ADAS/6-DoE.html#diagramma-box-cox",
    "title": "Design of Experiments",
    "section": "Diagramma Box-Cox",
    "text": "Diagramma Box-Cox\n\n\nIl diagramma Box-Cox individua anche un intervallo corrispondente ad una variazione inferiore al 95%\nQualunque valore di \\(\\lambda\\) interno a questo intervallo è statisticamente equivalente\nSi sceglie quindi il valore includo nell’intervallo e che rappresenta una trasformazione “comoda”\nAd es., se risultasse un \\(\\lambda\\) ottimale pari a 0.58, sceglierei comunque \\(\\lambda=0.5\\), che corrisponde alla trasformazione \\(y^*=\\sqrt{y}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nOgni volta che un modello lineare è sospetto (in una regressione o nell’analisi di un piano fattoriale) è opportuno tentare con una trasformazione Box-Cox. Nei PF, a volte la trasformazione Box-Cox può semplificare il modello (cioè ridurre il numero di fattori o interazioni significativi)"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#central-composite-design-ccd",
    "href": "slides/ADAS/6-DoE.html#central-composite-design-ccd",
    "title": "Design of Experiments",
    "section": "Central Composite Design (CCD)",
    "text": "Central Composite Design (CCD)\n\n\nVerrebbe automatico estendere un PF da \\(2^2\\) a \\(2^3\\) per valutare gli effetti quadratici\nIn questo modo, però, la sensibilità nelle direzioni assiali sarebbe inferiore alla sensibilità nelle direzioni diagonali, essendo l’intervallo di valutazione più piccolo nel primo caso\nSi preferisce quindi eseguire PF centrati con simmetria rotazionale attorno all’origine\nPer due fattori, i punti assiali sono estesi a distanza \\(\\sqrt{2}\\) dall’origine; nel generico caso \\(n\\)-dimensionale la distanza è \\((2^n)^{1/4}\\)\n\n\nDue fattoriTre fattori"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#piani-fattoriali-frazionati-ffp",
    "href": "slides/ADAS/6-DoE.html#piani-fattoriali-frazionati-ffp",
    "title": "Design of Experiments",
    "section": "Piani fattoriali frazionati (FFP)",
    "text": "Piani fattoriali frazionati (FFP)\n\n\nSupponiamo di considerare soltanto i vertici opposti del PF in figura: \\((1), ab, ac, bc\\)\nStiamo considerando metà del PF originale, che comunque include tutti i livelli dei tre fattori\nSicuramente il frazionamento riduce la completezza del modello, ma consente di risparmiare molti test\n\nCome scegliere il frazionamento per dimensioni superiori?\nQuali informazioni perdiamo?"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#scegliere-il-frazionamento-relazioni-definenti",
    "href": "slides/ADAS/6-DoE.html#scegliere-il-frazionamento-relazioni-definenti",
    "title": "Design of Experiments",
    "section": "Scegliere il frazionamento: relazioni definenti",
    "text": "Scegliere il frazionamento: relazioni definenti\n\n\nOsservando la matrice degli effetti, osserviamo che i trattamenti \\((1), ab, ac, bc\\) corrispondono alle righe per cui vale la relazione \\(I=-ABC\\). L’altra metà complementare corrisponde invece a \\(I=ABC\\)\nQueste relazioni sono chiamate relazioni definenti perché definiscono il PFF. È indifferente scegliere la metà positiva o quella negativa\n\n\n\n\n\n\ntrattamento\nI\nA\nB\nAB\nC\nAC\nBC\nABC\n\n\n\n\n(1)\n+\n-\n-\n+\n-\n+\n+\n-\n\n\na\n+\n+\n-\n-\n-\n-\n+\n+\n\n\nb\n+\n-\n+\n-\n-\n+\n-\n+\n\n\nab\n+\n+\n+\n+\n-\n-\n-\n-\n\n\nc\n+\n-\n-\n+\n+\n-\n-\n+\n\n\nac\n+\n+\n-\n-\n+\n+\n-\n-\n\n\nbc\n+\n-\n+\n-\n+\n-\n+\n-\n\n\nabc\n+\n+\n+\n+\n+\n+\n+\n+\n\n\n\n\n\n\n\n\nLa matrice degli effetti per un PF frazionato \\(2^{n-1}\\) più la relazione definente individuano univocamente un piano fattoriale frazionato"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#alias-e-informazioni-perdute",
    "href": "slides/ADAS/6-DoE.html#alias-e-informazioni-perdute",
    "title": "Design of Experiments",
    "section": "Alias e informazioni perdute",
    "text": "Alias e informazioni perdute\nIn un PF frazionato \\(2^{3-1}\\) con relazione definente \\(I=ABC\\), consideriamo questi effetti: \\[\n\\begin{align}\nA  &= (-(1)+a-b+ab-c+ac-bc+abc)/(2r) \\\\\nBC &= (\\underline{+(1)}+a-b\\underline{-ab}-c\\underline{-ac} \\underline{+bc}+abc)/(2r)\n\\end{align}\n\\] Siccome i trattamenti sottolineati non sono testati, l’effetto \\(A\\) risulta indistinguibile dall’effetto \\(BC\\)\nSi dice che \\(A\\) è in alias con \\(BC\\)\nData una certa relazione definente, le possibili strutture di alias possono essere ricavate dalla relazione stessa mediante un’algebra dedicata: \\(I\\cdot X=X\\), \\(X\\cdot X = X\\), \\(X\\cdot Y=XY\\).\nQuindi, risulta \\(A\\cdot I=A\\cdot ABC\\) cioè \\(A=BC\\), e altrettanto \\(B=AC\\) e \\(C=AB\\)."
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#alias-e-informazioni-perdute-1",
    "href": "slides/ADAS/6-DoE.html#alias-e-informazioni-perdute-1",
    "title": "Design of Experiments",
    "section": "Alias e informazioni perdute",
    "text": "Alias e informazioni perdute\nQuindi, frazionando un PF si perdono informazioni: si perde la capacità di discriminare tra gli effetti in alias. È evidente che più lunga è la relazione definente, più elevato sarà il grado di interazioni in alias con gli effetti diretti (es. \\(A=BCDEF\\))\nIn virtù del principio di sparsità degli effetti, tuttavia, questa perdita di informazioni non è drammatica. Il principio dice che in un processo la significatività di interazioni di alto livello è via via meno probabile all’aumentare del numro di fattori che le compongono\nDi conseguenza, un alias \\(A=BCDEF\\) può essere trascurato assumendo la significatività di \\(A\\) piuttosto che quella di \\(BCDEF\\) in virtù del PSdE"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#pf-frazionati-di-tipo-2n-p",
    "href": "slides/ADAS/6-DoE.html#pf-frazionati-di-tipo-2n-p",
    "title": "Design of Experiments",
    "section": "PF Frazionati di tipo \\(2^{n-p}\\)",
    "text": "PF Frazionati di tipo \\(2^{n-p}\\)\nÈ possibile frazionare un piano più di una volta, riducendo il numero di trattamenti a \\(2^{n-p}\\)\nPer ogni frazionamento è necessario scegliere una nuova relazione definente\nAd esempio, per \\(2^{7-2}\\) si possono scegliere le RD \\(I=ABCDE\\) e \\(I=CDEFG\\)\nPer queste due RD ne esiste una terza, dipendente: \\(I=ABFG\\). Due qualsiasi di queste tre RD sono equivalenti\nMinimum aberration design\n\nSi generano tutte le possibili \\(p\\)-uple di relazioni definenti, completandole con la dipendente\nSi conta il numero di lettere nelle \\((p+1)\\)-uple (es. precedente: \\(\\left&lt;5,5,4\\right&gt;\\))\nil design che minimizza il numero di stringhe con lunghezza minima è da preferire perché ha meno alias"
  },
  {
    "objectID": "slides/ADAS/6-DoE.html#per-fare-un-buon-pf",
    "href": "slides/ADAS/6-DoE.html#per-fare-un-buon-pf",
    "title": "Design of Experiments",
    "section": "Per fare un buon PF",
    "text": "Per fare un buon PF\n\nIniziare con un \\(2^n\\) per poi aumentarlo\nValutare l’opportunità di frazionamento e scegliere con attenzione le relazioni definenti\nEseguire sempre la verifica di adeguatezza del modello e raffinare il modello statistico di conseguenza\nDiscutere le interazione e gli effetti solo dopo aver raffinato il modello\nValutare gli effetti di alias\n\nTemi avanzati\n\nBlocking\nMinimum Aberration Design\nTrasformazioni del modello e trasformazioni Box-Cox"
  },
  {
    "objectID": "slides/ADAS/bilancia.html#schema",
    "href": "slides/ADAS/bilancia.html#schema",
    "title": "Esperimenti di taratura",
    "section": "Schema",
    "text": "Schema\n\n\nConsideriamo una bilancia a due piatti come in figura\nIl piatto sinistro è caricato con un peso noto \\(F_1\\) simile alla massa da pesare \\(F_2\\)\nL’angolo dell’ago all’equilibrio \\(\\delta\\) è funzione della differenza tra i pesi\nSiano:\n\n\\(l\\) la lunghezza dei bracci\n\\(h\\) la distanza verticale all’equilibrio tra il centro di massa del bilanciere e il fulcro\n\\(F_M\\) il peso del bilancere"
  },
  {
    "objectID": "slides/ADAS/bilancia.html#formule",
    "href": "slides/ADAS/bilancia.html#formule",
    "title": "Esperimenti di taratura",
    "section": "Formule",
    "text": "Formule\n\n\nBracci delle forze: \\[\n\\begin{align}\nb_1 =& \\left(\\frac{l}{2}+h\\tan\\delta\\right) \\\\\nb_2 =& l\\cos\\delta-b_1 = \\left(\\frac{l}{2}-h\\tan\\delta\\right)\\cos\\delta \\\\\nb_M =& h\\tan\\delta\\cos\\delta\n\\end{align}\n\\]\n\nEquilibrio dei momenti: \\[\n\\begin{align}\n0 =& F_1b_1 +F_Mb_M - F_2b_2 \\\\\n0 =& F_1(b_1-b_2) +F_Mb_M -\\Delta Fb_2 \\\\\n0 =& F_1(2h\\tan\\delta)+F_Mh\\tan\\delta + \\\\\n   & -\\Delta F(l/2-h\\tan\\delta)\n\\end{align}\n\\]\n\nQuindi: \\[\n\\tan\\delta = \\frac{l}{2h}\\frac{\\Delta F}{2F_1+F_M+\\Delta F} = a\\frac{\\Delta F}{2F_1+F_M+\\Delta F}\n\\]"
  },
  {
    "objectID": "slides/ADAS/bilancia.html#dati-raccolti-in-ordine-regolare",
    "href": "slides/ADAS/bilancia.html#dati-raccolti-in-ordine-regolare",
    "title": "Esperimenti di taratura",
    "section": "Dati raccolti in ordine regolare",
    "text": "Dati raccolti in ordine regolare\n\n\nGrafico \\(\\delta\\) vs. \\(F_1\\)\n\n\n\n\n\n\n\n\n\n\nGrafico \\(\\delta\\) vs. \\(\\Delta F\\)"
  },
  {
    "objectID": "slides/ADAS/bilancia.html#dati-raccolti-in-ordine-casuale",
    "href": "slides/ADAS/bilancia.html#dati-raccolti-in-ordine-casuale",
    "title": "Esperimenti di taratura",
    "section": "Dati raccolti in ordine casuale",
    "text": "Dati raccolti in ordine casuale\n\n\nGrafico \\(\\delta\\) vs. \\(F_1\\)\n\n\n\n\n\n\n\n\n\n\nGrafico \\(\\delta\\) vs. \\(\\Delta F\\)"
  },
  {
    "objectID": "slides/ADAS/bilancia.html#dati-ordinati-regressione",
    "href": "slides/ADAS/bilancia.html#dati-ordinati-regressione",
    "title": "Esperimenti di taratura",
    "section": "Dati ordinati, regressione",
    "text": "Dati ordinati, regressione\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRisulta: \\(a = 0.9551407\\) e \\(F_M=28.4649367\\), \\(R^2=1-\\frac{SS_\\mathrm{res}}{SS_\\mathrm{tot}}=1-\\frac{\\sum (y_i - \\widehat y_i)}{\\sum (y_i - \\bar y)^2} = 0.9865372\\)"
  },
  {
    "objectID": "slides/ADAS/bilancia.html#dati-casualizzati-regressione",
    "href": "slides/ADAS/bilancia.html#dati-casualizzati-regressione",
    "title": "Esperimenti di taratura",
    "section": "Dati casualizzati, regressione",
    "text": "Dati casualizzati, regressione\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRisulta: \\(a = 0.9857569\\) e \\(F_M=40.7223573\\), \\(R^2=1-\\frac{SS_\\mathrm{res}}{SS_\\mathrm{tot}}=1-\\frac{\\sum (y_i - \\widehat y_i)}{\\sum (y_i - \\bar y)^2} = 0.9824978\\)"
  },
  {
    "objectID": "slides/ADAS/bilancia.html#osservazioni",
    "href": "slides/ADAS/bilancia.html#osservazioni",
    "title": "Esperimenti di taratura",
    "section": "Osservazioni",
    "text": "Osservazioni\n\nSebbene il valore di \\(R^2\\) sia leggermente migliore per i dati non casualizzati, è evidente che la regressione è più vicina al modello nominale per il caso con i dati casualizzati\nQuesto insegna a considerare \\(R^2\\) solo come termine di confronto tra regressioni diverse ma sugli stessi dati\nSe la regressione è fatta sul modello \\(\\tan(\\delta)=f(F, \\Delta F)\\) anziché sul modello \\(\\delta = \\arctan(f(F, \\Delta f))\\) bisogna fare attenzione che i residui \\(y_i - \\widehat y_i\\) e i valori regressi \\(\\widehat y_i\\) sono calcolati come tangenti dell’angolo e non come angolo vero e proprio"
  },
  {
    "objectID": "slides/ADAS/bilancia.html#progetto",
    "href": "slides/ADAS/bilancia.html#progetto",
    "title": "Esperimenti di taratura",
    "section": "Progetto",
    "text": "Progetto\nIl progetto individuale richiede di eseguire un esperimento di taratura statica dello strumento bilancia a due piatti\n\nLo strumento virtuale è disponibile su https://p4010.shinyapps.io/bilancia/\nLo strumento ha parametri \\(h\\) e \\(l\\) ignoti ed è soggetto a disturbi modificanti (funzione del tempo) e interferenti (aleatori)\nPianificare l’esperimento perlustrando il dominio dei due parametri \\(F_1\\) e \\(\\Delta F\\) con un sufficiente numero di test\nEseguire tutte le prove e alla fine scaricare la tabella dei risultati, che non dovrà essere modificata\nPrendere nota della chiave di verifica visualizzata e salvarla su un file. La chiave è unica per ogni sessione sperimentale e per ogni file scaricato\nEseguire la regressione e determinare la relazione \\(\\delta=f(F_1, \\Delta F, a, F_M)\\), con \\(a\\) e \\(F_M\\) parametri del modello\nRealizzare un report .Rmd che descriva tutti i passi e le analisi effettuate e riporti la chiave di verifica\nÈ necessario consegnare il report in formato pdf e il file dati originale. Il report deve citare la chiave di verifica della sessione sperimentale\nIl report deve includere (come chunck con echo=TRUE) anche tutti i passaggi in R (inclusa l’analisi dei residui della regressione!), con la possibile eccezione del caricamento delle librerie\nCriteri di valutazione:\n\ncompletezza dell’analisi\ncorrettezza del risultato\nqualità dell’impaginazione e cura per l’aspetto del report e dei grafici\n\nOpzionale: eseguire un’analisi di bootstrap non parametrico per determinare l’intervallo di confidenza al 95% sui due parametri del modello"
  },
  {
    "objectID": "slides/ADAS/bilancia.html#report",
    "href": "slides/ADAS/bilancia.html#report",
    "title": "Esperimenti di taratura",
    "section": "Report",
    "text": "Report\nPer il report usare il template report.Rmd sulla repository GitHub del corso (cliccare qui per scaricare direttamente il file)\nIl report richiede la libreria memor: per installarla procedere come segue:\n\ninstall.packages(\"devtools\")\ndevtools::install_github(\"hebrewseniorlife/memor\")\n\nSeguire gli esempi e le indicazioni contenute nel template per l’inserimento di figure e tabelle e per la creazione di riferimenti"
  },
  {
    "objectID": "slides/ADAS/kfold.html#loading-the-data",
    "href": "slides/ADAS/kfold.html#loading-the-data",
    "title": "K-Fold Cross Validation",
    "section": "Loading the data",
    "text": "Loading the data\n\n\nWe load some data and make a quick plot\n\nThe relationship seems coarsely proportional\nThere seems to be a flex in the middle\nLet’s try first with a linear model with degree 1"
  },
  {
    "objectID": "slides/ADAS/kfold.html#linear-model-regression",
    "href": "slides/ADAS/kfold.html#linear-model-regression",
    "title": "K-Fold Cross Validation",
    "section": "Linear model regression",
    "text": "Linear model regression\nThe regression of a linear model is performed with the lm() function. It takes two arguments:\n\na formula, i.e. a description of the regression model\na data table, containing the data set to use for regression. The columns of the data set must have the same names used for the predictors\n\nThe formula is expressed in the formula notation, which is a map from an analytical regression model, as \\(y_i = a + bx_i + cx_i^2 + \\varepsilon_i\\) to a formula object as y~x + I(x^2)"
  },
  {
    "objectID": "slides/ADAS/kfold.html#linear-model-regression-1",
    "href": "slides/ADAS/kfold.html#linear-model-regression-1",
    "title": "K-Fold Cross Validation",
    "section": "Linear model regression",
    "text": "Linear model regression\nTo build a formula from a model you typically:\n\ndrop the grand average \\(a\\) and the residuals \\(\\varepsilon_i\\)\nwhen you need the power of a term (or any mathematical function applied to a term like a logarithm), you have to protect it with the identity function I()\nif you have more than one predictor, you can combine them as:\n\ny~x1 + x2, which corresponds to \\(y_i = a + bx_{1,i} + cx_{2,1} + \\varepsilon_i\\)\ny~x1 + x2 + x1:x2, which corresponds to \\(y_i = a + bx_{1,i} + cx_{2,1} + dx_{1,i}x_{2,i} +\\varepsilon_i\\)\n\nthe notation y~x1 + x2 + x1:x2 can be abbreviated as y~x1*x2\n\n\n\nTo remove from the model the grand average (called intercept), subtract 1: \\(y_i = bx_i + cx_i^2 + \\varepsilon_i\\) becomes y~x + I(x^2) - 1"
  },
  {
    "objectID": "slides/ADAS/kfold.html#linear-model-regression-2",
    "href": "slides/ADAS/kfold.html#linear-model-regression-2",
    "title": "K-Fold Cross Validation",
    "section": "Linear model regression",
    "text": "Linear model regression\nSo let’s build a linear model of degree 1 and plot the data with the regression line:\n\n\ndata.lm &lt;-lm(y~x - 1, data=data)\ndata %&gt;% \n  add_predictions(data.lm) %&gt;%\n  ggplot(aes(x=x)) +\n  geom_point(aes(y=y)) +\n  geom_line(aes(y=pred))"
  },
  {
    "objectID": "slides/ADAS/kfold.html#linear-model-regression-3",
    "href": "slides/ADAS/kfold.html#linear-model-regression-3",
    "title": "K-Fold Cross Validation",
    "section": "Linear model regression",
    "text": "Linear model regression\n\nIt is important to look at the residuals. They show a rather strong pattern, meaning that the linear relationship is underfitting the data\nThus we need to increase the degree of the fitting polynomial. But how much so?\n\n\n\ndata %&gt;% \n  add_residuals(data.lm) %&gt;% \n  ggplot(aes(x=x, y=resid)) +\n  geom_point()"
  },
  {
    "objectID": "slides/ADAS/kfold.html#hyper-parameters",
    "href": "slides/ADAS/kfold.html#hyper-parameters",
    "title": "K-Fold Cross Validation",
    "section": "Hyper-parameters",
    "text": "Hyper-parameters\nThe degree of the fitting polynomial is a hyper-parameter\n\nRegression parameters are the coefficients of the polynomial, to be calculated typically by minimizing the root mean square of the residuals\nThe degree of the polynomial is a parameter that defines the number of regression parameters, and that is why it is named a hyper-parameter\nIdentifying the best hyper-parameter(s) is the aim of validation and cross-validation strategies"
  },
  {
    "objectID": "slides/ADAS/kfold.html#multiple-regressions",
    "href": "slides/ADAS/kfold.html#multiple-regressions",
    "title": "K-Fold Cross Validation",
    "section": "Multiple regressions",
    "text": "Multiple regressions\nIn our case we want to compare polynomial fits up to degree 12\nWe use modelr::fit_with() to automate the building of many models together: The function needs two arguments:\n\nthe modeling function, typically lm()\nthe results of a call to formulas(), which in turn takes a list of formulas to be used\n\n\n\nNOTE: The list of formulas needs to have only right hand side formulas, being the first the response variable, the others are the model part combining the predictors"
  },
  {
    "objectID": "slides/ADAS/kfold.html#multiple-regression",
    "href": "slides/ADAS/kfold.html#multiple-regression",
    "title": "K-Fold Cross Validation",
    "section": "Multiple regression",
    "text": "Multiple regression\nLet’s see how it works. We first build a list of arguments:\n\ndeg &lt;- 2:12\nargs &lt;- list(\n  ~y, ~x,\n  map(deg, ~as.formula(paste0(\"~poly(x,\", ., \")\")))\n) %&gt;% \n  unlist()\n\nNow args is a list of formulas like ~y, ~x, ~poly(x, 2), ~poly(x, 3), etc.\n\n\nNOTE: the usage of unlist() at the end: the previous command returns a nested list (a list of lists), and unlist() flattens it into a simple plain list of formulas."
  },
  {
    "objectID": "slides/ADAS/kfold.html#multiple-regression-1",
    "href": "slides/ADAS/kfold.html#multiple-regression-1",
    "title": "K-Fold Cross Validation",
    "section": "Multiple regression",
    "text": "Multiple regression\nNow the formulas() function wants n parameters, each being a formula, while we have a list of formulas. We can solve this problem by using do.call() function, which calls a given function passing each element of a list as a separate argument:\n\nfits &lt;- data %&gt;% fit_with(\n  lm, \n  .formulas=do.call(formulas, args)\n)\n\nNow fits is a list or regression results, with polynomials from degree 1 to 12"
  },
  {
    "objectID": "slides/ADAS/kfold.html#quality-of-regression",
    "href": "slides/ADAS/kfold.html#quality-of-regression",
    "title": "K-Fold Cross Validation",
    "section": "Quality of regression",
    "text": "Quality of regression\nQuality of a regression can be verified with different metrics:\n\n\\(R^2=1-\\frac{\\sum (x_i - \\hat x_i)^2}{\\sum (x_i - \\bar x_i)^2}\\)\n\\(\\mathrm{MSE}=\\frac{\\sum(x_i - \\hat x_i)^2}{N}\\)\n\\(\\mathrm{RMSE}=\\sqrt{\\frac{\\sum(x_i - \\hat x_i)^2}{N}}\\)\n\\(\\mathrm{MAE}=\\frac{\\sum |x_i - \\hat x_i|}{N}\\)\n\\(\\mathrm{MAPE}=\\frac{1}{N}\\sum \\left|\\frac{x_i - \\hat x_i}{x_i}\\right|\\)\n\nTypically, the root means square of error (RMSE) and the mean absolute error (MAE) are the most commonly used metrics"
  },
  {
    "objectID": "slides/ADAS/kfold.html#quality-of-regression-1",
    "href": "slides/ADAS/kfold.html#quality-of-regression-1",
    "title": "K-Fold Cross Validation",
    "section": "Quality of regression",
    "text": "Quality of regression\nLet’s see how the RMSE and the \\(R^2\\) metrics change when the polynomial degree increases. To do that we build a table with three columns:\n\nthe degree of the polynomial\nthe \\(R^2\\) value\nthe RMSE value\n\nWe extract these data from the list of linear models above created, fits\nFor each fitted linear model (an entry in fits), the \\(R^2\\) and RMSE can be extracted with the functions rsquare() and rmse(), respectively"
  },
  {
    "objectID": "slides/ADAS/kfold.html#quality-of-regression-2",
    "href": "slides/ADAS/kfold.html#quality-of-regression-2",
    "title": "K-Fold Cross Validation",
    "section": "Quality of regression",
    "text": "Quality of regression\nWe use map_dbl() to map these functions over the list of polynomial degrees. The resulting table is then used to make a plot:\n\ntibble(\n  degree=c(1,deg), # deg starts from 2!\n  rsquare=fits %&gt;% map_dbl(~rsquare(., data)),\n  rmse=fits %&gt;% map_dbl(~rmse(., data))\n) %&gt;% \n  ggplot(aes(x=degree)) +\n  geom_line(aes(y=rmse), color=\"blue\") +\n  geom_line(aes(y=rsquare*4), color=\"red\") +\n  scale_y_continuous(sec.axis = sec_axis(\n    \\(x) scales::rescale(x, from=c(0,4), to=c(0,1)),\n    breaks=seq(0, 1, 0.1),\n    name=\"R squared\"))\n\n\nThe \\(R^2\\) increases pretty quickly and saturates after degree 3. The RMSE decreases sharply and monothonically. It’s hard to figure out the point where overfitting starts"
  },
  {
    "objectID": "slides/ADAS/kfold.html#quality-of-regression-2-output",
    "href": "slides/ADAS/kfold.html#quality-of-regression-2-output",
    "title": "K-Fold Cross Validation",
    "section": "Quality of regression",
    "text": "Quality of regression"
  },
  {
    "objectID": "slides/ADAS/kfold.html#intro",
    "href": "slides/ADAS/kfold.html#intro",
    "title": "K-Fold Cross Validation",
    "section": "Intro",
    "text": "Intro\nTo solve the problem we use K-fold cross validation. It is a regression strategy where we split the dataset into \\(k\\) subsets, or folds, with roughly the same amount of observations. Then:\n\nwe train the model over all the folds together except the first fold, and then we validate the model on the first fold, i.e. we calculate one or more metrics on the validation data\nwe repeat the previous step setting aside each fold, one at a time, and using it for validation, while the remaining folds are used for training\neach fold is used exactly once for validation, exactly \\(k-1\\) times for training\nwe calculate the overall metrics, by calculating the average of the \\(k\\) metrics evaluated for each validation step, or — equivalently — by appliying the above reported equations to the whole set of validation values"
  },
  {
    "objectID": "slides/ADAS/kfold.html#in-r",
    "href": "slides/ADAS/kfold.html#in-r",
    "title": "K-Fold Cross Validation",
    "section": "In R",
    "text": "In R\nIn R, we use the caret library to simplify this process. The caret::train() function performs the folding for a given model: it takes as arguments the model formula, the regression function (in our case lm()), the dataset, and a list of parameters that can be created with the supporting trainControl() function.\nThe trainControl() function is used to define the details on the cross validation strategy to use. In our case we use the repeated K-fold cross validation, named \"repeatedcv\", which repeates a K-fold a given number of times."
  },
  {
    "objectID": "slides/ADAS/kfold.html#in-r-1",
    "href": "slides/ADAS/kfold.html#in-r-1",
    "title": "K-Fold Cross Validation",
    "section": "In R",
    "text": "In R\nIn fact, the folds are defined by randomly sampling the initial dataset, so that the resulting RMSE (or any other metric) is also a random variable. Repeating the K-fold 100 times makes the whole process more robust:\n\nctrl &lt;- trainControl(method = \"repeatedcv\", number=5, repeats=100)\nmodel &lt;- train(y~poly(x,8), data=data, method=\"lm\", trControl=ctrl)\n\nmodel\n\nLinear Regression \n\n25 samples\n 1 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold, repeated 100 times) \nSummary of sample sizes: 20, 20, 21, 20, 19, 20, ... \nResampling results:\n\n  RMSE     Rsquared   MAE    \n  10.3213  0.9241026  6.25129\n\nTuning parameter 'intercept' was held constant at a\n value of TRUE"
  },
  {
    "objectID": "slides/ADAS/kfold.html#evaluate-the-results",
    "href": "slides/ADAS/kfold.html#evaluate-the-results",
    "title": "K-Fold Cross Validation",
    "section": "Evaluate the results",
    "text": "Evaluate the results\nThe model object contains a field named model$results that is a table with all the available performance metrics:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nintercept\nRMSE\nRsquared\nMAE\nRMSESD\nRsquaredSD\nMAESD\n\n\n\n\nTRUE\n10.3213\n0.9241026\n6.25129\n23.37541\n0.1183862\n12.14192\n\n\n\n\n\nNow we want to repeat the K-fold validation over the list of formulas corresponding to the set of polynomials with degrees from 1 to 12. We use again the map() function:\n\nfit_quality &lt;- tibble(\n  degree=c(1,deg),\n  results=map(degree, function(n) {\n    fm &lt;- paste0(\"y~poly(x,\", n, \")\") %&gt;% as.formula()\n    train(fm, data=data, method=\"lm\", trControl=ctrl)$results\n    })\n) %&gt;% \n  unnest(cols=results)\n\n\nNote the unnest() function at the end: the model field $results is actualy a table, so without that function in fit_quality we would get a column results that contains a list of tables. The unnest() function flattens this list of tables in place."
  },
  {
    "objectID": "slides/ADAS/kfold.html#evaluate-the-results-1",
    "href": "slides/ADAS/kfold.html#evaluate-the-results-1",
    "title": "K-Fold Cross Validation",
    "section": "Evaluate the results",
    "text": "Evaluate the results\nNow we can finally make a plot of the metrics as a function of the polynomial degree:\n\n\nfit_quality %&gt;% \n  select(-intercept, -starts_with(\"Rsquared\")) %&gt;% \n  pivot_longer(-degree, names_to = \"metric\") %&gt;% \n  ggplot(aes(x=degree, y=value, group=metric, color=metric)) + \n  geom_line() +\n  geom_point() +\n  scale_y_log10() +\n  scale_x_continuous(breaks=c(1,deg))"
  },
  {
    "objectID": "slides/ADAS/kfold.html#regression",
    "href": "slides/ADAS/kfold.html#regression",
    "title": "K-Fold Cross Validation",
    "section": "Regression",
    "text": "Regression\n\n\nFrom the previous plot we observe that the minima of any metric happens at degree 3:\n\nbelow that value we have underfitting\nabove we have overfitting (i.e. the model is loosing generality)\n\nSo we can finally accept the model \\(y_i=a + bx_i + cx_i^2 + dx_i^3 + \\varepsilon_i\\) (a degree 3 polynomial in \\(x_i\\)):\n\n\nRegressionResiduals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNOTE: The regression plot also includes 95% confidence intervals"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#useful-links",
    "href": "slides/DESANED/2-R-intro.html#useful-links",
    "title": "Introduction to R language",
    "section": "Useful links",
    "text": "Useful links\n\nGNU-R: https://cran.mirror.garr.it/CRAN/\nRStudio: https://posit.co/downloads/\nCheat sheet: https://posit.co/resources/cheatsheets/\nTidyverse: https://tidyverse.org"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#rstudio-environment",
    "href": "slides/DESANED/2-R-intro.html#rstudio-environment",
    "title": "Introduction to R language",
    "section": "RStudio environment",
    "text": "RStudio environment\n\nInstallation: first R, then RStudio\nRStudio works on folders or (better) projects (.Rproj)\nA project also contains settings that are specific and common to files in the folder\nAn RStudio session can operate on a single project\nMultiple sessions can be opened at the same time\nRStudio is a very powerful and complex environment, also suitable for compiling technical reports, articles, books and presentations (like this one)\n\n\n\nNote: These slides provide an embedded R environment similar to RStudio, and useful for quiclky try the code here discussed: click on the terminal icon on the bottom left, or just hit the § key."
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#the-r-language",
    "href": "slides/DESANED/2-R-intro.html#the-r-language",
    "title": "Introduction to R language",
    "section": "The R language",
    "text": "The R language\n\nR is a high-level, declarative, interpreted language with C-like syntax\nR is both a language and an interpreter\nR is a dynamically typed language\nR is used in both script mode and interactive mode\nR began as an open source GNU version of S, a proprietary language for statistical analysis\nRStudio is a proprietary (but free) IDE for R"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#assignments",
    "href": "slides/DESANED/2-R-intro.html#assignments",
    "title": "Introduction to R language",
    "section": "Assignments",
    "text": "Assignments\nEvery language uses variables to store values and objects through an assignment operation:\na &lt;- 1\n# but also\nb = 2\n# however arrow notation is preferred,\n# because it also works like this:\n3 -&gt; c\n# to display the value of a variable:\nc\n# in one fell swoop, assignment and display:\n(d &lt;- \"string\")\nExecuting a command directly provides a result:\n\n12*12\n\n[1] 144\n\n\n\n\nNote the text [1] at the beginning of the output line — it will be clear later"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#types-or-native-classes",
    "href": "slides/DESANED/2-R-intro.html#types-or-native-classes",
    "title": "Introduction to R language",
    "section": "Types, or native classes",
    "text": "Types, or native classes\n\nR has 6+1 native types or classes\n\ncharacter: \"a\", \"string\", 'my text'\nnumeric: 1, 3.1415\ninteger: 1L\nlogical: TRUE, FALSE (or T and F)\ncomplex: 1+4i\nfunction: a function\n(raw: bit sequence)\n\nEach instance is intrinsically a vector\nA scalar is simply a vector of length 1"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#special-values",
    "href": "slides/DESANED/2-R-intro.html#special-values",
    "title": "Introduction to R language",
    "section": "Special values",
    "text": "Special values\n\nThe following special values are defined:\n\nNA: missing value\nNULL: nothing\nInf: Infinite\nNaN: Not a Number (example 0/0)"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#coercion",
    "href": "slides/DESANED/2-R-intro.html#coercion",
    "title": "Introduction to R language",
    "section": "Coercion",
    "text": "Coercion\n\nWhen mixing different types, e.g. into a vector, R transforms them into a common type:\n\n\nc(1L, 7, \"2\")\n\n[1] \"1\" \"7\" \"2\"\n\nc(T, 0)\n\n[1] 1 0\n\nas.numeric(c(\"a\", \"1\"))\n\n[1] NA  1\n\nas.character(c(1, 1.7))\n\n[1] \"1\"   \"1.7\""
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#vectors",
    "href": "slides/DESANED/2-R-intro.html#vectors",
    "title": "Introduction to R language",
    "section": "Vectors",
    "text": "Vectors\n# They are constructed with the c() operator/function:\nv1 &lt;- c(10, 2, 7.5, 3)\n# or with a sequence:\nv2 &lt;- 1:10\n# also with specified pitch:\nv3 &lt;- seq(1, 10, 0.5)\n# Functions are called with parentheses,\n# separating arguments with ,\nNote the output for v3 in this case:\n\n\n [1]  1.0  1.5  2.0  2.5  3.0  3.5  4.0  4.5  5.0  5.5  6.0\n[12]  6.5  7.0  7.5  8.0  8.5  9.0  9.5 10.0\n\n\nThe first element of the first row is the [1] element of the vector, while the first element of the second row is the [12] element. In all, the vector v3 has 19 elements"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#vectors-1",
    "href": "slides/DESANED/2-R-intro.html#vectors-1",
    "title": "Introduction to R language",
    "section": "Vectors",
    "text": "Vectors\nVariables are natively vectors. Scalars are just vectors of dimension 1:\n\na &lt;- 10\nlength(a)\n\n[1] 1\n\nlength(v3)\n\n[1] 19\n\n\nFunctions therefore always act on vectors:\n\na * 2\n\n[1] 20\n\nv3 + 2\n\n [1]  3.0  3.5  4.0  4.5  5.0  5.5  6.0  6.5  7.0  7.5  8.0\n[12]  8.5  9.0  9.5 10.0 10.5 11.0 11.5 12.0"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#introspection",
    "href": "slides/DESANED/2-R-intro.html#introspection",
    "title": "Introduction to R language",
    "section": "Introspection",
    "text": "Introspection\n\nUseful functions for inspecting objects:\n\nmode(): storage mode\nclass(): class (high level, same as mode() for basic types)\ntypeof(): type (low level)\nlength(): vector length\nattributes(): metadata"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#matrices",
    "href": "slides/DESANED/2-R-intro.html#matrices",
    "title": "Introduction to R language",
    "section": "Matrices",
    "text": "Matrices\n\nThey are constructed with the matrix() function\n\n(m1 &lt;- matrix(1:10, 2, 5))\n\nthe array() function constructs n-dimensional arrays\nA matrix is a vector with dim attribute:\n\nattr(m1, \"dim\")\nv &lt;- 1:4\nattr(v, \"dim\") &lt;- c(2,2) # is equivalent to dim(m) &lt;- c(2,2)\nv"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#factors",
    "href": "slides/DESANED/2-R-intro.html#factors",
    "title": "Introduction to R language",
    "section": "Factors",
    "text": "Factors\n\nAn additional (non-base) but very common class is factor\nRepresents categorical variables (ordered or unordered)\n\n\n(vf &lt;- factor(LETTERS[1:5], levels=LETTERS[c(2, 1, 3, 5, 4)], ordered=T))\n\n[1] A B C D E\nLevels: B &lt; A &lt; C &lt; E &lt; D\n\nclass(vf)\n\n[1] \"ordered\" \"factor\" \n\ntypeof(vf)\n\n[1] \"integer\"\n\nvf[1] &lt; vf[3]\n\n[1] TRUE"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#strings",
    "href": "slides/DESANED/2-R-intro.html#strings",
    "title": "Introduction to R language",
    "section": "Strings",
    "text": "Strings\nA string can be thought of as an array of characters with a length greater than 1.\nThe most common string manipulation functions are cat(), paste(), and paste0(). The first is used to print the string as it is:\n\ncat(\"Hello!\")\n\nHello!\n\n\nThe two functions paste() and paste0() are used to join two or more strings, the first inserting a space in between, the second without space:\n\npaste(\"Hello,\", \"World!\")\n\n[1] \"Hello, World!\"\n\npaste0(\"Hello,\", \"World!\")\n\n[1] \"Hello,World!\""
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#indexing",
    "href": "slides/DESANED/2-R-intro.html#indexing",
    "title": "Introduction to R language",
    "section": "Indexing",
    "text": "Indexing\n\nR’s indexing syntax is very flexible and powerful\nas for vectors, square brackets [r,c] are used, the base is 1\nif an index is missing, it means “all rows|columns”\n\n\nv3[3]\n\n[1] 2\n\nm1[1,1]\n\n[1] 1\n\nm1[2,]\n\n[1]  2  4  6  8 10\n\nm1[,]\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    3    5    7    9\n[2,]    2    4    6    8   10"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#indexing-1",
    "href": "slides/DESANED/2-R-intro.html#indexing-1",
    "title": "Introduction to R language",
    "section": "Indexing",
    "text": "Indexing\n\nAn index can also be a vector of positions or a vector of Boolean values\n\n\nv1[c(2,4,1)] # extracts only elements 2, 4, and 1\n\n[1]  2  3 10\n\nv2[v2 %% 2 == 0] # extract elements divisible by 2\n\n[1]  2  4  6  8 10\n\n\nThe second case works thanks to the modulus operator:\n\nv2 %% 2 == 0 # modulus operator (remainder)\n\n [1] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE\n[10]  TRUE\n\n\n\n\nNote: TRUE and FALSE can be abbreviated to T and F"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#functions",
    "href": "slides/DESANED/2-R-intro.html#functions",
    "title": "Introduction to R language",
    "section": "Functions",
    "text": "Functions\n\nFunctions are first class objects, that is, they are valid types\ncan be assigned to variables and passed to other functions\n\n\nmy_fun &lt;- function(x) x^2\nmy_fun(1:5)\n\n[1]  1  4  9 16 25\n\nyour_fun &lt;- my_fun\nyour_fun(6)\n\n[1] 36\n\nmy_apply &lt;- function(x, f) f(x)\nmy_apply(10, my_fun)\n\n[1] 100\n\n\n\nIf the definition requires multiple lines, a block is used between {}\nEach function always returns the last evaluated expression\nOr explicitly via return()"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#arrow-functions-replacement-functions",
    "href": "slides/DESANED/2-R-intro.html#arrow-functions-replacement-functions",
    "title": "Introduction to R language",
    "section": "Arrow functions (replacement functions)",
    "text": "Arrow functions (replacement functions)\n\nWe’ve seen things like dim(v) &lt;- c(2,3): how do you declare them?\n\n\n`pwr&lt;-` &lt;- function(obj, value) obj ** value\na &lt;- 2\npwr(a) &lt;- 10\na\n\n[1] 1024\n\n\n\nThe last argument must be called value and represents the right side of the assignment!"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#flow-control",
    "href": "slides/DESANED/2-R-intro.html#flow-control",
    "title": "Introduction to R language",
    "section": "Flow control",
    "text": "Flow control\nR supports typical flow control instructions\n\nfor conditional statements:\n\nif(cond) expr\nif(cond) true.expr else false.expr\niffelse(cond, true.expr, false.expr)\n\nand for cycles:\n\nfor(var in seq) expr\nwhile(cond) expr\nrepeat expr\nbreak\nnext"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#function-arguments",
    "href": "slides/DESANED/2-R-intro.html#function-arguments",
    "title": "Introduction to R language",
    "section": "Function arguments",
    "text": "Function arguments\n\nTopics can be indicated by position or name\nNamed topics can appear in any order\nArguments may have a default, in which case they are optional\n\n\nf &lt;- function(x, y, n=10, test=F) {\n   ifelse(test, 0, x^y + n)\n}\nf(2, 10)\n\n[1] 1034\n\nf(test=F, y=10, x=2)\n\n[1] 1034\n\nf(test=T)\n\n[1] 0"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#difference-between---and",
    "href": "slides/DESANED/2-R-intro.html#difference-between---and",
    "title": "Introduction to R language",
    "section": "Difference between <- and =",
    "text": "Difference between &lt;- and =\n\nThe = operator as an assignment is valid only at the top-level\nThe &lt;- operator is valid everywhere, even as a function argument:\n\n\nsystem.time(m &lt;- mean(1:1E6))\n\n   user  system elapsed \n  0.002   0.000   0.002 \n\nm\n\n[1] 500000.5"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#dataframes",
    "href": "slides/DESANED/2-R-intro.html#dataframes",
    "title": "Introduction to R language",
    "section": "Dataframes",
    "text": "Dataframes\n\nIn R, dataframes are used rather than matrices\nThese are tables organized by columns, internally homogeneous but potentially of different types\n\n\ndf &lt;- data.frame(A=1:10, B=letters[1:10])\nhead(df)\n\n  A B\n1 1 a\n2 2 b\n3 3 c\n4 4 d\n5 5 e\n6 6 f"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#dataframes-1",
    "href": "slides/DESANED/2-R-intro.html#dataframes-1",
    "title": "Introduction to R language",
    "section": "Dataframes",
    "text": "Dataframes\n\nA dataframe can be indexed as an array (two indices)\nOr by selecting a column with the notation $\n\n\ndf[2,2]\n\n[1] \"b\"\n\ndf$B[2]\n\n[1] \"b\"\n\n\nAlso in assignment:\n\ndf$C &lt;- LETTERS[1:10]\nhead(df, 3)\n\n  A B C\n1 1 a A\n2 2 b B\n3 3 c C"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#lists",
    "href": "slides/DESANED/2-R-intro.html#lists",
    "title": "Introduction to R language",
    "section": "Lists",
    "text": "Lists\nA list is a sequence of key-value pairs, that is, a sequence of values identified by a name, or key.\nUnlike vectors (which are always homogeneous) they can contain heterogeneous values.\n\n(l &lt;- list(A=\"one\", B=\"two\", C=1:4))\n\n$A\n[1] \"one\"\n\n$B\n[1] \"two\"\n\n$C\n[1] 1 2 3 4\n\n\nA list can be indexed in three ways:\n\nwith the $ operator: extracts a single element by name\nwith the [] operator: extract elements by position and obtain a list\nwith the [[]] operator: extracts a single element per position"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#commonly-used-algorithms",
    "href": "slides/DESANED/2-R-intro.html#commonly-used-algorithms",
    "title": "Introduction to R language",
    "section": "Commonly used algorithms",
    "text": "Commonly used algorithms\n\nSorting: sort, rev, order\nSampling: sample, expand.grid\nAggregation: by, aggregate\nContingency tables: table"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#sorting-vectors",
    "href": "slides/DESANED/2-R-intro.html#sorting-vectors",
    "title": "Introduction to R language",
    "section": "Sorting vectors",
    "text": "Sorting vectors\nTo sort a vector you use the sort function:\n\nv &lt;- runif(5, 1, 10)\nsort(v)\n\n[1] 3.389578 4.349115 6.155680 9.070275 9.173870\n\nrev(sort(v))\n\n[1] 9.173870 9.070275 6.155680 4.349115 3.389578\n\nsort(v, decreasing = T)\n\n[1] 9.173870 9.070275 6.155680 4.349115 3.389578"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#sorting-dataframes",
    "href": "slides/DESANED/2-R-intro.html#sorting-dataframes",
    "title": "Introduction to R language",
    "section": "Sorting dataframes",
    "text": "Sorting dataframes\nTo reorder a data frame, the ordered indices are extracted:\n\ndf &lt;- data.frame(A=1:5, B=runif(5))\ndf[order(df$B),]\n\n  A         B\n1 1 0.2016819\n5 5 0.6291140\n4 4 0.6607978\n2 2 0.8983897\n3 3 0.9446753\n\n\nThe order function returns the indices of a vector ordered according to the values:\n\norder(df$B)\n\n[1] 1 5 4 2 3\n\n\nwhere the first is the index of the smallest value of df$B and the last is the index of the largest"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#sampling",
    "href": "slides/DESANED/2-R-intro.html#sampling",
    "title": "Introduction to R language",
    "section": "Sampling",
    "text": "Sampling\nSampling a set of data (a vector) means extracting a subset (called sample) of values randomly. This is done with the sample function:\n\nsample(1:10) # without reinsertion\n\n [1]  2  3  1  5  7 10  6  4  9  8\n\nsample(1:10, replace = T) # with reinsertion\n\n [1]  9  5  5  9  9  5  5  2 10  9\n\n\nThe sample size can be equal (case above) or smaller than the initial set:\n\nsample(1:10, size = 5)\n\n[1] 1 4 3 6 2\n\nsample(10) # random integer generation without repetition\n\n [1] 10  6  7  4  8  9  2  1  3  5"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#grids",
    "href": "slides/DESANED/2-R-intro.html#grids",
    "title": "Introduction to R language",
    "section": "Grids",
    "text": "Grids\nA grid is a matrix that contains all (ordered) combinations between \\(n\\) vectors of possibly different sizes. In R it is represented as a data frame and constructed with the expand.grid function:\n\n(df &lt;- expand.grid(A=1:2, B=c(\"-\", \"+\"), D=c(\"a\", \"b\", \"c\")))\n\n   A B D\n1  1 - a\n2  2 - a\n3  1 + a\n4  2 + a\n5  1 - b\n6  2 - b\n7  1 + b\n8  2 + b\n9  1 - c\n10 2 - c\n11 1 + c\n12 2 + c"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#aggregation",
    "href": "slides/DESANED/2-R-intro.html#aggregation",
    "title": "Introduction to R language",
    "section": "Aggregation",
    "text": "Aggregation\nAggregation means grouping rows having common elements in a data frame and applying a given function to each group. It is useful for example for calculating sub-totals.\nIn R it can be done using the by function or the aggregate function (changes the output type):\n\nby(df$A, INDICES = df$B, FUN=sum)\n\ndf$B: -\n[1] 9\n--------------------------------------------- \ndf$B: +\n[1] 9\n\naggregate(A~B, data = df, FUN = sum)\n\n  B A\n1 - 9\n2 + 9"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#contingency-tables",
    "href": "slides/DESANED/2-R-intro.html#contingency-tables",
    "title": "Introduction to R language",
    "section": "Contingency tables",
    "text": "Contingency tables\nA contingency table counts the occurrences between a pair of columns in a data frame:\n\nhead(airquality, n = 3)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n\nwith(airquality, table(OzHi = Ozone &gt; 80, Month,\n                        useNA = \"ifany\"))\n\n       Month\nOzHi     5  6  7  8  9\n  FALSE 25  9 20 19 27\n  TRUE   1  0  6  7  2\n  &lt;NA&gt;   5 21  5  5  1\n\n\nNOTE: with() is to save you from typing airquality$Ozone and airquality$Month"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#contingency-tables-1",
    "href": "slides/DESANED/2-R-intro.html#contingency-tables-1",
    "title": "Introduction to R language",
    "section": "Contingency tables",
    "text": "Contingency tables\n\nAlso useful is tapply(), which operates on a table similarly to aggregate functions:\n\n\nround(with(airquality,\n            tapply(Ozone, Month, mean, na.rm=T)), 1)\n\n   5    6    7    8    9 \n23.6 29.4 59.1 60.0 31.4 \n\n\nWith aggregate() you would do:\n\naggregate(Ozone~Month, data=airquality, FUN=mean, ra.rm=T)\n\n  Month    Ozone\n1     5 23.61538\n2     6 29.44444\n3     7 59.11538\n4     8 59.96154\n5     9 31.44828"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#inputoutput-to-file",
    "href": "slides/DESANED/2-R-intro.html#inputoutput-to-file",
    "title": "Introduction to R language",
    "section": "Input/output to file",
    "text": "Input/output to file\nSince statistics generally deals with large quantities of data, it is essential to be able to import and export data in generic formats.\nData is generally presented in tabular form (by rows and columns)\nThe simplest and most common formats are:\n\nFlat File: an ASCII text file containing row and column values; columns can be separated\n\nfixed length\nusing separator characters\n\nCSV (Comma-Separated Values): a special version of FF where column fields are separated by commas"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#input-from-file",
    "href": "slides/DESANED/2-R-intro.html#input-from-file",
    "title": "Introduction to R language",
    "section": "Input from file",
    "text": "Input from file\nA flat file with fields separated by spaces can look like this:\n# Data collected on 8/10/2023\nx y z\n1.2 3.7 2.7\n2.1 2.5 3.9\n3.8 2.2 6.8\nSuch a file can be imported as a data frame like this:\ndf &lt;- read.table(\"data_file.txt\", header=T, sep=\" \", comment.char=\"#\")\nThe read.table() function has numerous options that allow you to handle all possible cases in which files contain fields separated by specific characters (spaces or other)"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#input-from-file-1",
    "href": "slides/DESANED/2-R-intro.html#input-from-file-1",
    "title": "Introduction to R language",
    "section": "Input from file",
    "text": "Input from file\nA flat file with fixed width fields can look like this:\n# Data collected on 8/10/2023\nx y z\n1.2 3.7 2.7\n2.1 2.5 3.9\n3.8 2.2 6.8\nSuch a file can be imported as a data frame like this:\ndf &lt;- read.fwf(\"data_file.txt\", widths=5, header=T, skip=1)\nThe skip=1 parameter requires skipping the first line (comment)"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#input-from-csv-file",
    "href": "slides/DESANED/2-R-intro.html#input-from-csv-file",
    "title": "Introduction to R language",
    "section": "Input from CSV file",
    "text": "Input from CSV file\nCSV files are special FFs where the field separator is the comma. In these cases we use the read.csv() function which works similarly to read.table() but does not require specifying the separator.\nA CSV looks like this:\n# Data collected on 8/10/2023\nx,y,z\n1.2,3.7,2.7\n2.1,2.5,3.9\n3.8,2.2,6.8\nSoftware that uses Latin languages (Italian, Spanish, Portuguese and French) adopts the comma as decimal separator. Consequently, when these software (e.g. MS Excel) generate CSVs they use the semicolon as a field separator.\nIn this case from R it is necessary to use the read.csv2() function, which takes the comma as decimal separator and the semicolon as field separator."
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#output-to-file",
    "href": "slides/DESANED/2-R-intro.html#output-to-file",
    "title": "Introduction to R language",
    "section": "Output to file",
    "text": "Output to file\nThe opposite of importing a file to a data frame is exporting a data frame to a file.\nThis operation is performed with the opposite functions to the previous ones:\n\nwrite.table()\nwrite.fwf()\nwrite.csv() and write.csv2()\n\nAll these functions have two mandatory arguments: the data frame to save and the destination file:\nwrite.csv(df, \"data.csv\")\nOther optional arguments are used to customize the result."
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#tidyverse",
    "href": "slides/DESANED/2-R-intro.html#tidyverse",
    "title": "Introduction to R language",
    "section": "Tidyverse",
    "text": "Tidyverse\nAlong with RStudio, a new wave of R libraries has emerged that radically changes the approach. They go by the collective name of tidyverse\n\nggplot2: plots\npurrr: functional programming\ndplyr: data manipulation\nstringr: string manipulation"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#tidyverse-1",
    "href": "slides/DESANED/2-R-intro.html#tidyverse-1",
    "title": "Introduction to R language",
    "section": "Tidyverse",
    "text": "Tidyverse\nAlong with RStudio, a new wave of R libraries has emerged that radically changes the approach. They go by the collective name of tidyverse\n\ntibble: Improved data frames\nreadr: import data\ntidyr: data preparation\nlubridate: date manipulation"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#tidyverse-2",
    "href": "slides/DESANED/2-R-intro.html#tidyverse-2",
    "title": "Introduction to R language",
    "section": "Tidyverse",
    "text": "Tidyverse\nThe tidyverse approach has some common characteristics:\n\ndata in tidy format (one observation per row, one variable, or observing, per column)\ncomposition of graph functions with + (ggplot(...) + geom_line()), each function is a layer\nprefix notation with %&gt;% (a %&gt;% str() instead of str(a))\n\nIt is useful to consult the cheat sheets: https://posit.co/resources/cheatsheets/"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#infix-notation",
    "href": "slides/DESANED/2-R-intro.html#infix-notation",
    "title": "Introduction to R language",
    "section": "Infix notation",
    "text": "Infix notation\n# I create the histogram of a sample of 10 elements from 100 random numbers\n# infix:\nhist(sample(rnorm(100), 10))\nHardly readable, the first step of the algorithm is the internal one"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#infix-notation-sequenced",
    "href": "slides/DESANED/2-R-intro.html#infix-notation-sequenced",
    "title": "Introduction to R language",
    "section": "Infix notation, sequenced",
    "text": "Infix notation, sequenced\n# I create the histogram of a sample of 10 elements from 100 random numbers\n# infix:\nhist(sample(rnorm(100), 10))\n\n# sequenced infix:\ns &lt;- rnorm(100)\nc &lt;- sample(s, 10)\nhist(c)\nMore readable, the algorithm is more obvious, but requires the creation of intermediate variables"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#prefix-notation",
    "href": "slides/DESANED/2-R-intro.html#prefix-notation",
    "title": "Introduction to R language",
    "section": "Prefix notation",
    "text": "Prefix notation\n# I create the histogram of a sample of 10 elements from 100 random numbers\n# infix:\nhist(sample(rnorm(100), 10))\n\n# sequenced infix:\ns &lt;- rnorm(100)\nc &lt;- sample(s, 10)\nhist(c)\n\n# prefix with pipe:\nrnorm(100) %&gt;% sample(10) %&gt;% hist()\nMuch more readable, the sequential algorithm is obvious, no intermediate variables are needed"
  },
  {
    "objectID": "slides/DESANED/2-R-intro.html#prefix-notation-1",
    "href": "slides/DESANED/2-R-intro.html#prefix-notation-1",
    "title": "Introduction to R language",
    "section": "Prefix notation",
    "text": "Prefix notation\n# I create the histogram of a sample of 10 elements from 100 random numbers\n# infix:\nhist(sample(rnorm(100), 10))\n\n# sequenced infix:\ns &lt;- rnorm(100)\nc &lt;- sample(s, 10)\nhist(c)\n\n# prefix with pipe:\nrnorm(100) %&gt;% sample(10) %&gt;% hist()\n\n# even on multiple lines:\nrnorm(100) %&gt;%\n1   sample(10) %&gt;%\n2   hist\n\n1\n\nthe lines following the first must be indented\n\n2\n\nonly when using pipe, if there are no arguments the parentheses are optional"
  },
  {
    "objectID": "slides/DESANED/4-kfold.html#loading-the-data",
    "href": "slides/DESANED/4-kfold.html#loading-the-data",
    "title": "K-Fold Cross Validation",
    "section": "Loading the data",
    "text": "Loading the data\n\n\nWe load some data and make a quick plot\n\nThe relationship seems coarsely proportional\nThere seems to be a flex in the middle\nLet’s try first with a linear model with degree 1"
  },
  {
    "objectID": "slides/DESANED/4-kfold.html#linear-model-regression",
    "href": "slides/DESANED/4-kfold.html#linear-model-regression",
    "title": "K-Fold Cross Validation",
    "section": "Linear model regression",
    "text": "Linear model regression\nThe regression of a linear model is performed with the lm() function. It takes two arguments:\n\na formula, i.e. a description of the regression model\na data table, containing the data set to use for regression. The columns of the data set must have the same names used for the predictors\n\nThe formula is expressed in the formula notation, which is a map from an analytical regression model, as \\(y_i = a + bx_i + cx_i^2 + \\varepsilon_i\\) to a formula object as y~x + I(x^2)"
  },
  {
    "objectID": "slides/DESANED/4-kfold.html#linear-model-regression-1",
    "href": "slides/DESANED/4-kfold.html#linear-model-regression-1",
    "title": "K-Fold Cross Validation",
    "section": "Linear model regression",
    "text": "Linear model regression\nTo build a formula from a model you typically:\n\ndrop the grand average \\(a\\) and the residuals \\(\\varepsilon_i\\)\nwhen you need the power of a term (or any mathematical function applied to a term like a logarithm), you have to protect it with the identity function I()\nif you have more than one predictor, you can combine them as:\n\ny~x1 + x2, which corresponds to \\(y_i = a + bx_{1,i} + cx_{2,1} + \\varepsilon_i\\)\ny~x1 + x2 + x1:x2, which corresponds to \\(y_i = a + bx_{1,i} + cx_{2,1} + dx_{1,i}x_{2,i} +\\varepsilon_i\\)\n\nthe notation y~x1 + x2 + x1:x2 can be abbreviated as y~x1*x2\n\n\n\nTo remove from the model the grand average (called intercept), subtract 1: \\(y_i = bx_i + cx_i^2 + \\varepsilon_i\\) becomes y~x + I(x^2) - 1"
  },
  {
    "objectID": "slides/DESANED/4-kfold.html#linear-model-regression-2",
    "href": "slides/DESANED/4-kfold.html#linear-model-regression-2",
    "title": "K-Fold Cross Validation",
    "section": "Linear model regression",
    "text": "Linear model regression\nSo let’s build a linear model of degree 1 and plot the data with the regression line:\n\n\ndata.lm &lt;-lm(y~x - 1, data=data)\ndata %&gt;% \n  add_predictions(data.lm) %&gt;%\n  ggplot(aes(x=x)) +\n  geom_point(aes(y=y)) +\n  geom_line(aes(y=pred))"
  },
  {
    "objectID": "slides/DESANED/4-kfold.html#linear-model-regression-3",
    "href": "slides/DESANED/4-kfold.html#linear-model-regression-3",
    "title": "K-Fold Cross Validation",
    "section": "Linear model regression",
    "text": "Linear model regression\n\nIt is important to look at the residuals. They show a rather strong pattern, meaning that the linear relationship is underfitting the data\nThus we need to increase the degree of the fitting polynomial. But how much so?\n\n\n\ndata %&gt;% \n  add_residuals(data.lm) %&gt;% \n  ggplot(aes(x=x, y=resid)) +\n  geom_point()"
  },
  {
    "objectID": "slides/DESANED/4-kfold.html#hyper-parameters",
    "href": "slides/DESANED/4-kfold.html#hyper-parameters",
    "title": "K-Fold Cross Validation",
    "section": "Hyper-parameters",
    "text": "Hyper-parameters\nThe degree of the fitting polynomial is a hyper-parameter\n\nRegression parameters are the coefficients of the polynomial, to be calculated typically by minimizing the root mean square of the residuals\nThe degree of the polynomial is a parameter that defines the number of regression parameters, and that is why it is named a hyper-parameter\nIdentifying the best hyper-parameter(s) is the aim of validation and cross-validation strategies"
  },
  {
    "objectID": "slides/DESANED/4-kfold.html#multiple-regressions",
    "href": "slides/DESANED/4-kfold.html#multiple-regressions",
    "title": "K-Fold Cross Validation",
    "section": "Multiple regressions",
    "text": "Multiple regressions\nIn our case we want to compare polynomial fits up to degree 12\nWe use modelr::fit_with() to automate the building of many models together: The function needs two arguments:\n\nthe modeling function, typically lm()\nthe results of a call to formulas(), which in turn takes a list of formulas to be used\n\n\n\nNOTE: The list of formulas needs to have only right hand side formulas, being the first the response variable, the others are the model part combining the predictors"
  },
  {
    "objectID": "slides/DESANED/4-kfold.html#multiple-regression",
    "href": "slides/DESANED/4-kfold.html#multiple-regression",
    "title": "K-Fold Cross Validation",
    "section": "Multiple regression",
    "text": "Multiple regression\nLet’s see how it works. We first build a list of arguments:\n\ndeg &lt;- 2:12\nargs &lt;- list(\n  ~y, ~x,\n  map(deg, ~as.formula(paste0(\"~poly(x,\", ., \")\")))\n) %&gt;% \n  unlist()\n\nNow args is a list of formulas like ~y, ~x, ~poly(x, 2), ~poly(x, 3), etc.\n\n\nNOTE: the usage of unlist() at the end: the previous command returns a nested list (a list of lists), and unlist() flattens it into a simple plain list of formulas."
  },
  {
    "objectID": "slides/DESANED/4-kfold.html#multiple-regression-1",
    "href": "slides/DESANED/4-kfold.html#multiple-regression-1",
    "title": "K-Fold Cross Validation",
    "section": "Multiple regression",
    "text": "Multiple regression\nNow the formulas() function wants n parameters, each being a formula, while we have a list of formulas. We can solve this problem by using do.call() function, which calls a given function passing each element of a list as a separate argument:\n\nfits &lt;- data %&gt;% fit_with(\n  lm, \n  .formulas=do.call(formulas, args)\n)\n\nNow fits is a list or regression results, with polynomials from degree 1 to 12"
  },
  {
    "objectID": "slides/DESANED/4-kfold.html#quality-of-regression",
    "href": "slides/DESANED/4-kfold.html#quality-of-regression",
    "title": "K-Fold Cross Validation",
    "section": "Quality of regression",
    "text": "Quality of regression\nQuality of a regression can be verified with different metrics:\n\n\\(R^2=1-\\frac{\\sum (x_i - \\hat x_i)^2}{\\sum (x_i - \\bar x_i)^2}\\)\n\\(\\mathrm{MSE}=\\frac{\\sum(x_i - \\hat x_i)^2}{N}\\)\n\\(\\mathrm{RMSE}=\\sqrt{\\frac{\\sum(x_i - \\hat x_i)^2}{N}}\\)\n\\(\\mathrm{MAE}=\\frac{\\sum |x_i - \\hat x_i|}{N}\\)\n\\(\\mathrm{MAPE}=\\frac{1}{N}\\sum \\left|\\frac{x_i - \\hat x_i}{x_i}\\right|\\)\n\nTypically, the root means square of error (RMSE) and the mean absolute error (MAE) are the most commonly used metrics"
  },
  {
    "objectID": "slides/DESANED/4-kfold.html#quality-of-regression-1",
    "href": "slides/DESANED/4-kfold.html#quality-of-regression-1",
    "title": "K-Fold Cross Validation",
    "section": "Quality of regression",
    "text": "Quality of regression\nLet’s see how the RMSE and the \\(R^2\\) metrics change when the polynomial degree increases. To do that we build a table with three columns:\n\nthe degree of the polynomial\nthe \\(R^2\\) value\nthe RMSE value\n\nWe extract these data from the list of linear models above created, fits\nFor each fitted linear model (an entry in fits), the \\(R^2\\) and RMSE can be extracted with the functions rsquare() and rmse(), respectively"
  },
  {
    "objectID": "slides/DESANED/4-kfold.html#quality-of-regression-2",
    "href": "slides/DESANED/4-kfold.html#quality-of-regression-2",
    "title": "K-Fold Cross Validation",
    "section": "Quality of regression",
    "text": "Quality of regression\nWe use map_dbl() to map these functions over the list of polynomial degrees. The resulting table is then used to make a plot:\n\ntibble(\n  degree=c(1,deg), # deg starts from 2!\n  rsquare=fits %&gt;% map_dbl(~rsquare(., data)),\n  rmse=fits %&gt;% map_dbl(~rmse(., data))\n) %&gt;% \n  ggplot(aes(x=degree)) +\n  geom_line(aes(y=rmse), color=\"blue\") +\n  geom_line(aes(y=rsquare*4), color=\"red\") +\n  scale_y_continuous(sec.axis = sec_axis(\n    \\(x) scales::rescale(x, from=c(0,4), to=c(0,1)),\n    breaks=seq(0, 1, 0.1),\n    name=\"R squared\"))\n\n\nThe \\(R^2\\) increases pretty quickly and saturates after degree 3. The RMSE decreases sharply and monothonically. It’s hard to figure out the point where overfitting starts"
  },
  {
    "objectID": "slides/DESANED/4-kfold.html#quality-of-regression-2-output",
    "href": "slides/DESANED/4-kfold.html#quality-of-regression-2-output",
    "title": "K-Fold Cross Validation",
    "section": "Quality of regression",
    "text": "Quality of regression"
  },
  {
    "objectID": "slides/DESANED/4-kfold.html#intro",
    "href": "slides/DESANED/4-kfold.html#intro",
    "title": "K-Fold Cross Validation",
    "section": "Intro",
    "text": "Intro\nTo solve the problem we use K-fold cross validation. It is a regression strategy where we split the dataset into \\(k\\) subsets, or folds, with roughly the same amount of observations. Then:\n\nwe train the model over all the folds together except the first fold, and then we validate the model on the first model, i.e. we calculate one or more metrics on the validation data\nwe repeat the previous step setting aside each fold, one at a time, and using it for validation, while the remaining folds are used for training\neach fold is used exactly once for validation, exactly \\(k-1\\) times for training\nwe calculate the overall metrics, by calculating the average of the \\(k\\) metrics evaluated for each validation step, or — equivalently — by appliying the above reported equations to the whole set of validation values"
  },
  {
    "objectID": "slides/DESANED/4-kfold.html#in-r",
    "href": "slides/DESANED/4-kfold.html#in-r",
    "title": "K-Fold Cross Validation",
    "section": "In R",
    "text": "In R\nIn R, we use the caret library to simplify this process. The caret::train() function performs the folding for a given model: it takes as arguments the model formula, the regression function (in our case lm()), the dataset, and a list of parameters that can be created with the supporting trainControl() function.\nThe trainControl() function is used to define the details on the cross validation strategy to use. In our case we use the repeated K-fold cross validation, named \"repeatedcv\", which repeates a K-fold a given number of times."
  },
  {
    "objectID": "slides/DESANED/4-kfold.html#in-r-1",
    "href": "slides/DESANED/4-kfold.html#in-r-1",
    "title": "K-Fold Cross Validation",
    "section": "In R",
    "text": "In R\nIn fact, the folds are defined by randomly sampling the initial dataset, so that the resulting RMSE (or any other metric) is also a random variable. Repeating the K-fold 100 times makes the whole process more robust:\n\nctrl &lt;- trainControl(method = \"repeatedcv\", number=5, repeats=100)\nmodel &lt;- train(y~poly(x,8), data=data, method=\"lm\", trControl=ctrl)\n\nmodel\n\nLinear Regression \n\n25 samples\n 1 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold, repeated 100 times) \nSummary of sample sizes: 20, 20, 21, 20, 19, 20, ... \nResampling results:\n\n  RMSE     Rsquared   MAE    \n  10.3213  0.9241026  6.25129\n\nTuning parameter 'intercept' was held constant at a\n value of TRUE"
  },
  {
    "objectID": "slides/DESANED/4-kfold.html#evaluate-the-results",
    "href": "slides/DESANED/4-kfold.html#evaluate-the-results",
    "title": "K-Fold Cross Validation",
    "section": "Evaluate the results",
    "text": "Evaluate the results\nThe model object contains a field named model$results that is a table with all the available performance metrics:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nintercept\nRMSE\nRsquared\nMAE\nRMSESD\nRsquaredSD\nMAESD\n\n\n\n\nTRUE\n10.3213\n0.9241026\n6.25129\n23.37541\n0.1183862\n12.14192\n\n\n\n\n\nNow we want to repeat the K-fold validation over the list of formulas corresponding to the set of polynomials with degrees from 1 to 12. We use again the map() function:\n\nfit_quality &lt;- tibble(\n  degree=c(1,deg),\n  results=map(degree, function(n) {\n    fm &lt;- paste0(\"y~poly(x,\", n, \")\") %&gt;% as.formula()\n    train(fm, data=data, method=\"lm\", trControl=ctrl)$results\n    })\n) %&gt;% \n  unnest(cols=results)\n\n\nNote the unnest() function at the end: the model field $results is actualy a table, so without that function in fit_quality we would get a column results that contains a list of tables. The unnest() function flattens this list of tables in place."
  },
  {
    "objectID": "slides/DESANED/4-kfold.html#evaluate-the-results-1",
    "href": "slides/DESANED/4-kfold.html#evaluate-the-results-1",
    "title": "K-Fold Cross Validation",
    "section": "Evaluate the results",
    "text": "Evaluate the results\nNow we can finally make a plot of the metrics as a function of the polynomial degree:\n\n\nfit_quality %&gt;% \n  select(-intercept, -starts_with(\"Rsquared\")) %&gt;% \n  pivot_longer(-degree, names_to = \"metric\") %&gt;% \n  ggplot(aes(x=degree, y=value, group=metric, color=metric)) + \n  geom_line() +\n  geom_point() +\n  scale_y_log10() +\n  scale_x_continuous(breaks=c(1,deg))"
  },
  {
    "objectID": "slides/DESANED/4-kfold.html#regression",
    "href": "slides/DESANED/4-kfold.html#regression",
    "title": "K-Fold Cross Validation",
    "section": "Regression",
    "text": "Regression\n\n\nFrom the previous plot we observe that the minima of any metric happens at degree 3:\n\nbelow that value we have underfitting\nabove we have overfitting (i.e. the model is loosing generality)\n\nSo we can finally accept the model \\(y_i=a + bx_i + cx_i^2 + dx_i^3 + \\varepsilon_i\\) (a degree 3 polynomial in \\(x_i\\)):\n\n\nRegressionResiduals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNOTE: The regression plot also includes 95% confidence intervals"
  },
  {
    "objectID": "slides/DESANED/index.html",
    "href": "slides/DESANED/index.html",
    "title": "Design of Experiments and Statistical Analysis of Experimental Data",
    "section": "",
    "text": "Course offered to students of Space and Science Technology PhD programme."
  },
  {
    "objectID": "slides/DESANED/index.html#contents",
    "href": "slides/DESANED/index.html#contents",
    "title": "Design of Experiments and Statistical Analysis of Experimental Data",
    "section": "Contents:",
    "text": "Contents:\n\nIntro to Statistics\nIntro to R language\nRegression\nK-fold cross validation\nDoE\n\n\n\n\n\n\n\nCode Repository\n\n\n\nThe code is also made available on GitHub: https://github.com/pbosetti/DESAnED"
  },
  {
    "objectID": "slides/DESANED/index.html#bibliography",
    "href": "slides/DESANED/index.html#bibliography",
    "title": "Design of Experiments and Statistical Analysis of Experimental Data",
    "section": "Bibliography",
    "text": "Bibliography\n\nDouglas C. Montgomery, “Design and Analysis of Experiments”, 10th edition, Wiley, 2019, ISBN: 978-1-119-49244-3 (any cheaper edition is OK)\nHadley Wickham, Mine Çetinkaya-Rundel, Garrett Grolemund, “R for Data Science”, 2nd edition, O’Reilly, 2017, ISBN: 978-1492097402, (also online for free at https://r4ds.hadley.nz)\nFrançois Chollet with J. J. Allaire, “Deep Learning with R”, 2nd edition, Manning, 2019, ISBN 9781617295546"
  }
]